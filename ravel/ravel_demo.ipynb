{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate RAVEL with GPT-2 small and nnsight\n",
    "\n",
    "1. Load model\n",
    "2. Load dataset\n",
    "3. Load Sparse Autoencoders\n",
    "4. Find important directions for an Attribute (unique to Attribute or A_E combination)\n",
    "    -> I think unique to attribute? Same features for all \"Country\"-related objects (US, Germany, Japan). Not a suitable choice IMO if right.\n",
    "    -> Top features for each attribute by weight, train linear probe with L1 regularization on latents.\n",
    "5. Ravel evaluation\n",
    "    - Cause: Prediction accuracy of the true probe\n",
    "    - Isolation: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "RAVEL_LIB_DIR = '/share/u/can/ravel'\n",
    "RAVEL_SCRIPT_DIR = f'{RAVEL_LIB_DIR}/scripts'\n",
    "\n",
    "sys.path.append(RAVEL_LIB_DIR)\n",
    "sys.path.append(RAVEL_SCRIPT_DIR)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "MODEL_DIR = f'{RAVEL_LIB_DIR}/models'\n",
    "DATA_DIR = f'{RAVEL_LIB_DIR}/data'\n",
    "\n",
    "# NNsight Tracer Arguments\n",
    "tracer_kwargs = {'scan': False, 'validate': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# model_id = 'openai-community/gpt2'\n",
    "# model_id = 'EleutherAI/pythia-70m-deduped'\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "instance = \"tinyllama\"\n",
    "layer_idx = 14\n",
    "\n",
    "\n",
    "model = LanguageModel(model_id, dispatch=True, low_cpu_mem_usage=True, device_map='auto', cache_dir=MODEL_DIR,\n",
    "    torch_dtype=torch.bfloat16)\n",
    "submodule = model.model.layers[layer_idx]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=MODEL_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "VOCAB = sorted(tokenizer.vocab, key=tokenizer.vocab.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "entity_classes = [\n",
    "    'city',\n",
    "    'np_winner',\n",
    "    'occupation',\n",
    "    'physical_object',\n",
    "    'verb',\n",
    "]\n",
    "\n",
    "entity_type = 'city'\n",
    "\n",
    "with open(f'./data/base/ravel_{entity_type}_entity_attributes.json') as f:\n",
    "    entity_to_attribute_data = json.load(f)\n",
    "\n",
    "with open(f'./data/base/ravel_{entity_type}_attribute_to_prompts.json') as f:\n",
    "    attribute_to_prompts_data = json.load(f)\n",
    "\n",
    "with open(f'./data/base/ravel_{entity_type}_entity_to_split.json') as f: # for entity mode\n",
    "    entity_to_split_data = json.load(f)\n",
    "\n",
    "with open(f'./data/base/ravel_{entity_type}_prompt_to_split.json') as f: # for context mode\n",
    "    prompt_to_split_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which E <-> A_E pairs are known by chosen model.\n",
    "For each entity E x attribute A x attribute_prompt P, check if prediction yields correct answer.\n",
    "For those who do, split into train and test, as well as give token position.\n",
    "\n",
    "For tinyllama and city entity, this has been done in the tinyllama `tinyllama_city_[train].json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/{instance}/{instance}_{entity_type}_train.json') as f:\n",
    "    model_train_data = json.load(f)\n",
    "\n",
    "with open(f'./data/{instance}/{instance}_{entity_type}_context_test.json') as f:\n",
    "    model_context_test_data = json.load(f)\n",
    "\n",
    "with open(f'./data/{instance}/{instance}_{entity_type}_entity_test.json') as f:\n",
    "    model_entity_test_data = json.load(f)\n",
    "\n",
    "with open(f'./data/{instance}/{instance}_{entity_type}_prompt_to_entity_position.json') as f:\n",
    "    model_prompt_to_entity_position_data = json.load(f) # This is for prompts in all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_MAX_LEN = 48\n",
    "FEATURE_TYPES = datasets.Features({\"input\": datasets.Value(\"string\"), \"label\": datasets.Value(\"string\"),\n",
    "                              \"source_input\": datasets.Value(\"string\"), \"source_label\": datasets.Value(\"string\"),\n",
    "                              \"inv_label\": datasets.Value(\"string\"),\n",
    "                              'split': datasets.Value(\"string\"), 'source_split': datasets.Value(\"string\"),\n",
    "                              'entity': datasets.Value(\"string\"), 'source_entity': datasets.Value(\"string\")})\n",
    "\n",
    "\n",
    "# Load training dataset.\n",
    "split_to_raw_example = json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_train.json'), 'r'))\n",
    "# Load validation + test dataset.\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_context_test.json'), 'r')))\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_entity_test.json'), 'r')))\n",
    "\n",
    "# Prepend an extra token to avoid tokenization changes for Llama tokenizer.\n",
    "# Each sequence will start with <s> _ 0\n",
    "# SOS_PAD = '0'\n",
    "# NUM_SOS_TOKENS = 3\n",
    "# for split in split_to_raw_example:\n",
    "#   for i in range(len(split_to_raw_example[split])):\n",
    "#     split_to_raw_example[split][i]['inv_label'] = SOS_PAD + split_to_raw_example[split][i]['inv_label']\n",
    "#     split_to_raw_example[split][i]['label'] = SOS_PAD + split_to_raw_example[split][i]['label']\n",
    "\n",
    "\n",
    "# Load attributes (tasks) to prompt mapping.\n",
    "ALL_ATTR_TO_PROMPTS = json.load(open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_attribute_to_prompts.json')))\n",
    "\n",
    "# Load prompt to intervention location mapping.\n",
    "split_to_entity_pos = json.load(open(os.path.join(DATA_DIR, instance, f'{instance}_{entity_type}_prompt_to_entity_position.json')))\n",
    "SPLIT_TO_INV_LOCATIONS = {\n",
    "    f'{task}{split}': {'max_input_length': INPUT_MAX_LEN,\n",
    "                       'inv_position': [INPUT_MAX_LEN + pos]}\n",
    "    for task, pos in split_to_entity_pos.items()\n",
    "    for split in ('-train', '-test', '-val', '')\n",
    "}\n",
    "assert(min([min(v['inv_position']) for v in SPLIT_TO_INV_LOCATIONS.values()]) > 0)\n",
    "\n",
    "\n",
    "# Preprocess the dataset.\n",
    "def filter_inv_example(example):\n",
    "  return (example['label'] != example['inv_label'] and\n",
    "          example['source_split'] in SPLIT_TO_INV_LOCATIONS and\n",
    "          example['split'] in SPLIT_TO_INV_LOCATIONS)\n",
    "\n",
    "for split in split_to_raw_example:\n",
    "  random.shuffle(split_to_raw_example[split])\n",
    "  split_to_raw_example[split] = list(filter(filter_inv_example, split_to_raw_example[split]))\n",
    "  if len(split_to_raw_example[split]) == 0:\n",
    "    print('Empty split: \"%s\"' % split)\n",
    "\n",
    "# Remove empty splits.\n",
    "split_to_raw_example = {k: v for k, v in split_to_raw_example.items() if len(v) > 0}\n",
    "\n",
    "print(f\"#Training examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-train')]))}, \"\n",
    "      f\"#Validation examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-val')]))}, \"\n",
    "      f\"#Test examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-test')]))}\")\n",
    "split_to_dataset = {split: Dataset.from_list(\n",
    "    split_to_raw_example[split], features=FEATURE_TYPES)\n",
    "                    for split in split_to_raw_example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features from the SAE\n",
    "For each attribute, we will select the features that are most relevant to the attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_attribute = 'Country'\n",
    "dataset_name = f'{chosen_attribute}-train'\n",
    "\n",
    "split_to_dataset[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize probe training data\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "def pad_tokenized_input(tokenizer, input_str, max_len):\n",
    "    SOS_PAD = '0'\n",
    "    input_str = SOS_PAD + input_str\n",
    "    tokenized_input = tokenizer(input_str, return_tensors='pt', padding='max_length', truncation=True, max_length=max_len)\n",
    "    return tokenized_input['input_ids']\n",
    "\n",
    "def raw_prompt_generator(dataset, batch_size):\n",
    "    n_samples = len(dataset)\n",
    "    for batch_idx in range(n_samples // batch_size):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx + 1) * batch_size\n",
    "        batch = dataset[start_idx:end_idx]\n",
    "        prompt_batch = [example['input'] for example in batch]\n",
    "        tokenized_batch = [pad_tokenized_input(tokenizer, input_str, INPUT_MAX_LEN) for input_str in prompt_batch]\n",
    "        tokenized_batch = torch.cat(tokenized_batch, dim=0)\n",
    "        true_label_batch = [example['label'] for example in batch]\n",
    "        yield tokenized_batch, true_label_batch\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = split_to_raw_example[dataset_name]\n",
    "n_batches = len(train_dataset) // batch_size\n",
    "# n_batches = 10\n",
    "\n",
    "prompt_gen = raw_prompt_generator(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "unique_labels = set([example['label'] for example in train_dataset])\n",
    "label_to_probeidx = {label: i for i, label in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAE\n",
    "\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "\n",
    "d_model = 2048\n",
    "dict_size = 8192\n",
    "ae = AutoEncoder(d_model, dict_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize probe\n",
    "\n",
    "class AttributeProbe(torch.nn.Module):\n",
    "    def __init__(self, activation_dim, n_attribute_values):\n",
    "        super().__init__()\n",
    "        self.probe = torch.nn.Linear(activation_dim, n_attribute_values)\n",
    "    \n",
    "    def forward(self, activations):\n",
    "        return self.probe(activations)\n",
    "\n",
    "\n",
    "n_attribute_values = len(unique_labels)\n",
    "probe = AttributeProbe(dict_size, n_attribute_values).to(device)\n",
    "lr = 1e-2\n",
    "optimizer = torch.optim.AdamW(probe.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probe\n",
    "\n",
    "import einops\n",
    "\n",
    "all_prompts = []\n",
    "all_true_labels = []\n",
    "probe_losses = []\n",
    "for batch_idx in trange(n_batches):\n",
    "\n",
    "    # Draw new batch\n",
    "    prompts, true_labels = next(prompt_gen)\n",
    "    all_prompts.extend(prompts)\n",
    "    all_true_labels.extend(true_labels)\n",
    "    \n",
    "    # Get new SAE activations\n",
    "    with model.trace(prompts, **tracer_kwargs):\n",
    "        act = submodule.output[0].save()\n",
    "    latent_act = ae.encode(act)\n",
    "\n",
    "    # Train probe\n",
    "    optimizer.zero_grad()\n",
    "    logits = probe(latent_act)\n",
    "    # pred_labels = logits.argmax(dim=-1).to(torch.int)\n",
    "    true_labels = torch.tensor([label_to_probeidx[label] for label in true_labels], device=device)\n",
    "    true_labels_onehot = torch.nn.functional.one_hot(true_labels, num_classes=n_attribute_values)\n",
    "    true_labels = einops.repeat(true_labels_onehot, 'b o -> b p o', p=INPUT_MAX_LEN).to(torch.float)\n",
    "    loss = criterion(logits, true_labels)\n",
    "    probe_losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, top_indices = probe.probe.weight.topk(20, dim=1) # Top 20 features for each attribute\n",
    "attribute_feature_indices = set(top_indices.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# chosen_attribute = \"Continent\"\n",
    "# chosen_split = \"train\"\n",
    "\n",
    "# # Write a generator that generates text, attribute pairs in batches\n",
    "# # TODO implement shuffling\n",
    "\n",
    "# def prompt_generator(batch_size=128):\n",
    "#     prompt_templates = X_text['context'][chosen_split][chosen_attribute]\n",
    "#     entity_prompt_pairs = itertools.product(entity_to_attribute_data.keys(), prompt_templates)\n",
    "\n",
    "#     n_entities = len(entity_to_attribute_data.keys())\n",
    "#     n_pairs = n_entities * len(prompt_templates)\n",
    "#     if batch_size is None:\n",
    "#         batch_size = n_pairs\n",
    "#     total_batches = n_pairs // batch_size\n",
    "\n",
    "#     for batch_idx in range(total_batches):\n",
    "#         pairs_batch = [next(entity_prompt_pairs) for _ in range(batch_size)]\n",
    "#         prompt_batch = [prompt % entity for entity, prompt in pairs_batch]\n",
    "#         true_label_batch = [entity_to_attribute_data[entity][chosen_attribute] for entity, _ in pairs_batch]\n",
    "#         yield prompt_batch, true_label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cause intervention\n",
    "\n",
    "Find subspaces for each A.\n",
    "\n",
    "clean (base) E - A_E\n",
    "\n",
    "patch (inv) E' (A related features at final token of E')\n",
    "\n",
    "patch -> clean, accuracy of predicting A_E'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation\n",
    "\n",
    "Find subspaces for each A.\n",
    "\n",
    "for all other concepts (B_E)\n",
    "\n",
    "clean (base) E - B_E\n",
    "\n",
    "patch (inv) E' (A related features at final token of E')\n",
    "\n",
    "patch -> clean, accuracy of predicting B_E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
