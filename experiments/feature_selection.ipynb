{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import einops\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "import experiments.utils as utils\n",
    "from experiments.probe_training import *\n",
    "from experiments.pipeline_config import PipelineConfig\n",
    "\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "\n",
    "# Configuration\n",
    "DEBUGGING = False\n",
    "SEED = 42\n",
    "\n",
    "# Set up paths and model\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "tracer_kwargs = dict(scan=DEBUGGING, validate=DEBUGGING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_activations(\n",
    "    text_inputs: list[str], model: LanguageModel, batch_size: int, submodule: utils.submodule_alias\n",
    ") -> torch.Tensor:\n",
    "    # TODO: Rename text_inputs\n",
    "    text_batches = utils.batch_inputs(text_inputs, batch_size)\n",
    "\n",
    "    all_acts_list_BD = []\n",
    "    for text_batch_BL in text_batches:\n",
    "        with model.trace(\n",
    "            text_batch_BL,\n",
    "            **tracer_kwargs,\n",
    "        ):\n",
    "            attn_mask = model.input[1][\"attention_mask\"]\n",
    "            acts_BLD = submodule.output[0]\n",
    "            acts_BLD = acts_BLD * attn_mask[:, :, None]\n",
    "            acts_BD = acts_BLD.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts_BD = acts_BD.save()\n",
    "        all_acts_list_BD.append(acts_BD.value)\n",
    "\n",
    "    all_acts_bD = torch.cat(all_acts_list_BD, dim=0)\n",
    "    return all_acts_bD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "device = \"cuda\"\n",
    "train_set_size = 4000\n",
    "test_set_size = 1000\n",
    "context_length = 128\n",
    "include_gender = True\n",
    "model_dtype = torch.bfloat16\n",
    "\n",
    "probe_batch_size = 500\n",
    "llm_batch_size = 500\n",
    "\n",
    "# TODO: I think there may be a scoping issue with model and get_acts(), but we currently aren't using get_acts()\n",
    "model = LanguageModel(llm_model_name, device_map=device, dispatch=True, torch_dtype=model_dtype)\n",
    "probe_dir = \"trained_bib_probes\"\n",
    "only_model_name = llm_model_name.split(\"/\")[-1]\n",
    "\n",
    "model_eval_config = utils.ModelEvalConfig.from_full_model_name(llm_model_name)\n",
    "probe_layer = model_eval_config.probe_layer\n",
    "\n",
    "probe_output_filename = (\n",
    "    f\"{probe_dir}/{only_model_name}/probes_ctx_len_{context_length}_layer_{probe_layer}.pkl\"\n",
    ")\n",
    "\n",
    "epochs= 1\n",
    "save_results = True\n",
    "seed = SEED\n",
    "include_gender = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Because we save the probes, we always train them on all classes to avoid potential issues with missing classes. It's only a one-time cost.\"\"\"\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model_eval_config = utils.ModelEvalConfig.from_full_model_name(llm_model_name)\n",
    "d_model = model_eval_config.activation_dim\n",
    "probe_layer = model_eval_config.probe_layer\n",
    "probe_act_submodule = utils.get_submodule(model, \"resid_post\", probe_layer)\n",
    "\n",
    "dataset, df = load_and_prepare_dataset()\n",
    "\n",
    "train_bios, test_bios = get_train_test_data(\n",
    "    dataset,\n",
    "    train_set_size,\n",
    "    test_set_size,\n",
    "    include_gender,\n",
    ")\n",
    "\n",
    "train_bios = utils.tokenize_data(train_bios, model.tokenizer, context_length, device)\n",
    "test_bios = utils.tokenize_data(test_bios, model.tokenizer, context_length, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ids = [10]\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    # \"pythia70m_sweep_standard_ctx128_0712\": {\n",
    "    #     #     # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "    #     #     # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "    #     #     # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "    #     \"resid_post_layer_3\": {\"trainer_ids\": [6]},\n",
    "    #     #     \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "    # },\n",
    "    \"pythia70m_sweep_topk_ctx128_0730\": {\n",
    "        # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_4\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "}\n",
    "\n",
    "p_config = PipelineConfig()\n",
    "\n",
    "sweep_name, submodule_trainers = list(ae_sweep_paths.items())[0]\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(\n",
    "    p_config.dictionaries_path, sweep_name, submodule_trainers\n",
    ")\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "print(ae_paths)\n",
    "\n",
    "ae_path = ae_paths[0]\n",
    "\n",
    "submodule, dictionary, sae_config = utils.load_dictionary(model, ae_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_sae_activations(\n",
    "    text_inputs: list[str],\n",
    "    model: LanguageModel,\n",
    "    dictionary: AutoEncoder,\n",
    "    batch_size: int,\n",
    "    submodule: utils.submodule_alias,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # TODO: Rename text_inputs\n",
    "    text_batches = utils.batch_inputs(text_inputs, batch_size)\n",
    "\n",
    "    with torch.no_grad(), model.trace(\"_\"):\n",
    "        is_tuple = type(submodule.output.shape) == tuple\n",
    "\n",
    "    model_dtype = model.dtype\n",
    "\n",
    "    all_acts_list_BD = []\n",
    "    all_sae_acts_list_BF = []\n",
    "    for text_batch_BL in text_batches:\n",
    "        with model.trace(\n",
    "            text_batch_BL,\n",
    "            **tracer_kwargs,\n",
    "        ):\n",
    "            attn_mask = model.input[1][\"attention_mask\"]\n",
    "            acts_BLD = submodule.output\n",
    "\n",
    "            if is_tuple:\n",
    "                acts_BLD = acts_BLD[0]\n",
    "\n",
    "            acts_BLF = dictionary.encode(acts_BLD)\n",
    "            acts_BLF = acts_BLF * attn_mask[:, :, None]\n",
    "            acts_BF = acts_BLF.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts_BF = acts_BF.save()\n",
    "\n",
    "            acts_BLD = acts_BLD * attn_mask[:, :, None]\n",
    "            acts_BD = acts_BLD.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts_BD = acts_BD.save()\n",
    "        all_acts_list_BD.append(acts_BD.value)\n",
    "        all_sae_acts_list_BF.append(acts_BF.value.to(dtype=model_dtype))\n",
    "\n",
    "    all_acts_bD = torch.cat(all_acts_list_BD, dim=0)\n",
    "    all_sae_acts_bF = torch.cat(all_sae_acts_list_BF, dim=0)\n",
    "\n",
    "    return all_acts_bD, all_sae_acts_bF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_acts = {}\n",
    "all_test_acts = {}\n",
    "\n",
    "all_train_sae_acts = {}\n",
    "all_test_sae_acts = {}\n",
    "\n",
    "llm_batch_size = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, profession in enumerate(train_bios.keys()):\n",
    "        # if isinstance(profession, int):\n",
    "        #     continue\n",
    "\n",
    "        print(f\"Collecting activations for profession: {profession}\")\n",
    "\n",
    "        all_train_acts[profession], all_train_sae_acts[profession] = get_all_sae_activations(\n",
    "            train_bios[profession], model, dictionary, llm_batch_size, submodule\n",
    "        )\n",
    "        all_test_acts[profession], all_test_sae_acts[profession] = get_all_sae_activations(\n",
    "            test_bios[profession], model, dictionary, llm_batch_size, submodule\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tensor_memory_usage(tensor: torch.Tensor):\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        print(\"Input is not a tensor. Cannot calculate memory usage.\")\n",
    "        return\n",
    "    \n",
    "    memory = tensor.element_size() * tensor.nelement()\n",
    "    print(f\"Tensor Shape: {tensor.shape}\")\n",
    "    print(f\"Tensor Type: {tensor.dtype}\")\n",
    "    print(f\"Memory usage: {memory / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_acts = all_train_acts[0]\n",
    "sae_acts = all_train_sae_acts[0]\n",
    "print_tensor_memory_usage(model_acts)\n",
    "print_tensor_memory_usage(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe model and training\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim: int, dtype: torch.dtype):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True, dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "def train_probe(\n",
    "    train_input_batches: list,\n",
    "    train_label_batches: list[torch.Tensor],\n",
    "    test_input_batches: list,\n",
    "    test_label_batches: list[torch.Tensor],\n",
    "    get_acts: Callable,\n",
    "    precomputed_acts: bool,\n",
    "    dim: int,\n",
    "    epochs: int,\n",
    "    device: str,\n",
    "    model_dtype: torch.dtype,\n",
    "    lr: float = 1e-2,\n",
    "    seed: int = SEED,\n",
    "    verbose: bool = False,\n",
    ") -> tuple[Probe, float]:\n",
    "    \"\"\"input_batches can be a list of tensors or strings. If strings, get_acts must be provided.\"\"\"\n",
    "\n",
    "    if type(train_input_batches[0]) == str or type(test_input_batches[0]) == str:\n",
    "        assert precomputed_acts == False\n",
    "    elif type(train_input_batches[0]) == torch.Tensor or type(test_input_batches[0]) == torch.Tensor:\n",
    "        assert precomputed_acts == True\n",
    "\n",
    "    probe = Probe(dim, model_dtype).to(device)\n",
    "    optimizer = torch.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_idx = 0\n",
    "        for inputs, labels in zip(train_input_batches, train_label_batches):\n",
    "            if precomputed_acts:\n",
    "                acts_BD = inputs\n",
    "            else:\n",
    "                acts_BD = get_acts(inputs)\n",
    "            logits_B = probe(acts_BD)\n",
    "            loss = criterion(logits_B, labels.clone().detach().to(device=device, dtype=model_dtype))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_idx += 1\n",
    "\n",
    "        \n",
    "\n",
    "        train_accuracy = test_probe(\n",
    "            train_input_batches[:30], train_label_batches[:30], probe, get_acts, precomputed_acts\n",
    "        )\n",
    "\n",
    "\n",
    "        test_accuracy = test_probe(\n",
    "            test_input_batches, test_label_batches, probe, get_acts, precomputed_acts\n",
    "        )\n",
    "\n",
    "        if epoch == epochs - 1 and verbose:\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs} Loss: {loss.item()}, train accuracy: {train_accuracy}, test accuracy: {test_accuracy}\\n\")\n",
    "    \n",
    "    return probe, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.probe_training import prepare_probe_data\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def train_probe_on_activations(\n",
    "    train_activations: dict[str | int : torch.Tensor],\n",
    "    test_activations: dict[str | int : torch.Tensor],\n",
    "    select_top_k: Optional[int] = None,\n",
    ") -> tuple[dict[str | int : Probe], dict[str | int : float]]:\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    probes, test_accuracies = {}, {}\n",
    "\n",
    "    for profession in train_activations.keys():\n",
    "        if profession in utils.PAIRED_CLASS_KEYS.values():\n",
    "            continue\n",
    "\n",
    "        train_acts, train_labels = prepare_probe_data(train_activations, profession, probe_batch_size, select_top_k)\n",
    "\n",
    "        test_acts, test_labels = prepare_probe_data(test_activations, profession, probe_batch_size, select_top_k)\n",
    "\n",
    "        if profession == \"biased_male / biased_female\" or profession == \"male / female\":\n",
    "            probe_epochs = 1\n",
    "        else:\n",
    "            probe_epochs = epochs\n",
    "\n",
    "        activation_dim = train_acts[0].shape[1]\n",
    "\n",
    "        probe, test_accuracy = train_probe(\n",
    "            train_acts,\n",
    "            train_labels,\n",
    "            test_acts,\n",
    "            test_labels,\n",
    "            get_acts,\n",
    "            precomputed_acts=True,\n",
    "            epochs=probe_epochs,\n",
    "            dim=activation_dim,\n",
    "            device=device,\n",
    "            model_dtype=model_dtype,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        probes[profession] = probe\n",
    "        test_accuracies[profession] = test_accuracy\n",
    "\n",
    "    return probes, test_accuracies\n",
    "\n",
    "    # if save_results:\n",
    "    #     only_model_name = llm_model_name.split(\"/\")[-1]\n",
    "    #     os.makedirs(f\"{probe_dir}\", exist_ok=True)\n",
    "    #     os.makedirs(f\"{probe_dir}/{only_model_name}\", exist_ok=True)\n",
    "\n",
    "    #     with open(probe_output_filename, \"wb\") as f:\n",
    "    #         pickle.dump(probes, f)\n",
    "\n",
    "\n",
    "model_activation_probes, model_activation_accuracies = train_probe_on_activations(all_train_acts, all_test_acts)\n",
    "sae_activation_probes, sae_activation_accuracies = train_probe_on_activations(all_train_sae_acts, all_test_sae_acts)\n",
    "sae_activation_top_1_probes, sae_activation_top_1_accuracies = train_probe_on_activations(all_train_sae_acts, all_test_sae_acts, select_top_k=1)\n",
    "sae_activation_top_5_probes, sae_activation_top_5_accuracies = train_probe_on_activations(all_train_sae_acts, all_test_sae_acts, select_top_k=5)\n",
    "sae_activation_top_10_probes, sae_activation_top_10_accuracies = train_probe_on_activations(all_train_sae_acts, all_test_sae_acts, select_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diffs = []\n",
    "int_diffs = []\n",
    "str_diffs = []\n",
    "\n",
    "for class_name in model_activation_accuracies.keys():\n",
    "    model_acc = model_activation_accuracies[class_name][0]\n",
    "    sae_acc = sae_activation_accuracies[class_name][0]\n",
    "    # sae_acc = sae_activation_top_1_accuracies[class_name][0]\n",
    "    # sae_acc = sae_activation_top_5_accuracies[class_name][0]\n",
    "    # sae_acc = sae_activation_top_10_accuracies[class_name][0]\n",
    "\n",
    "    diff = model_acc - sae_acc\n",
    "    print(f\"Class: {class_name}, Model Acc: {model_acc}, SAE Acc: {sae_acc}, Diff: {diff}\")\n",
    "\n",
    "    if isinstance(class_name, int):\n",
    "        int_diffs.append(diff)\n",
    "    if isinstance(class_name, str):\n",
    "        str_diffs.append(diff)\n",
    "    all_diffs.append(diff)\n",
    "\n",
    "print(f\"\\nAverage difference: {np.mean(all_diffs)}\")\n",
    "print(f\"Average difference for int classes: {np.mean(int_diffs)}\")\n",
    "print(f\"Average difference for str classes: {np.mean(str_diffs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.probe_training import get_activation_distribution_diff\n",
    "\n",
    "sae_feature_distribution_differences = {}\n",
    "for profession in all_train_sae_acts.keys():\n",
    "    if profession in utils.PAIRED_CLASS_KEYS.values():\n",
    "        continue\n",
    "\n",
    "    sae_feature_distribution_differences[profession] = get_activation_distribution_diff(all_train_sae_acts, profession)\n",
    "\n",
    "print(sae_feature_distribution_differences[0].sum())\n",
    "print(sae_feature_distribution_differences[\"male / female\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_class_indices = [\n",
    "    \"male / female\",\n",
    "    \"professor / nurse\",\n",
    "    \"male_professor / female_nurse\",\n",
    "    \"biased_male / biased_female\",\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    6,\n",
    "]\n",
    "\n",
    "saved_sae_feature_distribution_differences = {}\n",
    "\n",
    "for class_index in chosen_class_indices:\n",
    "    saved_sae_feature_distribution_differences[class_index] = sae_feature_distribution_differences[class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ae_path}/node_effects.pkl\", \"rb\") as f:\n",
    "    node_effects_attrib_patching = pickle.load(f)\n",
    "\n",
    "with open(f\"{ae_path}/node_effects_dist_diff.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sae_feature_distribution_differences, f)\n",
    "\n",
    "class_idx = \"male / female\"\n",
    "# class_idx = 0\n",
    "\n",
    "node_effect = node_effects_attrib_patching[class_idx]\n",
    "sae_distribution = sae_feature_distribution_differences[class_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"node effect stats: {node_effect.mean():.4f}, {node_effect.std().item():.4f}, {node_effect.max().item():.4f}, {node_effect.min().item():.4f}\")\n",
    "print(f\"sae distribution stats: {sae_distribution.mean():.4f}, {sae_distribution.std().item():.4f}, {sae_distribution.max().item():.4f}, {sae_distribution.min().item():.4f}\")\n",
    "import torch\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    return (tensor - tensor.mean()) / tensor.std()\n",
    "\n",
    "def compare_top_values(tensor1, tensor2, top_n=10):\n",
    "    # Ensure tensors are on the same device\n",
    "    device = tensor1.device\n",
    "    tensor2 = tensor2.to(device)\n",
    "\n",
    "    # Convert to float32 for calculations\n",
    "    tensor1 = tensor1.to(torch.float32)\n",
    "    tensor2 = tensor2.to(torch.float32)\n",
    "\n",
    "    # Normalize tensors\n",
    "    norm_tensor1 = normalize_tensor(tensor1)\n",
    "    norm_tensor2 = normalize_tensor(tensor2)\n",
    "    \n",
    "    # Get indices of top N values\n",
    "    top_indices1 = torch.argsort(norm_tensor1, descending=True)[:top_n]\n",
    "    top_indices2 = torch.argsort(norm_tensor2, descending=True)[:top_n]\n",
    "    \n",
    "    print(f\"Top {top_n} indices in normalized node_effect:\")\n",
    "    for i, idx in enumerate(top_indices1):\n",
    "        print(f\"  {i+1}. Index {idx.item()}: {norm_tensor1[idx].item():.4f} (original: {tensor1[idx].item():.4f})\")\n",
    "    \n",
    "    print(f\"\\nTop {top_n} indices in normalized sae_distribution:\")\n",
    "    for i, idx in enumerate(top_indices2):\n",
    "        print(f\"  {i+1}. Index {idx.item()}: {norm_tensor2[idx].item():.4f} (original: {tensor2[idx].item():.4f})\")\n",
    "    \n",
    "    # Compare common indices\n",
    "    common_indices = set(top_indices1.tolist()) & set(top_indices2.tolist())\n",
    "    print(f\"\\nCommon indices in top {top_n}: {common_indices}\")\n",
    "    \n",
    "    if common_indices:\n",
    "        print(\"\\nValues at common indices:\")\n",
    "        for idx in common_indices:\n",
    "            print(f\"  Index {idx}: node_effect = {norm_tensor1[idx].item():.4f}, sae_distribution = {norm_tensor2[idx].item():.4f}\")\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = torch.corrcoef(torch.stack([norm_tensor1, norm_tensor2]))[0, 1]\n",
    "    print(f\"\\nCorrelation between normalized tensors: {correlation.item():.4f}\")\n",
    "\n",
    "# Usage:\n",
    "compare_top_values(node_effect, sae_distribution, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_probes(\n",
    "#     train_set_size: int,\n",
    "#     test_set_size: int,\n",
    "#     model: LanguageModel,\n",
    "#     context_length: int,\n",
    "#     probe_batch_size: int,\n",
    "#     llm_batch_size: int,\n",
    "#     device: str,\n",
    "#     probe_output_filename: str,\n",
    "#     probe_dir: str = \"trained_bib_probes\",\n",
    "#     llm_model_name: str = \"EleutherAI/pythia-70m-deduped\",\n",
    "#     epochs: int = 10,\n",
    "#     model_dtype: torch.dtype = torch.bfloat16,\n",
    "#     save_results: bool = True,\n",
    "#     seed: int = SEED,\n",
    "#     include_gender: bool = False,\n",
    "# ) -> dict[int, float]:\n",
    "#     \"\"\"Because we save the probes, we always train them on all classes to avoid potential issues with missing classes. It's only a one-time cost.\"\"\"\n",
    "#     torch.manual_seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     model_eval_config = utils.ModelEvalConfig.from_full_model_name(llm_model_name)\n",
    "#     d_model = model_eval_config.activation_dim\n",
    "#     probe_layer = model_eval_config.probe_layer\n",
    "#     probe_act_submodule = utils.get_submodule(model, \"resid_post\", probe_layer)\n",
    "\n",
    "#     dataset, df = load_and_prepare_dataset()\n",
    "\n",
    "#     train_bios, test_bios = get_train_test_data(\n",
    "#         dataset,\n",
    "#         train_set_size,\n",
    "#         test_set_size,\n",
    "#         include_gender,\n",
    "#     )\n",
    "\n",
    "#     train_bios = utils.tokenize_data(train_bios, model.tokenizer, context_length, device)\n",
    "#     test_bios = utils.tokenize_data(test_bios, model.tokenizer, context_length, device)\n",
    "\n",
    "#     probes, test_accuracies = {}, {}\n",
    "\n",
    "#     all_train_acts = {}\n",
    "#     all_test_acts = {}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i, profession in enumerate(train_bios.keys()):\n",
    "#             # if isinstance(profession, int):\n",
    "#             #     continue\n",
    "\n",
    "#             print(f\"Collecting activations for profession: {profession}\")\n",
    "\n",
    "#             all_train_acts[profession] = get_all_activations(\n",
    "#                 train_bios[profession], model, llm_batch_size, probe_act_submodule\n",
    "#             )\n",
    "#             all_test_acts[profession] = get_all_activations(\n",
    "#                 test_bios[profession], model, llm_batch_size, probe_act_submodule\n",
    "#             )\n",
    "\n",
    "#     torch.set_grad_enabled(True)\n",
    "\n",
    "#     for profession in all_train_acts.keys():\n",
    "#         if profession in utils.PAIRED_CLASS_KEYS.values():\n",
    "#             continue\n",
    "\n",
    "#         train_acts, train_labels = prepare_probe_data(all_train_acts, profession, probe_batch_size)\n",
    "\n",
    "#         test_acts, test_labels = prepare_probe_data(all_test_acts, profession, probe_batch_size)\n",
    "\n",
    "#         if profession == \"biased_male / biased_female\" or profession == \"male / female\":\n",
    "#             probe_epochs = 1\n",
    "#         else:\n",
    "#             probe_epochs = epochs\n",
    "\n",
    "#         probe, test_accuracy = train_probe(\n",
    "#             train_acts,\n",
    "#             train_labels,\n",
    "#             test_acts,\n",
    "#             test_labels,\n",
    "#             get_acts,\n",
    "#             precomputed_acts=True,\n",
    "#             epochs=probe_epochs,\n",
    "#             dim=d_model,\n",
    "#             device=device,\n",
    "#             model_dtype=model_dtype,\n",
    "#         )\n",
    "\n",
    "#         probes[profession] = probe\n",
    "#         test_accuracies[profession] = test_accuracy\n",
    "\n",
    "#     if save_results:\n",
    "#         only_model_name = llm_model_name.split(\"/\")[-1]\n",
    "#         os.makedirs(f\"{probe_dir}\", exist_ok=True)\n",
    "#         os.makedirs(f\"{probe_dir}/{only_model_name}\", exist_ok=True)\n",
    "\n",
    "#         with open(probe_output_filename, \"wb\") as f:\n",
    "#             pickle.dump(probes, f)\n",
    "\n",
    "#     return test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# device = \"cuda\"\n",
    "# train_set_size = 1000\n",
    "# test_set_size = 1000\n",
    "# context_length = 128\n",
    "# include_gender = True\n",
    "# model_dtype = torch.bfloat16\n",
    "\n",
    "# # TODO: I think there may be a scoping issue with model and get_acts(), but we currently aren't using get_acts()\n",
    "# model = LanguageModel(llm_model_name, device_map=device, dispatch=True, torch_dtype=model_dtype)\n",
    "# probe_dir = \"trained_bib_probes\"\n",
    "# only_model_name = llm_model_name.split(\"/\")[-1]\n",
    "\n",
    "# model_eval_config = utils.ModelEvalConfig.from_full_model_name(llm_model_name)\n",
    "# probe_layer = model_eval_config.probe_layer\n",
    "\n",
    "# probe_output_filename = (\n",
    "#     f\"{probe_dir}/{only_model_name}/probes_ctx_len_{context_length}_layer_{probe_layer}.pkl\"\n",
    "# )\n",
    "\n",
    "# test_accuracies = train_probes(\n",
    "#     train_set_size=1000,\n",
    "#     test_set_size=1000,\n",
    "#     model=model,\n",
    "#     context_length=128,\n",
    "#     probe_batch_size=500,\n",
    "#     llm_batch_size=500,\n",
    "#     llm_model_name=llm_model_name,\n",
    "#     epochs=10,\n",
    "#     device=device,\n",
    "#     probe_output_filename=probe_output_filename,\n",
    "#     probe_dir=probe_dir,\n",
    "#     seed=SEED,\n",
    "#     include_gender=include_gender,\n",
    "#     model_dtype=model_dtype,\n",
    "# )\n",
    "# print(test_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
