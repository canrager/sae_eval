{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative assessment of Autointerp for Targeted Probe Perturbation metric\n",
    "For example: An SAE with L0 500 has good performance before auto-interp, and poor performance after. What features are being rejected by the autointerp?\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Probe to perturbate is trained on layer 24 / 26 resid post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments.utils as utils\n",
    "import pickle\n",
    "import os\n",
    "import experiments.autointerp as autointerp\n",
    "from nnsight import LanguageModel\n",
    "from experiments.pipeline_config import PipelineConfig\n",
    "import torch as t\n",
    "\n",
    "from experiments.bib_intervention import select_features\n",
    "from experiments.pipeline_config import FeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionaries\n",
    "\n",
    "DICTIONARIES_PATH = \"../dictionary_learning/dictionaries/gemma-2-2b-saved-data\"\n",
    "trainer_ids = [5]\n",
    "ae_sweep_paths = {\n",
    "    # \"gemma-2-2b_sweep_jumprelu_0902_probe_layer24_results\": {\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "    # \"gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer24_results\": {\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer24_results\": {\n",
    "        \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "    # \"gemma-2-2b_sweep_standard_ctx128_ef2_0824_probe_layer_24_results\": {\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "    # \"gemma-2-2b_sweep_topk_ctx128_ef2_0824_probe_layer_24_results\": {\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "}\n",
    "\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "ae_paths = []\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "\n",
    "    ae_group_paths = utils.get_ae_group_paths(\n",
    "        DICTIONARIES_PATH, sweep_name, submodule_trainers\n",
    "    )\n",
    "    ae_paths.extend(utils.get_ae_paths(ae_group_paths))\n",
    "\n",
    "print(f'available paths: {ae_paths}\\n')\n",
    "ae_path = ae_paths[0]\n",
    "print(f'Selecting path: {ae_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute max activating examples, if they haven't been computed yet\n",
    "\n",
    "# p_config = PipelineConfig()\n",
    "# model_eval_config = utils.ModelEvalConfig.from_sweep_name(sweep_name)\n",
    "# model_name = model_eval_config.full_model_name\n",
    "\n",
    "# llm_batch_size, patching_batch_size, eval_results_batch_size = utils.get_batch_sizes(\n",
    "#     model_eval_config,\n",
    "#     p_config.reduced_GPU_memory,\n",
    "#     p_config.train_set_size,\n",
    "#     p_config.test_set_size,\n",
    "#     p_config.probe_train_set_size,\n",
    "#     p_config.probe_test_set_size,\n",
    "# )\n",
    "\n",
    "# model = LanguageModel(\n",
    "#     model_name,\n",
    "#     device_map=p_config.device,\n",
    "#     dispatch=True,\n",
    "#     attn_implementation=\"eager\",\n",
    "#     torch_dtype=p_config.model_dtype,\n",
    "# )\n",
    "\n",
    "# autointerp.get_autointerp_inputs_for_all_saes(\n",
    "#         model,\n",
    "#         p_config.max_activations_collection_n_inputs,\n",
    "#         llm_batch_size,\n",
    "#         p_config.autointerp_context_length,\n",
    "#         p_config.top_k_inputs_act_collect,\n",
    "#         ae_paths,\n",
    "#         force_rerun=p_config.force_max_activations_recompute,\n",
    "#     )\n",
    "\n",
    "# # Load max activations\n",
    "# with open(os.path.join(ae_path, \"max_activating_inputs.pkl\"), \"rb\") as f:\n",
    "#     max_activating_inputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load class_accuracies after ablation\n",
    "# def load_class_accuracies(ae_path: str, scoring_method: str):\n",
    "#     assert scoring_method in ['attrib', 'auto_interp']\n",
    "\n",
    "#     filename = f\"{ae_path}/class_accuracies_{scoring_method}.pkl\"\n",
    "#     with open(filename, \"rb\") as f:\n",
    "#             class_accuracies = pickle.load(f)\n",
    "#     return class_accuracies\n",
    "\n",
    "# probe_acc_attrib = load_class_accuracies(ae_path, 'attrib')\n",
    "# probe_autointerp = load_class_accuracies(ae_path, 'auto_interp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load importance scores\n",
    "with open(os.path.join(ae_path, \"node_effects.pkl\"), \"rb\") as f:\n",
    "    scores_attrib = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects_auto_interp.pkl\"), \"rb\") as f:\n",
    "    scores_autointerp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load top features from importance score files\n",
    "\n",
    "selection_method = FeatureSelection.top_n\n",
    "num_top_features_from_attrib = 20 # aka threshold\n",
    "class_indices = [0, 1, 2, 6] # Effectively filtering out spurious correlation classes, these are the only TPP available\n",
    "dict_size = next(iter(scores_attrib.values())).shape[0]\n",
    "\n",
    "top_latent_tensor_attrib = select_features(\n",
    "    selection_method=selection_method,\n",
    "    node_effects=scores_attrib,\n",
    "    T_effects=[num_top_features_from_attrib],\n",
    "    T_max_sideeffect=None,\n",
    "    dict_size=dict_size,\n",
    ")\n",
    "top_latent_tensor_autointerp = select_features(\n",
    "    selection_method=selection_method,\n",
    "    node_effects=scores_autointerp,\n",
    "    T_effects=[num_top_features_from_attrib],\n",
    "    T_max_sideeffect=None,\n",
    "    dict_size=dict_size,\n",
    ")\n",
    "\n",
    "# Reformat indices\n",
    "top_latent_indices_attrib = {\n",
    "    k: v.nonzero().squeeze()\n",
    "    for k, v in top_latent_tensor_attrib[num_top_features_from_attrib].items()\n",
    "    if k in class_indices\n",
    "}\n",
    "top_latent_indices_autointerp = {\n",
    "    k: v.nonzero().squeeze()\n",
    "    for k, v in top_latent_tensor_autointerp[num_top_features_from_attrib].items()\n",
    "    if k in class_indices\n",
    "}\n",
    "top_latent_indices_not_autointerp = {\n",
    "    k: v[t.isin(v, top_latent_indices_autointerp[k], invert=True)]\n",
    "    for k, v in top_latent_indices_attrib.items()\n",
    "    if k in class_indices\n",
    "}\n",
    "\n",
    "for k in class_indices:\n",
    "    print()\n",
    "    print(f'Class {k}')\n",
    "    print(f'Attrib: {len(top_latent_indices_attrib[k])}, {top_latent_indices_attrib[k]}')\n",
    "    print(f'AutoInterp: {len(top_latent_indices_autointerp[k])}, {top_latent_indices_autointerp[k]}')\n",
    "    print(f'AutoInterp: {len(top_latent_indices_autointerp[k])}, {top_latent_tensor_autointerp[num_top_features_from_attrib][k][top_latent_indices_autointerp[k]]}')\n",
    "    print(f'Attrib not AutoInterp: {len(top_latent_indices_not_autointerp[k])}, {(top_latent_indices_not_autointerp[k])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max act examples per feature\n",
    "\n",
    "# Show max act of autointerp included / rejected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are ablation differences so small?\n",
    "# Attribution patching is a bad approximation and misses indirectly relevant features? Do acutal patching instead?\n",
    "# SAE latents are correlated? Patching a single latent will not have a large effect?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
