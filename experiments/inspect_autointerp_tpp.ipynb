{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative assessment of Autointerp for Targeted Probe Perturbation metric\n",
    "For example: An SAE with L0 500 has good performance before auto-interp, and poor performance after. What features are being rejected by the autointerp?\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Probe to perturbate is trained on layer 24 / 26 resid post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments.utils as utils\n",
    "import pickle\n",
    "import os\n",
    "import experiments.autointerp as autointerp\n",
    "from nnsight import LanguageModel\n",
    "from experiments.pipeline_config import PipelineConfig\n",
    "import torch as t\n",
    "\n",
    "from experiments.bib_intervention import select_features\n",
    "from experiments.pipeline_config import FeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available paths: ['../dictionary_learning/dictionaries/gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_11/trainer_0', '../dictionary_learning/dictionaries/gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_11/trainer_3', '../dictionary_learning/dictionaries/gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_11/trainer_5', '../dictionary_learning/dictionaries/gemma-2-2b_sweep_standard_ctx128_ef8_0824/resid_post_layer_11/trainer_0', '../dictionary_learning/dictionaries/gemma-2-2b_sweep_standard_ctx128_ef8_0824/resid_post_layer_11/trainer_3', '../dictionary_learning/dictionaries/gemma-2-2b_sweep_standard_ctx128_ef8_0824/resid_post_layer_11/trainer_5']\n",
      "\n",
      "Selecting path: ../dictionary_learning/dictionaries/gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_11/trainer_5\n"
     ]
    }
   ],
   "source": [
    "# Define dictionaries\n",
    "\n",
    "# DICTIONARIES_PATH = \"../dictionary_learning/dictionaries/gemma-2-2b-saved-data\"\n",
    "# trainer_ids = [5]\n",
    "# ae_sweep_paths = {\n",
    "#     # \"gemma-2-2b_sweep_jumprelu_0902_probe_layer24_results\": {\n",
    "#     #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     # },\n",
    "#     # \"gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer24_results\": {\n",
    "#     #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     # },\n",
    "#     \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer24_results\": {\n",
    "#         \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "#     # \"gemma-2-2b_sweep_standard_ctx128_ef2_0824_probe_layer_24_results\": {\n",
    "#     #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     # },\n",
    "#     # \"gemma-2-2b_sweep_topk_ctx128_ef2_0824_probe_layer_24_results\": {\n",
    "#     #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     # },\n",
    "# }\n",
    "\n",
    "DICTIONARIES_PATH = \"../dictionary_learning/dictionaries\"\n",
    "\n",
    "trainer_ids = [0, 3, 5]\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824\": {\n",
    "        # \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_7\": {\"trainer_ids\": trainer_ids},\n",
    "        \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_15\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "},\n",
    "    \"gemma-2-2b_sweep_standard_ctx128_ef8_0824\": {\n",
    "        # \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_7\": {\"trainer_ids\": trainer_ids},\n",
    "        \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_15\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "ae_paths = []\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "\n",
    "    ae_group_paths = utils.get_ae_group_paths(\n",
    "        DICTIONARIES_PATH, sweep_name, submodule_trainers\n",
    "    )\n",
    "    ae_paths.extend(utils.get_ae_paths(ae_group_paths))\n",
    "\n",
    "print(f'available paths: {ae_paths}\\n')\n",
    "ae_path = ae_paths[2]\n",
    "print(f'Selecting path: {ae_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute max activating examples, if they haven't been computed yet\n",
    "\n",
    "# p_config = PipelineConfig()\n",
    "# model_eval_config = utils.ModelEvalConfig.from_sweep_name(sweep_name)\n",
    "# model_name = model_eval_config.full_model_name\n",
    "\n",
    "# llm_batch_size, patching_batch_size, eval_results_batch_size = utils.get_batch_sizes(\n",
    "#     model_eval_config,\n",
    "#     p_config.reduced_GPU_memory,\n",
    "#     p_config.train_set_size,\n",
    "#     p_config.test_set_size,\n",
    "#     p_config.probe_train_set_size,\n",
    "#     p_config.probe_test_set_size,\n",
    "# )\n",
    "\n",
    "# model = LanguageModel(\n",
    "#     model_name,\n",
    "#     device_map=p_config.device,\n",
    "#     dispatch=True,\n",
    "#     attn_implementation=\"eager\",\n",
    "#     torch_dtype=p_config.model_dtype,\n",
    "# )\n",
    "\n",
    "# autointerp.get_autointerp_inputs_for_all_saes(\n",
    "#         model,\n",
    "#         p_config.max_activations_collection_n_inputs,\n",
    "#         llm_batch_size,\n",
    "#         p_config.autointerp_context_length,\n",
    "#         p_config.top_k_inputs_act_collect,\n",
    "#         ae_paths,\n",
    "#         force_rerun=p_config.force_max_activations_recompute,\n",
    "#     )\n",
    "\n",
    "# # Load max activations\n",
    "# with open(os.path.join(ae_path, \"max_activating_inputs.pkl\"), \"rb\") as f:\n",
    "#     max_activating_inputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load class_accuracies after ablation\n",
    "# def load_class_accuracies(ae_path: str, scoring_method: str):\n",
    "#     assert scoring_method in ['attrib', 'auto_interp']\n",
    "\n",
    "#     filename = f\"{ae_path}/class_accuracies_{scoring_method}.pkl\"\n",
    "#     with open(filename, \"rb\") as f:\n",
    "#             class_accuracies = pickle.load(f)\n",
    "#     return class_accuracies\n",
    "\n",
    "# probe_acc_attrib = load_class_accuracies(ae_path, 'attrib')\n",
    "# probe_autointerp = load_class_accuracies(ae_path, 'auto_interp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load importance scores\n",
    "with open(os.path.join(ae_path, \"node_effects.pkl\"), \"rb\") as f:\n",
    "    scores_attrib = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects_auto_interp.pkl\"), \"rb\") as f:\n",
    "    scores_autointerp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: only 16 non-zero effects found for ablation class 0, which is less than the requested 100.\n",
      "WARNING: only 26 non-zero effects found for ablation class 1, which is less than the requested 100.\n",
      "WARNING: only 37 non-zero effects found for ablation class 2, which is less than the requested 100.\n",
      "WARNING: only 16 non-zero effects found for ablation class 0, which is less than the requested 20.\n",
      "\n",
      "Class 0\n",
      "Attrib: 100\n",
      "AutoInterp: 16\n",
      "Attrib not AutoInterp: 88\n",
      "Should be empty: Top 100 autointerp features which are not in top 100 attrib: 4\n",
      "Should be empty: Top 20 autointerp features which are not in top 100 attrib: 4\n",
      "\n",
      "Class 1\n",
      "Attrib: 100\n",
      "AutoInterp: 26\n",
      "Attrib not AutoInterp: 87\n",
      "Should be empty: Top 100 autointerp features which are not in top 100 attrib: 13\n",
      "Should be empty: Top 20 autointerp features which are not in top 100 attrib: 9\n",
      "\n",
      "Class 2\n",
      "Attrib: 100\n",
      "AutoInterp: 37\n",
      "Attrib not AutoInterp: 76\n",
      "Should be empty: Top 100 autointerp features which are not in top 100 attrib: 13\n",
      "Should be empty: Top 20 autointerp features which are not in top 100 attrib: 6\n"
     ]
    }
   ],
   "source": [
    "# Load top features from importance score files\n",
    "\n",
    "selection_method = FeatureSelection.top_n\n",
    "T_AP = 100 # num_top_features_from_attrib\n",
    "T_LLM = 20 # num_top_features_autointerp\n",
    "class_indices = [0, 1, 2] # Effectively filtering out spurious correlation classes, these are the only TPP available\n",
    "dict_size = next(iter(scores_attrib.values())).shape[0]\n",
    "\n",
    "latent_tensor_attrib_T_AP = select_features(\n",
    "    selection_method=selection_method,\n",
    "    node_effects=scores_attrib,\n",
    "    T_effects=[T_AP],\n",
    "    T_max_sideeffect=None,\n",
    "    dict_size=dict_size,\n",
    ")\n",
    "latent_tensor_autointerp_T_AP = select_features(\n",
    "    selection_method=selection_method,\n",
    "    node_effects=scores_autointerp,\n",
    "    T_effects=[T_AP],\n",
    "    T_max_sideeffect=None,\n",
    "    dict_size=dict_size,\n",
    ")\n",
    "latent_tensor_autointerp_T_LLM = select_features(\n",
    "    selection_method=selection_method,\n",
    "    node_effects=scores_autointerp,\n",
    "    T_effects=[T_LLM],\n",
    "    T_max_sideeffect=None,\n",
    "    dict_size=dict_size,\n",
    ")\n",
    "\n",
    "# Reformat indices\n",
    "top_latent_indices_attrib_T_AP = {\n",
    "    k: v.nonzero().squeeze()\n",
    "    for k, v in latent_tensor_attrib_T_AP[T_AP].items()\n",
    "    if k in class_indices\n",
    "}\n",
    "top_latent_indices_autointerp_T_AP = {\n",
    "    k: v.nonzero().squeeze()\n",
    "    for k, v in latent_tensor_autointerp_T_AP[T_AP].items()\n",
    "    if k in class_indices\n",
    "}\n",
    "top_latent_indices_autointerp_T_LLM = {\n",
    "    k: v.nonzero().squeeze()\n",
    "    for k, v in latent_tensor_autointerp_T_LLM[T_LLM].items()\n",
    "    if k in class_indices\n",
    "}\n",
    "top_latent_indices_not_autointerp = {\n",
    "    k: v[t.isin(v, top_latent_indices_autointerp_T_AP[k], invert=True)]\n",
    "    for k, v in top_latent_indices_attrib_T_AP.items()\n",
    "    if k in class_indices\n",
    "}\n",
    "top_latent_indices_not_attrib_T_AP = {\n",
    "    k: v[t.isin(v, top_latent_indices_attrib_T_AP[k], invert=True)]\n",
    "    for k, v in top_latent_indices_autointerp_T_AP.items()\n",
    "    if k in class_indices\n",
    "} # should be empty\n",
    "top_latent_indices_not_attrib_T_LLM = {\n",
    "    k: v[t.isin(v, top_latent_indices_attrib_T_AP[k], invert=True)]\n",
    "    for k, v in top_latent_indices_autointerp_T_LLM.items()\n",
    "    if k in class_indices\n",
    "} # should be empty\n",
    "\n",
    "for k in class_indices:\n",
    "    print()\n",
    "    print(f'Class {k}')\n",
    "    print(f'Attrib: {len(top_latent_indices_attrib_T_AP[k])}')\n",
    "    print(f'AutoInterp: {len(top_latent_indices_autointerp_T_AP[k])}')\n",
    "    print(f'Attrib not AutoInterp: {len(top_latent_indices_not_autointerp[k])}')\n",
    "    print(f'Should be empty: Top {T_AP} autointerp features which are not in top {T_AP} attrib: {len(top_latent_indices_not_attrib_T_AP[k])}')\n",
    "    print(f'Should be empty: Top {T_LLM} autointerp features which are not in top {T_AP} attrib: {len(top_latent_indices_not_attrib_T_LLM[k])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max act examples per feature\n",
    "\n",
    "# Show max act of autointerp included / rejected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are ablation differences so small?\n",
    "# Attribution patching is a bad approximation and misses indirectly relevant features? Do acutal patching instead?\n",
    "# SAE latents are correlated? Patching a single latent will not have a large effect?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
