{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative assessment of Autointerp for Targeted Probe Perturbation metric\n",
    "For example: An SAE with L0 500 has good performance before auto-interp, and poor performance after. What features are being rejected by the autointerp?\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Probe to perturbate is trained on layer 24 / 26 resid post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments.utils as utils\n",
    "import pickle\n",
    "import os\n",
    "import experiments.autointerp as autointerp\n",
    "from nnsight import LanguageModel\n",
    "from experiments.pipeline_config import PipelineConfig\n",
    "import torch as t\n",
    "\n",
    "from experiments.bib_intervention import select_features\n",
    "from experiments.pipeline_config import FeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available paths: ['../dictionary_learning/dictionaries/gemma-2-2b-saved-data/gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer24_results/resid_post_layer_11/trainer_5']\n",
      "\n",
      "Selecting path: ../dictionary_learning/dictionaries/gemma-2-2b-saved-data/gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer24_results/resid_post_layer_11/trainer_5\n"
     ]
    }
   ],
   "source": [
    "# Define dictionaries\n",
    "\n",
    "DICTIONARIES_PATH = \"../dictionary_learning/dictionaries/gemma-2-2b-saved-data\"\n",
    "trainer_ids = [5]\n",
    "ae_sweep_paths = {\n",
    "    # \"gemma-2-2b_sweep_jumprelu_0902_probe_layer24_results\": {\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "    # \"gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer24_results\": {\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer24_results\": {\n",
    "        \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "    # \"gemma-2-2b_sweep_standard_ctx128_ef2_0824_probe_layer_24_results\": {\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "    # \"gemma-2-2b_sweep_topk_ctx128_ef2_0824_probe_layer_24_results\": {\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "}\n",
    "\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "ae_paths = []\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "\n",
    "    ae_group_paths = utils.get_ae_group_paths(\n",
    "        DICTIONARIES_PATH, sweep_name, submodule_trainers\n",
    "    )\n",
    "    ae_paths.extend(utils.get_ae_paths(ae_group_paths))\n",
    "\n",
    "print(f'available paths: {ae_paths}\\n')\n",
    "ae_path = ae_paths[0]\n",
    "print(f'Selecting path: {ae_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute max activating examples, if they haven't been computed yet\n",
    "\n",
    "# p_config = PipelineConfig()\n",
    "# model_eval_config = utils.ModelEvalConfig.from_sweep_name(sweep_name)\n",
    "# model_name = model_eval_config.full_model_name\n",
    "\n",
    "# llm_batch_size, patching_batch_size, eval_results_batch_size = utils.get_batch_sizes(\n",
    "#     model_eval_config,\n",
    "#     p_config.reduced_GPU_memory,\n",
    "#     p_config.train_set_size,\n",
    "#     p_config.test_set_size,\n",
    "#     p_config.probe_train_set_size,\n",
    "#     p_config.probe_test_set_size,\n",
    "# )\n",
    "\n",
    "# model = LanguageModel(\n",
    "#     model_name,\n",
    "#     device_map=p_config.device,\n",
    "#     dispatch=True,\n",
    "#     attn_implementation=\"eager\",\n",
    "#     torch_dtype=p_config.model_dtype,\n",
    "# )\n",
    "\n",
    "# autointerp.get_autointerp_inputs_for_all_saes(\n",
    "#         model,\n",
    "#         p_config.max_activations_collection_n_inputs,\n",
    "#         llm_batch_size,\n",
    "#         p_config.autointerp_context_length,\n",
    "#         p_config.top_k_inputs_act_collect,\n",
    "#         ae_paths,\n",
    "#         force_rerun=p_config.force_max_activations_recompute,\n",
    "#     )\n",
    "\n",
    "# # Load max activations\n",
    "# with open(os.path.join(ae_path, \"max_activating_inputs.pkl\"), \"rb\") as f:\n",
    "#     max_activating_inputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load class_accuracies after ablation\n",
    "# def load_class_accuracies(ae_path: str, scoring_method: str):\n",
    "#     assert scoring_method in ['attrib', 'auto_interp']\n",
    "\n",
    "#     filename = f\"{ae_path}/class_accuracies_{scoring_method}.pkl\"\n",
    "#     with open(filename, \"rb\") as f:\n",
    "#             class_accuracies = pickle.load(f)\n",
    "#     return class_accuracies\n",
    "\n",
    "# probe_acc_attrib = load_class_accuracies(ae_path, 'attrib')\n",
    "# probe_autointerp = load_class_accuracies(ae_path, 'auto_interp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load importance scores\n",
    "with open(os.path.join(ae_path, \"node_effects.pkl\"), \"rb\") as f:\n",
    "    scores_attrib = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects_auto_interp.pkl\"), \"rb\") as f:\n",
    "    scores_autointerp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: only 8 non-zero effects found for ablation class male / female, which is less than the requested 20.\n",
      "WARNING: only 5 non-zero effects found for ablation class professor / nurse, which is less than the requested 20.\n",
      "WARNING: only 13 non-zero effects found for ablation class male_professor / female_nurse, which is less than the requested 20.\n",
      "WARNING: only 13 non-zero effects found for ablation class biased_male / biased_female, which is less than the requested 20.\n",
      "WARNING: only 0 non-zero effects found for ablation class 0, which is less than the requested 20.\n",
      "WARNING: No non-zero effects found for ablation class 0. Returning an empty mask.\n",
      "WARNING: only 4 non-zero effects found for ablation class 1, which is less than the requested 20.\n",
      "WARNING: only 9 non-zero effects found for ablation class 2, which is less than the requested 20.\n",
      "WARNING: only 2 non-zero effects found for ablation class 6, which is less than the requested 20.\n",
      "\n",
      "Class 0\n",
      "Attrib: 20, tensor([  394,   453,   994,  1834,  1849,  2075,  3650,  4058,  5205,  6082,\n",
      "        10010, 10446, 10627, 12031, 12424, 12954, 13107, 14939, 15263, 17170])\n",
      "AutoInterp: 0, tensor([], dtype=torch.int64)\n",
      "AutoInterp: 0, tensor([], dtype=torch.bool)\n",
      "Attrib not AutoInterp: 20, tensor([  394,   453,   994,  1834,  1849,  2075,  3650,  4058,  5205,  6082,\n",
      "        10010, 10446, 10627, 12031, 12424, 12954, 13107, 14939, 15263, 17170])\n",
      "\n",
      "Class 1\n",
      "Attrib: 20, tensor([  377,   453,  1381,  1849,  2075,  4554,  5163,  5733,  6820,  8040,\n",
      "         8381, 12424, 12954, 12996, 13221, 13346, 13762, 14154, 16645, 17307])\n",
      "AutoInterp: 4, tensor([    1,  6082, 12424, 16645])\n",
      "AutoInterp: 4, tensor([True, True, True, True])\n",
      "Attrib not AutoInterp: 18, tensor([  377,   453,  1381,  1849,  2075,  4554,  5163,  5733,  6820,  8040,\n",
      "         8381, 12954, 12996, 13221, 13346, 13762, 14154, 17307])\n",
      "\n",
      "Class 2\n",
      "Attrib: 20, tensor([  377,   453,   577,  2075,  3648,  3650,  4058,  5826,  6864,  7064,\n",
      "        10302, 10446, 10602, 11246, 12424, 12945, 12996, 14824, 14939, 15214])\n",
      "AutoInterp: 9, tensor([    3,     7,     8,  3913, 10446, 11246, 12945, 14824, 15214])\n",
      "AutoInterp: 9, tensor([True, True, True, True, True, True, True, True, True])\n",
      "Attrib not AutoInterp: 15, tensor([  377,   453,   577,  2075,  3648,  3650,  4058,  5826,  6864,  7064,\n",
      "        10302, 10602, 12424, 12996, 14939])\n",
      "\n",
      "Class 6\n",
      "Attrib: 20, tensor([ 1291,  1590,  1834,  4008,  6082,  6820, 10602, 10627, 10968, 11919,\n",
      "        12424, 12768, 12900, 13107, 14122, 14939, 15263, 15681, 15728, 16290])\n",
      "AutoInterp: 2, tensor([ 8465, 14122])\n",
      "AutoInterp: 2, tensor([True, True])\n",
      "Attrib not AutoInterp: 19, tensor([ 1291,  1590,  1834,  4008,  6082,  6820, 10602, 10627, 10968, 11919,\n",
      "        12424, 12768, 12900, 13107, 14939, 15263, 15681, 15728, 16290])\n"
     ]
    }
   ],
   "source": [
    "# Load top features from importance score files\n",
    "\n",
    "selection_method = FeatureSelection.top_n\n",
    "num_top_features_from_attrib = 20 # aka threshold\n",
    "class_indices = [0, 1, 2, 6] # Effectively filtering out spurious correlation classes, these are the only TPP available\n",
    "dict_size = next(iter(scores_attrib.values())).shape[0]\n",
    "\n",
    "top_latent_tensor_attrib = select_features(\n",
    "    selection_method=selection_method,\n",
    "    node_effects=scores_attrib,\n",
    "    T_effects=[num_top_features_from_attrib],\n",
    "    T_max_sideeffect=None,\n",
    "    dict_size=dict_size,\n",
    ")\n",
    "top_latent_tensor_autointerp = select_features(\n",
    "    selection_method=selection_method,\n",
    "    node_effects=scores_autointerp,\n",
    "    T_effects=[num_top_features_from_attrib],\n",
    "    T_max_sideeffect=None,\n",
    "    dict_size=dict_size,\n",
    ")\n",
    "\n",
    "# Reformat indices\n",
    "top_latent_indices_attrib = {\n",
    "    k: v.nonzero().squeeze()\n",
    "    for k, v in top_latent_tensor_attrib[num_top_features_from_attrib].items()\n",
    "    if k in class_indices\n",
    "}\n",
    "top_latent_indices_autointerp = {\n",
    "    k: v.nonzero().squeeze()\n",
    "    for k, v in top_latent_tensor_autointerp[num_top_features_from_attrib].items()\n",
    "    if k in class_indices\n",
    "}\n",
    "top_latent_indices_not_autointerp = {\n",
    "    k: v[t.isin(v, top_latent_indices_autointerp[k], invert=True)]\n",
    "    for k, v in top_latent_indices_attrib.items()\n",
    "    if k in class_indices\n",
    "}\n",
    "\n",
    "for k in class_indices:\n",
    "    print()\n",
    "    print(f'Class {k}')\n",
    "    print(f'Attrib: {len(top_latent_indices_attrib[k])}, {top_latent_indices_attrib[k]}')\n",
    "    print(f'AutoInterp: {len(top_latent_indices_autointerp[k])}, {top_latent_indices_autointerp[k]}')\n",
    "    print(f'AutoInterp: {len(top_latent_indices_autointerp[k])}, {top_latent_tensor_autointerp[num_top_features_from_attrib][k][top_latent_indices_autointerp[k]]}')\n",
    "    print(f'Attrib not AutoInterp: {len(top_latent_indices_not_autointerp[k])}, {(top_latent_indices_not_autointerp[k])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max act examples per feature\n",
    "\n",
    "# Show max act of autointerp included / rejected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are ablation differences so small?\n",
    "# Attribution patching is a bad approximation and misses indirectly relevant features? Do acutal patching instead?\n",
    "# SAE latents are correlated? Patching a single latent will not have a large effect?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
