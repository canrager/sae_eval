{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "activation_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "\n",
    "# data preparation hyperparameters\n",
    "batch_size = 128\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for professions\n",
    "profession_dict = {\n",
    "    'accountant': 0,\n",
    "    'architect': 1,\n",
    "    'attorney': 2,\n",
    "    'chiropractor': 3,\n",
    "    'comedian': 4,\n",
    "    'composer': 5,\n",
    "    'dentist': 6,\n",
    "    'dietitian': 7,\n",
    "    'dj': 8,\n",
    "    'filmmaker': 9,\n",
    "    'interior_designer': 10,\n",
    "    'journalist': 11,\n",
    "    'model': 12,\n",
    "    'nurse': 13,\n",
    "    'painter': 14,\n",
    "    'paralegal': 15,\n",
    "    'pastor': 16,\n",
    "    'personal_trainer': 17,\n",
    "    'photographer': 18,\n",
    "    'physician': 19,\n",
    "    'poet': 20,\n",
    "    'professor': 21,\n",
    "    'psychologist': 22,\n",
    "    'rapper': 23,\n",
    "    'software_engineer': 24,\n",
    "    'surgeon': 25,\n",
    "    'teacher': 26,\n",
    "    'yoga_teacher': 27\n",
    "}\n",
    "\n",
    "# Reverse the profession dictionary for easy lookup\n",
    "profession_dict_rev = {v: k for k, v in profession_dict.items()}\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Create a combined label column for (multiclass x binary)\n",
    "df['combined_label'] = df['profession'].astype(str) + '_' + df['gender'].astype(str)\n",
    "\n",
    "# Plot the number of samples per (multiclass x binary) label\n",
    "label_counts = df['combined_label'].value_counts().sort_index()\n",
    "smallest_label_count = label_counts.min()\n",
    "print(f'Smallest label count: {smallest_label_count}')\n",
    "\n",
    "# Create labels with profession names and gender\n",
    "labels = [profession_dict_rev[int(label.split('_')[0])] + ' (Male)' if label.split('_')[1] == '0' \n",
    "            else profession_dict_rev[int(label.split('_')[0])] + ' (Female)' for label in label_counts.index]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(labels, label_counts)\n",
    "plt.xlabel('(Profession x Gender) Label')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Number of Samples per (Profession x Gender) Label')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_per_group = 1024\n",
    "\n",
    "# Balance the dataset within each profession\n",
    "balanced_df_list = []\n",
    "for profession in df['profession'].unique():\n",
    "    prof_df = df[df['profession'] == profession]\n",
    "    min_count = prof_df['gender'].value_counts().min()\n",
    "    if min_count >= min_samples_per_group:    \n",
    "        balanced_prof_df = prof_df.groupby('gender').apply(lambda x: x.sample(n=min_samples_per_group)).reset_index(drop=True)\n",
    "        balanced_df_list.append(balanced_prof_df)\n",
    "\n",
    "balanced_df = pd.concat(balanced_df_list).reset_index(drop=True)\n",
    "\n",
    "# Shuffle per profession\n",
    "grouped = balanced_df.groupby('profession')['hard_text'].apply(list)\n",
    "bios_gender_balanced = {}\n",
    "\n",
    "for label, texts in grouped.items():\n",
    "    shuffled_texts = shuffle(texts)\n",
    "    bios_gender_balanced[label] = shuffled_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_classes(data_dict, chosen_class):\n",
    "    # Step 1: Generate a random list of class indices\n",
    "    total_samples = len(data_dict[chosen_class])\n",
    "    all_classes = list(data_dict.keys())\n",
    "    all_classes.remove(chosen_class)\n",
    "    \n",
    "    random_class_indices = random.choices(all_classes, k=total_samples)\n",
    "    \n",
    "    # Step 2: Count the number of samples to draw from each class index\n",
    "    samples_count = defaultdict(int)\n",
    "    for class_idx in random_class_indices:\n",
    "        samples_count[class_idx] += 1\n",
    "    \n",
    "    # Step 3: Uniformly sample the required amount of samples without replacement\n",
    "    sampled_data = []\n",
    "    for class_idx, count in samples_count.items():\n",
    "        sampled_data.extend(random.sample(data_dict[class_idx], count))\n",
    "    \n",
    "    return sampled_data\n",
    "\n",
    "def create_labeled_dataset(data_dict, chosen_class, batch_size):\n",
    "    in_class_data = data_dict[chosen_class]\n",
    "    other_class_data = sample_from_classes(data_dict, chosen_class)\n",
    "\n",
    "    # Step 1: Label the datasets\n",
    "    in_class_labeled = [(sample, 0) for sample in in_class_data]\n",
    "    other_class_labeled = [(sample, 1) for sample in other_class_data]\n",
    "\n",
    "    # Step 2: Concatenate the datasets\n",
    "    combined_dataset = in_class_labeled + other_class_labeled\n",
    "\n",
    "    # Step 3: Shuffle the combined dataset\n",
    "    random.shuffle(combined_dataset)\n",
    "    bio_texts, bio_labels = zip(*combined_dataset)\n",
    "    text_batches = [bio_texts[i:i + batch_size] for i in range(0, len(combined_dataset), batch_size)]\n",
    "    label_batches = [t.tensor(bio_labels[i:i + batch_size], device=DEVICE) for i in range(0, len(combined_dataset), batch_size)]\n",
    "\n",
    "    return text_batches, label_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "\n",
    "layer = 4 # model layer for attaching linear classification head\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probe(text_batches, label_batches, get_acts, lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to(DEVICE)\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = t.zeros(epochs * len(text_batches))\n",
    "    batch_idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        for text, labels in zip(text_batches, label_batches):\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, t.tensor(labels, device=DEVICE, dtype=t.float32))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses[batch_idx] = loss\n",
    "            batch_idx += 1\n",
    "    return probe, losses\n",
    "\n",
    "def get_acts(text):\n",
    "    with t.no_grad(): \n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.input[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes, losses = {}, {}\n",
    "\n",
    "for profession in bios_gender_balanced.keys():\n",
    "    t.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f'Training probe for profession: {profession}')\n",
    "    text_batches, label_batches = create_labeled_dataset(bios_gender_balanced, profession, batch_size)\n",
    "    probe, loss = train_probe(\n",
    "        text_batches,\n",
    "        label_batches,\n",
    "        get_acts,\n",
    "        epochs=1\n",
    "    )\n",
    "    probes[profession] = probe\n",
    "    losses[profession] = loss\n",
    "\n",
    "# make subfolder for saving\n",
    "os.makedirs('trained_bib_probes', exist_ok=True)\n",
    "\n",
    "# save probes, losses\n",
    "t.save(probes, 'trained_bib_probes/probes_0705.pt')\n",
    "t.save(losses, 'trained_bib_probes/losses_0705.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_probe(text_batches, label_batches, probe, get_acts, label_idx=0, seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for text, labels in zip(text_batches, label_batches):\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
