{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path: str) -> Dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def load_pkl(file_path: str) -> Dict:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "file1 = 'llm_autointerp/manual_labels_can_final.json'\n",
    "file2 = 'llm_autointerp/llm_results_can_final_sonnet.pkl'\n",
    "\n",
    "manual_file = load_json(file1)\n",
    "llm_file = load_pkl(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 1\n",
    "\n",
    "print(f'##### Example Prompts\\n {manual_file[str(sample_idx)]['example_prompts']}\\n\\n')\n",
    "print(f'##### Manual chain of thought\\n{manual_file[str(sample_idx)]['chain_of_thought']}\\n\\n')\n",
    "print(f'##### LLM chain of thought\\n{llm_file[sample_idx][0]}\\n\\n')\n",
    "print(f'manual labels {manual_file[str(sample_idx)]['per_class_scores']}')\n",
    "print(f'LLM labels    {llm_file[sample_idx][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores_manual(data: Dict, is_valid: List[bool]) -> Dict[str, List[int]]:\n",
    "    manual_labels = {}\n",
    "    for i, item in enumerate(data.values()):\n",
    "        if is_valid[i]:\n",
    "            for category, score in item['per_class_scores'].items():\n",
    "                if category not in manual_labels:\n",
    "                    manual_labels[category] = []\n",
    "                manual_labels[category].append(score)\n",
    "    return manual_labels\n",
    "\n",
    "def extract_scores_llm(data: List[Tuple[str, Dict[str, int], bool, str]]) -> Dict[str, List[int]]:\n",
    "    is_valid = []\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        scores = item[1]  # The scores dictionary is the second element of each tuple\n",
    "        if scores is None:\n",
    "            is_valid.append(False)\n",
    "        else:\n",
    "            is_valid.append(True)\n",
    "            for category, score in scores.items():\n",
    "                if category not in result:\n",
    "                    result[category] = []\n",
    "                result[category].append(score)\n",
    "    return result, is_valid\n",
    "\n",
    "\n",
    "llm_labels, is_valid = extract_scores_llm(llm_file)\n",
    "manual_labels = extract_scores_manual(manual_file, is_valid)\n",
    "\n",
    "print(llm_labels['gender'][:10])\n",
    "print(manual_labels['gender'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores_manual(data: Dict, is_valid: List[bool]) -> Dict[str, List[int]]:\n",
    "    manual_labels = {}\n",
    "    for i, item in enumerate(data.values()):\n",
    "        if is_valid[i]:\n",
    "            for category, score in item['per_class_scores'].items():\n",
    "                if category not in manual_labels:\n",
    "                    manual_labels[category] = []\n",
    "                manual_labels[category].append(score)\n",
    "    return manual_labels\n",
    "\n",
    "def extract_scores_llm(data: List[Tuple[str, Dict[str, int], bool, str]]) -> Dict[str, List[int]]:\n",
    "    is_valid = []\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        scores = item[1]  # The scores dictionary is the second element of each tuple\n",
    "        if scores is None:\n",
    "            is_valid.append(False)\n",
    "        else:\n",
    "            is_valid.append(True)\n",
    "            for category, score in scores.items():\n",
    "                if category not in result:\n",
    "                    result[category] = []\n",
    "                result[category].append(score)\n",
    "    return result, is_valid\n",
    "\n",
    "def cohens_kappa(scores1: Dict[str, List[int]], scores2: Dict[str, List[int]]) -> Dict[str, float]:\n",
    "    def kappa(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        n = len(a)\n",
    "        categories = np.unique(np.concatenate([a, b]))\n",
    "        n_categories = len(categories)\n",
    "        \n",
    "        # Observed agreement\n",
    "        observed = np.sum(a == b) / n\n",
    "        \n",
    "        # Expected agreement\n",
    "        expected = sum((np.sum(a == i) / n) * (np.sum(b == i) / n) for i in categories)\n",
    "        \n",
    "        # Compute kappa\n",
    "        kappa = (observed - expected) / (1 - expected + EPS)\n",
    "        return kappa\n",
    "\n",
    "    results = {}\n",
    "    for category in scores1.keys():\n",
    "        a = np.array(scores1[category])\n",
    "        b = np.array(scores2[category])\n",
    "        results[category] = kappa(a, b)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_kappa_for_files(file1: str, file2: str) -> Dict[str, float]:\n",
    "    manual_labels = load_json(file1)\n",
    "    llm_labels = load_pkl(file2)\n",
    "\n",
    "    print(f'Length of manual labels: {len(manual_labels)}')\n",
    "    print(f'Length of LLM labels: {len(llm_labels)}')\n",
    "\n",
    "    # Find overlapping keys\n",
    "    # overlap = set(data1.keys()) & set(data2.keys())\n",
    "    # print(f'Number of shared keys: {len(overlap)}')\n",
    "    # data1_overlap, data2_overlap = {}, {}\n",
    "    # for key in overlap:\n",
    "    #     data1_overlap[key] = data1[key]\n",
    "    #     data2_overlap[key] = data2[key]\n",
    "\n",
    "\n",
    "\n",
    "    scores_llm, is_valid_llm_output = extract_scores_llm(llm_labels)\n",
    "    print(f'Number of invalid valid scores: {len(is_valid_llm_output) - sum(is_valid_llm_output)}')\n",
    "    scores_manual = extract_scores_manual(manual_labels, is_valid_llm_output)\n",
    "    \n",
    "    return cohens_kappa(scores_llm, scores_manual)\n",
    "\n",
    "\n",
    "kappa_scores = compute_kappa_for_files(file1, file2)\n",
    "\n",
    "print(\"Cohen's Kappa scores for each category:\")\n",
    "for category, score in kappa_scores.items():\n",
    "    print(f\"{category}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
