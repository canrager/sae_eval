{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autointerp for SHIFT evals\n",
    "LLM judge decides whether a neuron/latent is related to a natural language concept. Inputs to Autointerp LLM judge:\n",
    "- Max activating examples\n",
    "- DLA top promoted tokens\n",
    "\n",
    "\n",
    "### Functionality of this notebook\n",
    "Inputs: \n",
    "- model, datset, dictionaries\n",
    "- list of concepts to check whether it is related to sth.\n",
    "\n",
    "Outputs:\n",
    "- node_effects.pkl per dictionary per concept with binary yes/no decision on whether feature is related to prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "import datasets\n",
    "import anthropic\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import experiments.utils as utils\n",
    "from experiments.autointerp import (\n",
    "    get_max_activating_prompts, \n",
    "    compute_dla, \n",
    "    format_examples,\n",
    "    evaluate_binary_llm_output\n",
    ")\n",
    "from experiments.llm_autointerp.prompt_builder import build_prompt\n",
    "from experiments.llm_autointerp.prompts import build_system_prompt\n",
    "\n",
    "DEBUGGING = True\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securely input the API key\n",
    "api_key = input(\"Enter your API key: \")\n",
    "os.environ['ANTHROPIC_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model_dtype = t.bfloat16\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=DEVICE,\n",
    "    dispatch=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=model_dtype,\n",
    ")\n",
    "model_unembed = model.embed_out # For direct logit attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "num_contexts = 10000\n",
    "context_length = 128\n",
    "batch_size = 250\n",
    "\n",
    "dataset = datasets.load_dataset(\"georgeyw/dsir-pile-100k\", streaming=False)\n",
    "data = model.tokenizer(dataset[\"train\"][\"contents\"][:num_contexts], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=context_length).to(DEVICE).data\n",
    "batched_data = utils.batch_inputs(data, batch_size)\n",
    "\n",
    "\n",
    "# Class specific data\n",
    "\n",
    "profession_dict = {\n",
    "    \"accountant\": 0, \"architect\": 1, \"attorney\": 2, \"chiropractor\": 3,\n",
    "    \"comedian\": 4, \"composer\": 5, \"dentist\": 6, \"dietitian\": 7,\n",
    "    \"dj\": 8, \"filmmaker\": 9, \"interior_designer\": 10, \"journalist\": 11,\n",
    "    \"model\": 12, \"nurse\": 13, \"painter\": 14, \"paralegal\": 15,\n",
    "    \"pastor\": 16, \"personal_trainer\": 17, \"photographer\": 18, \"physician\": 19,\n",
    "    \"poet\": 20, \"professor\": 21, \"psychologist\": 22, \"rapper\": 23,\n",
    "    \"software_engineer\": 24, \"surgeon\": 25, \"teacher\": 26, \"yoga_teacher\": 27,\n",
    "    \"profession\": -4, \"gender\": -2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary\n",
    "\n",
    "dictionaries_path = \"../dictionary_learning/dictionaries\"\n",
    "\n",
    "# Current recommended way to generate graphs. You can copy paste ae_sweep_paths directly from bib_intervention.py\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\"resid_post_layer_3\": {\"trainer_ids\": [6]}},\n",
    "    # \"pythia70m_sweep_gated_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [9]}},\n",
    "    # \"pythia70m_sweep_topk_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [10]}},\n",
    "    # \"gemma-2-2b_sweep_topk_ctx128_0817\": {\"resid_post_layer_12\": {\"trainer_ids\": [2]}}, \n",
    "}\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(dictionaries_path, sweep_name, submodule_trainers)\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "ae_path = ae_paths[0]\n",
    "submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.autointerp import get_autointerp_inputs_for_all_saes\n",
    "\n",
    "all_latent_indices = t.arange(dictionary.dict_size)\n",
    "k_inputs_per_feature = 5\n",
    "\n",
    "get_autointerp_inputs_for_all_saes(\n",
    "    model, \n",
    "    n_inputs=num_contexts,\n",
    "    batch_size=batch_size,\n",
    "    context_length=context_length,\n",
    "    top_k_inputs=k_inputs_per_feature,\n",
    "    ae_paths=ae_paths,\n",
    "    force_rerun=False,\n",
    ")\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "with open(os.path.join(ae_path, \"max_activating_inputs.pkl\"), \"rb\") as f:\n",
    "    file = pickle.load(f)\n",
    "    \n",
    "max_token_idxs_FKL = file[\"max_tokens_FKL\"]\n",
    "max_activations_FKL = file[\"max_activations_FKL\"]\n",
    "top_dla_token_idxs_FK = file[\"dla_results_FK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format max_activating_inputs by << emphasizing>> max act examples\n",
    "# moving max token idxs to device is four times as slow as leaving it on CPU\n",
    "\n",
    "num_top_emphasized_tokens = 5\n",
    "formatting_batch_size = len(max_token_idxs_FKL)\n",
    "formatting_batch_size = 100\n",
    "\n",
    "example_prompts = format_examples(model.tokenizer, max_token_idxs_FKL[:formatting_batch_size], max_activations_FKL[:formatting_batch_size], num_top_emphasized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.llm_autointerp.prompts import build_system_prompt, create_few_shot_examples, create_unlabeled_prompts\n",
    "from experiments import utils_bib_dataset\n",
    "\n",
    "system_prompt = build_system_prompt(\n",
    "    concepts=list(utils_bib_dataset.profession_dict.keys()),\n",
    ")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DIR = \"llm_autointerp\"\n",
    "import json\n",
    "\n",
    "with open(f\"{PROMPT_DIR}/manual_labels_few_shot.json\", \"r\") as f:\n",
    "        few_shot_manual_labels = json.load(f)\n",
    "\n",
    "few_shot_examples = create_few_shot_examples(few_shot_manual_labels);\n",
    "print(few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dla_token_strs_FK = utils.list_decode(top_dla_token_idxs_FK, model.tokenizer)\n",
    "\n",
    "create_unlabeled_prompts(example_prompts, top_dla_token_strs_FK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_latent_indices = t.arange(dictionary.dict_size)\n",
    "k_inputs_per_feature = 5\n",
    "\n",
    "get_autointerp_inputs_for_all_saes(\n",
    "    model, \n",
    "    n_inputs=num_contexts,\n",
    "    batch_size=batch_size,\n",
    "    context_length=context_length,\n",
    "    top_k_inputs=k_inputs_per_feature,\n",
    "    ae_paths=ae_paths,\n",
    "    force_rerun=False,\n",
    ")\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "with open(os.path.join(ae_path, \"max_activating_inputs.pkl\"), \"rb\") as f:\n",
    "    file = pickle.load(f)\n",
    "    \n",
    "max_token_idxs_FKL = file[\"max_tokens_FKL\"]\n",
    "max_activations_FKL = file[\"max_activations_FKL\"]\n",
    "top_dla_token_idxs_FK = file[\"dla_results_FK\"]\n",
    "\n",
    "with open(f\"llm_autointerp/manual_labels_few_shot.json\", \"r\") as f:\n",
    "    few_shot_manual_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.llm_autointerp.prompts import build_system_prompt, create_few_shot_examples, create_unlabeled_prompts\n",
    "from experiments import utils_bib_dataset\n",
    "from experiments.llm_autointerp import llm_utils\n",
    "from experiments.autointerp import get_autointerp_inputs_for_all_saes\n",
    "import json\n",
    "\n",
    "def node_effects_autointerp(model, node_effects_classprobe, max_token_idxs_FKL, max_activations_FKL, top_dla_token_idxs_FK, few_shot_manual_labels, num_top_features_from_probe=5, chosen_class_idxs):  \n",
    "\n",
    "\n",
    "\n",
    "    num_top_emphasized_tokens = 5\n",
    "    formatting_batch_size = len(max_token_idxs_FKL)\n",
    "    formatting_batch_size = 100\n",
    "\n",
    "    system_prompt = build_system_prompt(concepts=list(utils_bib_dataset.profession_dict.keys()),)\n",
    "    few_shot_examples = create_few_shot_examples(few_shot_manual_labels)\n",
    "\n",
    "    unlabeled_prompts = format_examples(model.tokenizer, max_token_idxs_FKL[:formatting_batch_size], max_activations_FKL[:formatting_batch_size], num_top_emphasized_tokens)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Few shot example is using {llm_utils.count_tokens(few_shot_examples)} tokens\")\n",
    "    print(f\"System prompt is using {llm_utils.count_tokens(system_prompt[0]['text'])} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO prompt caching\n",
    "# TODO parallelize LLM inference\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "llm_outputs_direct_prompt = []\n",
    "for i, messages in tqdm(enumerate(prompts), desc=\"LLM inference\", total=len(prompts)):\n",
    "\n",
    "    # barrier for testing\n",
    "    if i == 5: \n",
    "        print(\"stopping LLM inference early for testing\")\n",
    "        break\n",
    "\n",
    "    llm_out = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=system_prompt,\n",
    "        messages=messages,\n",
    "    )\n",
    "    llm_outputs_direct_prompt.append(llm_out.content)\n",
    "\n",
    "print(f'Total number of LLM outputs: {len(llm_outputs_direct_prompt)}\\n')\n",
    "llm_outputs_direct_prompt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for out in llm_outputs_direct_prompt:\n",
    "    print(f'{out[0].text.split('yes_or_no_decisions = ')[-1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction with Instructor\n",
    "\n",
    "import instructor # pip install -U instructor\n",
    "from anthropic import Anthropic\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Decisions(BaseModel):\n",
    "    gender: str\n",
    "    professors: str\n",
    "    nurses: str\n",
    "\n",
    "\n",
    "client = instructor.from_anthropic(Anthropic())\n",
    "\n",
    "llm_outputs_instructor = []\n",
    "for i, messages in tqdm(enumerate(prompts), desc=\"LLM inference\", total=len(prompts)):\n",
    "\n",
    "    # barrier for testing\n",
    "    if i == 5: \n",
    "        print(\"stopping LLM inference early for testing\")\n",
    "        break\n",
    "\n",
    "    resp = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=system_prompt,\n",
    "        messages=messages,\n",
    "        response_model=Decisions,\n",
    "    )\n",
    "    llm_outputs_instructor.append(resp)\n",
    "\n",
    "print(f'Total number of LLM outputs: {len(llm_outputs_instructor)}\\n')\n",
    "llm_outputs_instructor[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note for the simple example of gender, doctor, nurse; Prompting with Instructor yielded a different result than direct prompting! I do not fully trust Instructor and would default to regex, if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern matching\n",
    "\n",
    "def extract_and_convert_json(input_string):\n",
    "    # Regular expression to find JSON-like dictionaries\n",
    "    match = re.search(r'\\{.*?\\}', input_string)\n",
    "    \n",
    "    if match:\n",
    "        json_string = match.group(0)\n",
    "        # Convert the extracted string to a dictionary\n",
    "        return ast.literal_eval(json_string)\n",
    "    else:\n",
    "        raise ValueError(\"No JSON-like dictionary found in the input string.\")\n",
    "\n",
    "llm_outputs_json = []\n",
    "for out in llm_outputs_direct_prompt:\n",
    "    out = out[0].text\n",
    "    json_str = out.split('yes_or_no_decisions')[-1]\n",
    "    yes_or_no_decisions = extract_and_convert_json(json_str)\n",
    "    llm_outputs_json.append(yes_or_no_decisions)\n",
    "\n",
    "llm_outputs_json[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE assumes that feature_idx starts from 0\n",
    "# NOTE node_effects does currently not contain tensors\n",
    "# NOTE we currently do not check whether all classes are contained in all llm json outputs, this could lead to feature idx mismatching\n",
    "\n",
    "node_effects = defaultdict(list)\n",
    "for feat_idx, decisions in enumerate(llm_outputs_json):\n",
    "    for class_name, decision in decisions.items():\n",
    "        class_idx = profession_dict[class_name]\n",
    "        decision_bool = evaluate_binary_llm_output(decision)\n",
    "        node_effects[class_idx].append(decision_bool)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects_autointerp.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(node_effects, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
