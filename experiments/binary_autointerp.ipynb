{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autointerp for SHIFT evals\n",
    "LLM judge decides whether a neuron/latent is related to a natural language concept. Inputs to Autointerp LLM judge:\n",
    "- Max activating examples\n",
    "- DLA top promoted tokens\n",
    "\n",
    "\n",
    "### Functionality of this notebook\n",
    "Inputs: \n",
    "- model, datset, dictionaries\n",
    "- list of concepts to check whether it is related to sth.\n",
    "\n",
    "Outputs:\n",
    "- node_effects.pkl per dictionary per concept with binary yes/no decision on whether feature is related to prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "import datasets\n",
    "import anthropic\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import experiments.utils as utils\n",
    "from experiments.autointerp import (\n",
    "    get_max_activating_prompts, \n",
    "    compute_dla, \n",
    "    format_examples,\n",
    "    evaluate_binary_llm_output\n",
    ")\n",
    "from experiments.explainers.simple.prompt_builder import build_prompt\n",
    "from experiments.explainers.simple.prompts import build_system_prompt\n",
    "\n",
    "DEBUGGING = True\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securely input the API key\n",
    "api_key = input(\"Enter your API key: \")\n",
    "os.environ['ANTHROPIC_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model_dtype = t.bfloat16\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=DEVICE,\n",
    "    dispatch=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=model_dtype,\n",
    ")\n",
    "model_unembed = model.embed_out # For direct logit attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "num_contexts = 10000\n",
    "context_length = 128\n",
    "batch_size = 250\n",
    "\n",
    "dataset = datasets.load_dataset(\"georgeyw/dsir-pile-100k\", streaming=False)\n",
    "data = model.tokenizer(dataset[\"train\"][\"contents\"][:num_contexts], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=context_length).to(DEVICE).data\n",
    "batched_data = utils.batch_inputs(data, batch_size)\n",
    "\n",
    "\n",
    "# Class specific data\n",
    "\n",
    "profession_dict = {\n",
    "    \"accountant\": 0, \"architect\": 1, \"attorney\": 2, \"chiropractor\": 3,\n",
    "    \"comedian\": 4, \"composer\": 5, \"dentist\": 6, \"dietitian\": 7,\n",
    "    \"dj\": 8, \"filmmaker\": 9, \"interior_designer\": 10, \"journalist\": 11,\n",
    "    \"model\": 12, \"nurse\": 13, \"painter\": 14, \"paralegal\": 15,\n",
    "    \"pastor\": 16, \"personal_trainer\": 17, \"photographer\": 18, \"physician\": 19,\n",
    "    \"poet\": 20, \"professor\": 21, \"psychologist\": 22, \"rapper\": 23,\n",
    "    \"software_engineer\": 24, \"surgeon\": 25, \"teacher\": 26, \"yoga_teacher\": 27,\n",
    "    \"profession\": -4, \"gender\": -2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary from ../dictionary_learning/dictionaries/pythia70m_sweep_standard_ctx128_0712/resid_post_layer_3/trainer_6\n"
     ]
    }
   ],
   "source": [
    "# Load dictionary\n",
    "\n",
    "dictionaries_path = \"../dictionary_learning/dictionaries\"\n",
    "\n",
    "# Current recommended way to generate graphs. You can copy paste ae_sweep_paths directly from bib_intervention.py\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\"resid_post_layer_3\": {\"trainer_ids\": [6]}},\n",
    "    # \"pythia70m_sweep_gated_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [9]}},\n",
    "    # \"pythia70m_sweep_topk_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [10]}},\n",
    "    # \"gemma-2-2b_sweep_topk_ctx128_0817\": {\"resid_post_layer_12\": {\"trainer_ids\": [2]}}, \n",
    "}\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(dictionaries_path, sweep_name, submodule_trainers)\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "ae_path = ae_paths[0]\n",
    "submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:07<00:00,  5.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get max_activating_inputs \n",
    "# and direct logit attribution (DLA) scores per feature\n",
    "\n",
    "# all_latent_indices = t.arange(dictionary.dict_size)\n",
    "# all_latent_indices = t.tensor([0])\n",
    "all_latent_indices = t.arange(0, 10)\n",
    "k_inputs_per_feature = 10\n",
    "\n",
    "max_token_idxs_FKL, max_activations_FKL = get_max_activating_prompts(\n",
    "    model, \n",
    "    submodule, \n",
    "    batched_data, \n",
    "    all_latent_indices, \n",
    "    batch_size, \n",
    "    dictionary, \n",
    "    k=k_inputs_per_feature,\n",
    "    context_length=context_length\n",
    ")\n",
    "\n",
    "# TODO write out max activating prompts to a file\n",
    "\n",
    "top_dla_token_idxs_FK = compute_dla(\n",
    "    all_latent_indices,\n",
    "    dictionary.decoder.weight,\n",
    "    model_unembed.weight,\n",
    "    k_inputs_per_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format max_activating_inputs by << emphasizing>> max act examples\n",
    "\n",
    "num_top_emphasized_tokens = 5\n",
    "\n",
    "example_prompts = format_examples(model, max_token_idxs_FKL, max_activations_FKL, num_top_emphasized_tokens)\n",
    "top_dla_tokens_FK = model.tokenizer.batch_decode(top_dla_token_idxs_FK, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = build_system_prompt(\n",
    "    cot=False,\n",
    "    concepts=list(profession_dict.keys()),\n",
    "    logits=True\n",
    ")\n",
    "\n",
    "prompts = []\n",
    "for example_prompt, top_dla_tokens_K in zip(example_prompts, top_dla_tokens_FK):\n",
    "    message = build_prompt(\n",
    "        examples=example_prompt,\n",
    "        cot=False,\n",
    "        top_logits=top_dla_tokens_K,\n",
    "    )\n",
    "    prompts.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text',\n",
       "  'text': 'You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and decide whether its behavior is related to a concept for each concept in (accountant, architect, attorney, chiropractor, comedian, composer, dentist, dietitian, dj, filmmaker, interior_designer, journalist, model, nurse, painter, paralegal, pastor, personal_trainer, photographer, physician, poet, professor, psychologist, rapper, software_engineer, surgeon, teacher, yoga_teacher, profession, gender).\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nYou will also be shown a list called Top_logits. The logits promoted by the neuron shed light on how the neuron\\'s activation influences the model\\'s predictions or outputs. Look at this list of Top_logits and refine your hypotheses from part 1. It is possible that this list is more informative than the examples from part 1.\\n\\nPay close attention to the words in this list and write down what they have in common. Then look at what they have in common, as well as patterns in the tokens you found in Part 1, to produce a single explanation for what features of text cause the neuron to activate. Propose your explanation in the following format:\\nyes_or_no_decisions = {\"accountant\": \"your_decision\", \"architect\": \"your_decision\", \"attorney\": \"your_decision\", \"chiropractor\": \"your_decision\", \"comedian\": \"your_decision\", \"composer\": \"your_decision\", \"dentist\": \"your_decision\", \"dietitian\": \"your_decision\", \"dj\": \"your_decision\", \"filmmaker\": \"your_decision\", \"interior_designer\": \"your_decision\", \"journalist\": \"your_decision\", \"model\": \"your_decision\", \"nurse\": \"your_decision\", \"painter\": \"your_decision\", \"paralegal\": \"your_decision\", \"pastor\": \"your_decision\", \"personal_trainer\": \"your_decision\", \"photographer\": \"your_decision\", \"physician\": \"your_decision\", \"poet\": \"your_decision\", \"professor\": \"your_decision\", \"psychologist\": \"your_decision\", \"rapper\": \"your_decision\", \"software_engineer\": \"your_decision\", \"surgeon\": \"your_decision\", \"teacher\": \"your_decision\", \"yoga_teacher\": \"your_decision\", \"profession\": \"your_decision\", \"gender\": \"your_decision\"}\\nGuidelines:\\n\\nYou will be given a list of text examples on which the neuron activates. The specific tokens which cause the neuron to activate will appear between delimiters like <<this>>. The activation value of the token is given after each token in parentheses like <<this>>(3).\\n\\n- For each concept in (accountant, architect, attorney, chiropractor, comedian, composer, dentist, dietitian, dj, filmmaker, interior_designer, journalist, model, nurse, painter, paralegal, pastor, personal_trainer, photographer, physician, poet, professor, psychologist, rapper, software_engineer, surgeon, teacher, yoga_teacher, profession, gender), try to judge whether the neurons behavior is related to the concept. Simply make a choice based on the text features that activate the neuron, and what its role might be based on the tokens it predicts.\\n- If part of the text examples or predicited tokens are incorrectly formatted, please ignore them.\\n- If you are not able to find any coherent description of the neurons behavior, decide that the neuron is not related to any concept.\\n- The last line of your response must be your binary decisions, yes or no in the following format: yes_or_no_decisions = {\"accountant\": \"your_decision\", \"architect\": \"your_decision\", \"attorney\": \"your_decision\", \"chiropractor\": \"your_decision\", \"comedian\": \"your_decision\", \"composer\": \"your_decision\", \"dentist\": \"your_decision\", \"dietitian\": \"your_decision\", \"dj\": \"your_decision\", \"filmmaker\": \"your_decision\", \"interior_designer\": \"your_decision\", \"journalist\": \"your_decision\", \"model\": \"your_decision\", \"nurse\": \"your_decision\", \"painter\": \"your_decision\", \"paralegal\": \"your_decision\", \"pastor\": \"your_decision\", \"personal_trainer\": \"your_decision\", \"photographer\": \"your_decision\", \"physician\": \"your_decision\", \"poet\": \"your_decision\", \"professor\": \"your_decision\", \"psychologist\": \"your_decision\", \"rapper\": \"your_decision\", \"software_engineer\": \"your_decision\", \"surgeon\": \"your_decision\", \"teacher\": \"your_decision\", \"yoga_teacher\": \"your_decision\", \"profession\": \"your_decision\", \"gender\": \"your_decision\"}'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1: let her crown rest in his lap.With a flick of a wrist, the holograms multiplied and surrounded him in pairs of two, collecting the wavelength data of both counterparts.While he pressed several instructions into the screens, the king idly explained to both near men, \"Make sure no one interrupts me while I\\'m in this state of comat <<ose>>, understood?\" Both gave a shaky nod of confirmation.With a series of deep breaths, Munto cleared his thoughts and attempted to even his rapid pulse. His hands hovered over each side of her vulnerable temple. He took one last glance at a particular grid, which\\n\\n\\n\\nTop_logits: ������                �\\n                                   �'}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"\\nExample 1: for retrograde cerebral perfusion were placed. With systemic temperature of 18° <<C>>, deep hypothermic circulatory arrest was initiated. Retrograde cerebral perfusion via the SVC cannula was established at 10\\u2009 <<mL>>/ <<kg>> <</>> <<min>>. Upon direct inspection of the aorta, the intimal tear was noted to be at the sinotubular junction (STJ), with slight extension into the noncoronary sinus. The sinuses were not significantly dilated and the aortic valve demonstrated normal leaflet coaptation. The repair was performed with replacement of the ascending aorta and hemiarch from the STJ, with resuspension of aortic\\n\\nExample 2: Oxytocin affects utilization of noradrenaline in distinct limbic-forebrain regions of the rat brain.\\nThe effects of oxytocin, administered intracerebroventricularly in doses of 1, 10, << 100>> and << 1000>> << pm>> <<ol>> <<,>> were studied on the disappearance of catecholamines induced by alpha-methyl-p-tyrosine in microdissected nuclei of the rat brain. Oxytocin dose-dependently decreased the utilization of noradrenaline in the lateral and medial septal nuclei and anterior hypothalamic area, whereas an enhanced utilization was observed in the nucleus supraopticus. Tendency towards a change\\n\\nExample 3: were equally round, reactive to light bilaterally, and he had no photophobia or blurred vision. At the time, he was in a rigid, opisthotonic posture with his neck fixed backward and laterally. His consciousness was clear, and he was able to answer all questions; vital signs were 162/90 << mmHg>> and 98 << beats>> <</>> <<min>>. Intravenous midazolam 2 << mg>> was administered once, and about 5 minutes later the symptom began to abate and about 20 minutes later the dystonic reaction had completely resolved. Arterial blood gas analysis and other laboratory findings were within normal limits. The patient remained in the PACU\\n\\nExample 4: stabilization of characteristics is limited in a polycrystalline or amorphous thin film. A film thickness of at least 200.ANG. is required in order to obtain a stable film, while the area of the capacitance part 3607 must also be increased in this case and hence it is impossible to cope with refinement of the device.\\n(Problems of Prior Art 17 and Prior Art 18)\\nFor example, a flash EPROM has a high data reading speed of about several 10 to 200 n <<sec>>. in general, while a data writing or erasing operation requires an extremely << long>> time of several.mu. <<sec>> <<.>> to several m <<sec>>\\n\\nExample 5: same size. The common weights are 0.6-ounce, 2-ounce, and 8-ounce.\\n\\nTo determine the amount of cake yeast you need from an active dry yeast amount in your recipe, use the following conversions:\\n\\n  1/4- <<ounce>> of active dry yeast equals 2/3-ounce of cake yeast\\n\\n  2 1/4 << teaspoons>> << of>> active dry yeast equals 2/3-ounce of cake yeast\\n\\n  3 packages of active dry yeast weighing 1/4 << ounce>> each equal 2 << ounces>> of cake yeast\\n\\n## Active dry yeast\\n\\nActive dry yeast is processed one step further than compressed yeast.\\n\\nExample 6: KHL players. The practical significance of those variables was also confirmed: it was high for BF ( <<kg>> <<)>> and intermediate for FFM ( <<kg>>). As for the other parameters, no statistically significant differences were observed.\\n\\nSegmental analysis of the ELH and KHL forwards revealed that fat was evenly distributed on the right and left extremities, with slightly more fat on lower extremities. There were no statistically << or>> practically significant differences in fat distribution among individual segments, although the ELH forwards had more fat on their trunks (2.09 << %>>).\\n\\nThe BM, BMI, FFMI, and FFM of the ELH defense\\n\\nExample 7: by the grand-children of Ulfríkr in commemoration of his receiving two danegelds in England.\\n\\nFurther payments were made in 1002, and especially in 1007 when Aethelred bought two years peace with the Danes for 36,000 pounds (13,436 << kg>> <<)>> << of>> silver. In 1012, following the capture and murder of the Archbishop of Canterbury, and the sack of Canterbury, the Danes were bought off with another 48,000 pounds (17,916 << kg>> <<)>> of silver.\\n\\nIn 1016 Sweyn Forkbeard's son, Canute,\\n\\nExample 8: again isolated from a blood culture on day 37. Despite the continued isolation of *S. rubidaea* from blood, the neonate improved and, once afebrile (day 40), she was transferred to the nursery where she began to gain weight. On reaching a body weight of 1.8 << kg>> << (>> <<day>> 75 of life <<)>> she was discharged.\\n\\nA third patient (2199) on 7 July suggested either inadequate fumigation of the NICU or human-to-human contact transmission. The neonate was born through assisted vaginal delivery at 28 weeks of << gestation>>. An sonogram performed 1 month prior to delivery suggested\\n\\nExample 9: China Plain in the summer maize growing seasons of 2012 and 2013. The study field is located in a warm, semihumid region with a continental climate. The average << annual>> rainfall is 786.3\\u2009 <<mm>> <<,>> and 65.1 <<%>> of the local rainfall is concentrated in the summer, which can satisfy the water requirement for all growth stages of summer maize. The soil of the experimental site is loamy (40% sand, 44% silt, and 16% clay) with 32.4% field water << capacity>> \\\\[[@B26]\\\\]. In the 2012 and 2013 summer maize growing seasons, total rainfall was 337.1 and 461.\\n\\nExample 10: gout was diagnosed with gouty arthritis based on recurring arthritis in the first MTP joint of the left foot for 9 years. He was treated with non-steroidal anti-inflammatory drugs (NSAIDs <<)>> during episodes, but he was not treated for hyperuricemia. He took 45\\u200a <<g>> << of>> alcohol every << day>> and had overconsum <<ption>> of purine-rich foods. In May 2017, he presented with arthralgia in the first MTP joints of both feet and in the left ankle. Despite treatment with NSAIDs, arthralgia expanded to the right knee and the right ankle in July 2017. He was referred\\n\\n\\n\\nTop_logits: �����                      \\n\\n               \\n                    \\n        \"}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user', 'content': '\\n\\n\\nTop_logits: �����������'}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\n\\n\\nTop_logits: ာally�ုrangleတ”)ovascularhopsurious'}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1: \\n602 F.3d 605 (2010)\\nTyrone Lorenzo ROBINSON, << Plaintiff>>- <<Appellant>>, and\\nTonya Ledell Robinson, << Plaintiff>>,\\nv.\\nJoseph Franklin CLIPSE, Public Safety Trooper First Class, << Defendant>>-Appellee, and\\nSouth Carolina Department of Public Safety; South Carolina Highway Patrol, << Defendants>>.\\nNo. 08-6670.\\nUnited States Court of Appeals, Fourth Circuit.\\nArgued: March 23, 2010.\\nDecided: April 28, 2010.\\n*606 ARGUED: Christopher Vieira, Duke University School of Law, Durham, North Carolina, for\\n\\nExample 2: 315 F.3d 637\\nUNITED STATES of America, << Plaintiff>> <<->> <<Appellee>> <<,>>v.Kenneth GREGORY; Lisa Lockhart, << Defendants>>-Appellants.\\nNo. 01-5942.\\nNo. 01-6445.\\nUnited States Court of Appeals, Sixth Circuit.\\nArgued: December 9, 2002.\\nDecided and Filed: January 13, 2003.\\n\\nStuart J. Canale (argued and briefed), Assistant United States Attorney, Memphis, TN, for U.S.\\nStephen B. Shankman (argued and briefed), M. Dianne Smothers (\\n\\nExample 3: 45 F.3d 582\\n31 Fed.R.Serv.3d 728, 33 U.S.P.Q.2d 1634\\nALEXIS LICHINE & CIE., << Plaintiff>> <<,>> << Appellee>>,v.SACHA A. LICHINE ESTATE SELECTIONS, LTD, and Sacha Lichine, <<Defendants>>, << Appellants>>.\\nNo. 94-1918.\\nUnited States Court of Appeals,First Circuit.\\nHeard Dec. 8, 1994.Decided Jan. 30, 1995.\\n\\nStanley S. Arkin with whom Harry B. Feder, New\\n\\nExample 4: \\n117 F.Supp.2d 257 (2000)\\nCONSOLIDATED EDISON << COMPANY>> OF NEW YORK, INC <<.,>> << Plaintiff>> <<,>>\\nv.\\nGeorge E. PATAKI, in his official capacity as Governor of the State of New York; Maureen O. Helmer, in her official capacity as Chairman of the New York Public Service Commission; and Thomas J. Dunleavy, James D. Bennett, Leonard Weiss and Neal N. Galvin in their official capacities as commissioners of the New York Public Service Commission, << Defendants>>.\\nSheldon Silver, Speaker of the New York State Assembly, and Richard L\\n\\nExample 5: \\n150 F.Supp. 864 (1957)\\nHatsue Ishii GILLES, << Plaintiff>> <<,>>\\nv.\\nAlbert DEL GUERCIO, as District Director, Immigration and Naturalization Service, Los Angeles, California, << Defendant>>.\\nNo. 19918.\\nUnited States District Court S. D. California, Central Division.\\nMay 9, 1957.\\n*865 Theodore E. Bowen, Los Angeles, Cal., for << plaintiff>>.\\nLaughlin E. Waters, U. S. Atty., Max F. Deutz, Arline Martin, Asst. U. S. Attys., Los\\n\\nExample 6: \\n116 Cal.App.3d 141 (1981)\\n171 Cal. Rptr. 461\\nTHE PEOPLE, << Plaintiff>> << and>> << Respondent>>,\\nv.\\nWILLIE REYNOLDS, << Defendant>> and Appellant.\\nDocket No. 20307.\\nCourt of Appeals of California, First District, Division Three.\\nFebruary 2, 1981.\\n*142 COUNSEL\\nJohn Raymond, under appointment by the Court of Appeal, for << Defendant>> and Appellant.\\nGeorge Deukmejian, Attorney General, Robert H. Philibosian, Chief Assistant Attorney General, Edward P. O\\'Brien, Assistant Attorney General, Charles James\\n\\nExample 7: manufacturing company in western New York in order to become President of the Macintosh-Hemphill division of Gulf & Western Manufacturing Company (\"Mac-Hemp\"). << Plaintiff>> became the division\\'s president and invested personal funds in Mac-Hemp, forming a corporation to do so. That corporation is now in bankruptcy, due mainly to liabilities which, << plaintiff>> contends, the defendants knew or should have known of but failed to reveal to him. << Plaintiff>> argues that defendants did not inform him of certain unfunded pension liabilities and vested retiree medical insurance benefits at the time he was hired as division president in 1982 and when he purchased Mac-H\\n\\nExample 8: book, no cash book, and no ledger. << Plaintiff>> W. C. Jackson took an inventory of the merchandise in the building on October 1, 1946, and the fire occurred on October 13. To the inventory was added the amount shown by invoices representing materials received between the first and the thirteenth of October, and amounts which Jackson testified represented materials withdrawn during that period were subtracted. He testified that he had personal knowledge of the withdrawals and did not depend upon records for his knowledge. The records were not introduced. Section 501, << Title>> 12 Oklahoma Statutes 1941, provides in substance that upon an affirmative showing of their authenticity and correctness,\\n\\nExample 9: \\n839 P.2d 356 (1992)\\nUNION PACIFIC RESOURCES << COMPANY>>, << Appellant>> ( <<Plaintiff>> <<),>>\\nv.\\n <<STATE>> of Wyoming; Wyoming Department of Revenue; Earl Kabeiseman, in his official capacity as Director of the Wyoming Department of Revenue; Wyoming State Board of Equalization; Wyoming Tax Commission; Nancy D. Freudenthal, Marvin Applequist, II, and C.H. Brown, III, in their official capacities as members of the Wyoming State Board of Equalization and the Wyoming Tax Commission; Wyoming Department of Audit; Roger W. Dewey, in his capacity as Director of the Wyoming\\n\\nExample 10: 2009, Lyshell Wilson filed her complaint, which << states>>:\\n\\n      On December 22, 2007, << Plaintiff>> Lyshell Wilson, a business invitee, entered\\n      Citi Trends located at 4547 North State Street, Jackson, Mississippi, for the\\n      purpose of shopping for clothing. While shopping on the premises of Citi\\n\\x0c       Trends, Lyshell Wilson was brutally injured when an employee of Citi Trends,\\n       Jocelyn Howard, maliciously, reckless, negligently, and violently attacked\\n       Lyshell Wilson with a pair of scissors.\\n\\nOn May 26, 2009, Howard\\n\\n\\n\\nTop_logits: �������ster����'}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"\\nExample 1: the region’s tropical forests – the biggest lung of our world – and the vanquishing of peoples like the Awajún and Wampis would be a tragic loss for us all.\\n\\nOctober 11, 2017\\n\\n* Birgit Weiler is Director of the Area of Research at the University Antonio Ruiz de Montoya in Lima; collaborates closely with the Vicariate of Jaén (Catholic << Church>>) and with the << Aw>> <<aj>> <<ún>> << and>> Wampis; and contributes to CLALS’s project on religion and climate change.\\n\\nA ceremony at Mount Huaytapallana during the Andean\\n\\nExample 2: medical degree from Northeastern Ohio Universities College of Medicine and completed his residency in general surgery at Northeastern Ohio University College of Medicine, Akron General Medical Center in Akron, Ohio. He completed fellow <<ships>> in trauma surgery research at Case Western Reserve University School of Medicine in Cleveland, surgical critical << care>> and trauma surgery at University of North Carolina at Chapel Hill in Chapel Hill, N.C., and << burn>> << surgery>> at the North Carolina Jaycee Burn Center at University of North Carolina at Chapel Hill. He also completed the Charles Fox Traveling Burn Fellowship with the American << Burn>> Association. In 2009, Guy received a Masters of Management in Health Care\\n\\nExample 3: Detective Nate Lahey, Jack Falahee as Connor Walsh, Aja Naomi King as Michaela Pratt, Matt McGorry as Asher Millstone, Karla Souza as Laurel Castillo, Charlie Weber as Frank Delfino, Liza Weil as Bonnie Winterbottom, << Conrad>> Ricamora as Oliver Hampton, Rome Flynn as Gabriel Maddox, Am <<ir>> <<ah>> Vann as Tegan Price and Timothy Hutton as Emmett Crawford.Guest starring is B.K. Cannon as Sophie Dolan, John Hensley as Inter <<im>> D.A. Ronald Miller, Gly <<nn>> Turman as Nate\\n\\nExample 4: be next.\\n\\nIn a Democracy Now! special, we spend the hour with four former U.S. intelligence officials — all whistleblowers\\n\\nthemselves — who have just returned from visiting National Security Agency whistleblower Edward Snowden in Russia. They are former CIA analyst Ray McGovern, former FBI agent Coleen Rowley, former National << Security>> << Agency>> senior executive Thomas Drake, and former << U>>. <<S>> <<.>> Justice…\\n\\nClick Link Below For Eid Prayer Information In Atlanta Area:…\\n\\nHer story is one of triumph, faith and perseverance- reminding us of the power of God's mercy and grace.\\n\\n\\nExample 5: scoring.\\n\\nAfter snagging the puck in the neutral zone and skating up the left wing, Gilbert Brule slipped the puck to surging defenceman Tom Gilbert, who surprised Smith from the mid-slot and gave << the>> Oil <<ers>> a 3-2 lead, their first of the game.\\n\\nEdmonton came within seconds of closing the game with a win, but a late goal by Tampa's Kurtis Foster tied the game at three. Although the Oil <<ers>> were gifted a late power <<play>> thanks to a delay of game penalty to the Lightning, the club was unable to secure a game-w <<inner>> in regulation and was forced\\n\\nExample 6: Bitcoin; the talks between North and South Korea; Monday night’s College Football Championship between Alabama and Georgia; and the NFL Play <<offs>> were some of the most-talked-about stories on news/talk radio yesterday, according to ongoing research from TALKERS magazine.\\n\\nWABC’s John Batchelor to Broad <<cast>> from Qatar. Night host John Batchelor is broadcasting his WABC, New York talk show from Doha, Qatar this week after accepting an invitation from the Middle << East>> nation. Batchelor is leading a delegation of visitors including former << Michigan>> Congressman Thaddeus McCottter and New << York>>\\n\\nExample 7: Kansas City Chiefs 2012 Draft Preview\\n\\nGeneral manager Scott Pioli dedicated much of the Chiefs' resources in free agency towards shoring up the league's second-worst scoring offense from last season, which could mean he'll be turning his attention to a defense that's somewhat thin in a few places during the draft. The depth issues are most prevalent along the front line, with Kansas << City>> in need of a replacement for aging nose tackle Kelly Gregg and << starting>> << end>> Glenn Dorsey entering the final year of his contract, and the team could additionally use a third << safety>> with << starter>>'s skills with young standout Eric Berry coming off a\\n\\nExample 8: was responsible for the study conception and design, interpretation of the analyses, << as>> well as critical revision and final approval of the manuscript. All authors read and approved the final manuscript.\\n\\nAcknowledgements\\n================\\n\\nThe authors thank all of the patients who participated in the study, as well as Mr Robert LeGros, << BA>> for help with data collection, Mr Victor Dinglas, M <<PH>> for assistance with data management, and Pran <<ita>> Tamma MD, << M>>HS, for her editorial assistance. The four participating centers were Johns Hopkins Hospital and Johns Hopkins Bayview Medical Center, both part of Johns Hopkins University, the University of Maryland\\n\\nExample 9: own a giant U.S. theatrical exhibition chain.) A TV news montage of the world going kablooey — << Europe>> in crisis, cyberhackers shutting down power grids, Asian conflagrations and Middle << East>> attacks, and << Obama>> and << Biden>> and << Hillary>> Clinton trying to calm a nation’s frayed nerves — lets us know that things are not going swimmingly. And then Kim Jong-un’s army arrives.\\n\\nLuckily, a bunch of defiant Washington teens jump in a pickup truck, crash through some barricades, and make for a cabin in the woods, where they plot a guerrilla\\n\\nExample 10: and NSF, United States of America. In addition, individual groups and members have received support from BCKDF, the Canada Council, CANARIE, CRC, Compute Canada, << F>>QRNT, and the Ontario Innovation Trust, Canada; E <<PL>>ANET, ERC, ERDF, FP7, Horizon 2020 and Marie Skłodowska-Curie Actions, European Union; Invest <<isse>> <<ments>> d'Avenir Labex and Idex, ANR, Région Auvergne and Fondation Partager le Savoir, France; D <<FG>> and AvH Foundation, Germany; Herakleitos,\\n\\n\\n\\nTop_logits: ��++,...](� member he matters etcications assured\"}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\n\\n\\nTop_logits: ������������ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ'}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user', 'content': '\\n\\n\\nTop_logits: ���������ansas'}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1: Q:\\n\\nWorkplace is rescinding on a promise made upon hire\\n\\nWe are in California in case it helps when giving advice on her situation.\\nMy wife was hired at her << workplace>> back << in>> February. Part of her negotiation at << hire>> was that they would accommodate her university << schedule>> by allowing her to leave a little early on certain days so that she could make it to class on time. Every << semester>> she leaves one hour early two days a week.\\nThe arrangement has been fine up until this past Thursday. Her new manager gave her an ultimatum to decide by Tuesday. She told her that they can no longer accommodate her\\n\\nExample 2: Q:\\n\\nWhen did ancient religions start to experience dropping numbers of adherents in Europe?\\n\\nI am talking about << ancient>> Greek <<,>> Roman << and>> Norwegian religions.\\nWhen did ancient religions start to lose ground to << modern>> religions such as Catholicism in Europe?\\n\\nA:\\n\\nChristianity didn\\'t really begin to crowd out the other religions in Europe until after Constantine and Julian\\'s reigns in the Roman Empire, although well before that time (the 190s << CE>>) the Empire had began an Imperial cult in which the Emperor was revered as a god, which probably resulted in a decline of the older religions. Per Wikipedia\\n\\nExample 3: Q:\\n\\nLecturer giving automatic top mark to the first person to submit\\n\\nA friend of mine is studying biology at a UK university and told me about this situation that happened last semester.\\nAn assignment was set on one of her << courses>>, and as the deadline date approached, the lecturer sent out an email stating that the first person to submit their << assignment>> would be << guaranteed>> << an>> automatic first (the << top>> mark available).\\nShe felt this was unfair (what if the first person to submit rushed their work and completed the questions poorly, yet were rewarded for their haste?) but due to the intimidating nature of this particular lect\\n\\nExample 4: Q:\\n\\nDid baroque composers expect you to \" <<bring>> out\" the voices in their pieces, the way today\\'s critics seem to enjoy in players?\\n\\nSo, I recently had a, sadly, way too short conversation with a pianist on the train.\\nAccording to the guy, << in>> the 17th and 18th << century>> polyphonic keyboard works were not played in such a way that the individual << voices>> would be emphasized, but rather in a way so that they would \"blend in\".\\nWith the music in front of him/her the listener would be able to admire the mastery of the composer << in>> building the\\n\\nExample 5: 2011 Asian Karate Championships\\n\\nThe 2011 Asian Karate Championships are the 10th edition of the Asian Karate Championships, and were held in Haixia Sports Center, Quanzhou, China from July 21 to July 24, 2011.\\n\\nMedalists\\n\\nMen\\n\\nWomen\\n\\nMedal table\\n\\nReferences\\n Results\\n\\nExternal links\\n akf-karatedo.com\\n\\nAsian Championships\\nAsian Karate Championships\\nCategory:Asian Karate Championships\\nKarate ChampionshipsMonday, April 20, 2009\\n\\nCH Blue Run\\'s << She>> <<\\'s>> << a>> Ca <<ution>>\\n\\nAmber finished her championship << with>>\\n\\nExample 6: Q:\\n\\nA corresponding riddle\\n\\nI\\'ve been keeping up a correspondence with a friend, a time-travelling historian. Her latest project has been visiting Camelot to understand the dynamics of King Arthur\\'s knights, and she\\'s been writing to me as the project continues.  \\nThe first missive told me she << was>> preparing to << set>> off with one of the knights. It sounded oddly like they were going to play golf, but that\\'s probably just me.  \\nThe << second>> note was << very>> abbreviated - even combined with what I knew from the first, << she>> only just had time to tell me they were on\\n\\nExample 7: If this is your first visit, be sure to check out the FAQ by clicking the link above.\\nYou may have to register before you can post: click the register link above to proceed.\\nTo start viewing messages, select the forum that you want to visit from the selection below.\\n <<\\n>> <<For>> those of you who missed it at Chess <<ville>>, << my>> << interview>> with CJA\\nAward winning historian Olimpiu Urcan is now in The Chess Journalist,\\nthe magazine of the Chess Journalists of America. You may download the\\nSeptember 2006 issue he\\n\\nYou were alive in 1890?\\n\\n\\nExample 8: Q:\\n\\nInterpretation of some lines in The Dolphins by Carol Ann Duffy\\n\\nIn the << poem>> \"The D <<olphins>>\" (see the summary on Beaming Notes), what does the following line mean:\\n\\n\"for the world will not deep <<en>> to dream in\". \\n\\nI am unable to understand the usage of \"dream in\"\\nAlso, \\n\\n\"out of love reflects me << for>> << myself>>\" \\n\\nis really unclear to me.\\nI am not sure why the poet uses the word \"this\" in the third line where the dolphin says: \\n\\nOutside this world you cannot breathe for\\n\\nExample 9: (artist)\\nCategory:1972 albums\\nCategory:Columbia Records albums\\nCategory:Repertoire Records albums\\nCategory:Vertigo Records albumsSF woman accused of stealing from girl charged\\n\\nMonday, July 21, 2014\\n\\nSAN FRANCISCO -- A woman who << allegedly>> << robbed>> << an>> 8- <<year>>-old girl << selling>> candy in San Francisco\\'s Western Addition neighborhood last week has been charged with robbery and child endangerment, the San Francisco District Attorney\\'s Office spokesman said.\\n\\nLadonna Christian, 46, of San Francisco, was arrested Thursday evening after she allegedly robbed and assaulted the young girl on\\n\\nExample 10: Pages\\n\\nMonday, October 1, 2012\\n\\nHas John Farrell Lost Control of the Clubhouse?\\n\\nAny time there are rumours about the Blue Jays, the natural instinct is to brush them off. After all, Toronto is linked to dozens of rumours throughout the season. Most of the time it\\'s regarding free agents or trades, but this rum <<our>> is a different kind of animal altogether.Omar Vizquel\\'s comments from last week indicated John Farrell is running a loose ship << as>> << the>> Blue Jays manager. As far as I\\'m concerned, where\\'s smoke... there\\'s << fire>> <<.>> As George Cost\\n\\n\\n\\nTop_logits: \".\"ikipediaenron.....��hline...?\"?\",dfrac\"?'}],\n",
       " [{'role': 'user',\n",
       "   'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\nExample 1: and secure << administration>> of the correct << dose>>. << The>> product is approved in the US under the trade-name Paser® and is currently used in Europe on a named patient basis. The positive opinion issued by the CHMP on November 21, 2013, is the concluding step of the European Medicinal Agencies review of the application for EU marketing authorization. The application will now be sent to the European Commission for formal << approval>> of marketing authorization, which is normally expected within 60 days. The product has an orphan drug designation which, following marketing authorization, will << render>> the product at least ten years of market exclusivity within the EU\\n\\nAbout Orphan\\n\\nExample 2: restricted generally to the recommended guidelines, what would the impact be on government drug expenditure? The aims of this study were to: (i) investigate trends in the duration and << dose>> << of>> PPI << prescribing>> in a national community << drug>> scheme in Ireland in a one year period 2007--2008; (ii) determine potential cost savings in a one year period (2007--2008) by examining different scenarios in prescribing patterns of PPIs according to clinical and << cost>>-effectiveness guidelines and (iii) compare potential cost savings stratified by different age groups.\\n\\nMethods\\n=======\\n\\nStudy population\\n----------------\\n\\nThe National Shared Services Primary Care Reimbursement Service\\n\\nExample 3: Oxytocin affects utilization of noradrenaline in distinct limbic-forebrain regions of the rat brain.\\nThe effects of oxytocin, administered intracerebroventricularly in << doses>> << of>> << 1>>, 10, 100 and 1000 pmol, were studied on the disappearance of catecholamines induced by alpha-methyl-p-tyrosine in microdissected nuclei of the rat brain. Oxytocin << dose>> <<->>dependently decreased the utilization of noradrenaline in the lateral and medial septal nuclei and anterior hypothalamic area, whereas an enhanced utilization was observed in the nucleus supraopticus. Tendency towards a change\\n\\nExample 4: During a clinical high << dose>> << rate>> (HDR) brachythe <<rapy>> procedure catheters are inserted into a << target>> region within a person, wherein through the inserted catheters << radiation>> sources are introduced into the target region in accordance with a treatment plan, which defines dwell times and dwell locations, for treating the target region. The treatment plan is determined in advance based on, inter alia, three-dimensional poses and shapes of the inserted catheters, wherein for determining the three-dimensional poses and shapes of the catheters within the person a user introduces sequentially a guidewire into the catheters, while the position of the tip of the guidewire within the\\n\\nExample 5: the rocks below. Adding an extra << dose>> << of>> intrigue was << a>> << view>> of the local oystermen nearby arranging their nets.\\n\\nBreakfast includes various omelets, freshly baked croissants, berber breads and pain perdu as well as jams and yogurt served with beghrir and meloui, traditional Moroccan pancakes, and toast with amlou — a splendid alternative to peanut butter, made with almonds, honey and argan oil. Guests are presented with a simple one <<->>page international news update for easy reading.\\nLa Sultana offers multiple excursions including special boat\\n\\nExample 6: hospital clinic for headaches. Due to hypertension 170/100 and blood glucose 207, she was immediately referred to maternity hospital. She was infused with magnesium sulfate. The first << dose>> of hydralazine ampoule 5 mg was << injected>> for the mother, and then 20 min later, the blood pressure was checked to be 160/100, the second << dose>> of hydralazine was injected, which the blood pressure was 170/100 and then the third << dose>> was << injected>> that her blood pressure was 180/110. Eventually, the mother, with a diagnosis of severe preeclampsia and nonresponse to medication, was carried to operation room by wheelchair without\\n\\nExample 7: Veno-occlusive disease: cytokines, genetics, and haemostasis.\\nHepatic veno-occlusive disease (VOD) is a major cause of morbidity and mortality following high << dose>> << cytotoxic>> therapy for stem cell transplantation (SCT). Pre-existing liver damage, SCT-related therapy, and genetic polymorphisms all appear to increase the risk of developing VOD. Studies of biological markers during SCT suggest that cytokines, haemostasis, and hepatic drug metabolism via the glutathione pathway are all involved in the pathogenesis of VOD. Until recently, treatment options were limited and experimental therapies directed at the pathogenesis of the disease were mostly\\n\\nExample 8: WEEKEND EDITION: Video-game Wars Heat Up With Explosive 2008 Titles\\n\\nSAN FRANCISCO (Dow Jones) - Video-game players who are accustomed to waiting for the holiday season to see the hottest new releases will get an early << dose>> << of>> yulet <<ide>> joy this year -- in late April, to be exact.\\n\\nThat\\'s << when>> \"Grand Theft Auto IV\" will hit store shelves -- likely on its way to racking up sales of 10 million units or more by the end of the year, according to most estimates.\\n\\nBut game fanatics worried about spoiling\\n\\nExample 9: to struggle. Lots of hair pulling and light slapping. Cowgirl and doggy style. Like see some sexy clothing left on hte models.\\n\\n2008-Apr-27 - Anthony Rosano, Lexi Love\\n\\nDr. Rosano examines a patient for an apparent case of sex addiction. He prescribes some tight rope bondage and shows Lexi Love how it\\'s done. She gets tied up and her clothes are ripped off her hot body, weights are hung from her perky tits and she gets a hard << dose>> << of>> spanking. This heats up her hungry cunt and she begs to be\\n\\nExample 10: breakdowns, as well as the data-informed speculation. -Joe from the Carolinas\\n\\nIn his first public appearance for several years, Rick Strassman shares the background, performance, and interpretation of his 1990s groundbreaking research at the University of New Mexico.\\n\\nIn this series of experiments—the first new American clinical research with psychedelic drugs in a generation—dozens of human volunteers received hundreds of << doses>> << of>> << D>>MT, one of the most powerful psychedelics known to science.\\n\\nThe story of his research project, published as DMT: The Spirit Molecule, raises intriguing questions about\\n\\n\\n\\nTop_logits: �����������'}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM inference:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM inference:  50%|█████     | 5/10 [00:33<00:33,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping LLM inference early for testing\n",
      "Total number of LLM outputs: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[TextBlock(text='I apologize, but the Top_logits list appears to be corrupted or improperly formatted in this case. Without valid information from the Top_logits, I\\'ll have to base my analysis solely on the text example provided.\\n\\nBased on the text example alone, this neuron seems to activate on the end of words, particularly when they form part of a longer word that\\'s split across tokens. In this case, it activates on \"ose\" which is likely part of the word \"comatose\".\\n\\nHowever, without more examples or valid Top_logits data, it\\'s difficult to make a confident assessment about what concept this neuron might be related to. The single example doesn\\'t provide enough context to reliably connect it to any of the specific professions or concepts listed.\\n\\nGiven the limited and potentially unreliable information, I cannot confidently associate this neuron\\'s behavior with any of the listed concepts. Therefore, I will mark all concepts as \"no\" in this case.\\n\\nyes_or_no_decisions = {\"accountant\": \"no\", \"architect\": \"no\", \"attorney\": \"no\", \"chiropractor\": \"no\", \"comedian\": \"no\", \"composer\": \"no\", \"dentist\": \"no\", \"dietitian\": \"no\", \"dj\": \"no\", \"filmmaker\": \"no\", \"interior_designer\": \"no\", \"journalist\": \"no\", \"model\": \"no\", \"nurse\": \"no\", \"painter\": \"no\", \"paralegal\": \"no\", \"pastor\": \"no\", \"personal_trainer\": \"no\", \"photographer\": \"no\", \"physician\": \"no\", \"poet\": \"no\", \"professor\": \"no\", \"psychologist\": \"no\", \"rapper\": \"no\", \"software_engineer\": \"no\", \"surgeon\": \"no\", \"teacher\": \"no\", \"yoga_teacher\": \"no\", \"profession\": \"no\", \"gender\": \"no\"}', type='text')],\n",
       " [TextBlock(text=\"I apologize, but it seems the Top_logits information is not properly formatted or is missing in your input. Without this crucial information, I cannot refine my analysis or make accurate judgments about the neuron's behavior in relation to the given concepts.\\n\\nBased solely on the examples provided, I can make some observations:\\n\\n1. The neuron seems to activate on units of measurement, particularly those related to medical and scientific contexts. Examples include:\\n   - Temperature: °C\\n   - Volume: mL\\n   - Weight: kg\\n   - Pressure: mmHg\\n   - Time: sec, min\\n   - Concentration: pmol\\n   - Length/distance: Å (Angstrom)\\n\\n2. The neuron also activates on mathematical operators or symbols used in measurements:\\n   - Division symbol: /\\n   - Percentage: %\\n\\n3. There's a focus on medical and scientific terminology, especially in contexts related to:\\n   - Surgical procedures\\n   - Pharmacology\\n   - Physiology\\n   - Chemistry\\n   - Nutrition\\n\\n4. The neuron seems to activate on numerical values associated with these measurements.\\n\\nHowever, without the Top_logits information, I cannot make a confident decision about whether this neuron's behavior is specifically related to any of the professions or concepts listed. The examples suggest a potential relation to medical or scientific fields, but this is not conclusive enough to make definitive yes/no decisions for each concept.\\n\\nGiven the lack of complete information, I will have to abstain from providing the yes_or_no_decisions as requested. If you can provide the Top_logits information, I'd be happy to refine my analysis and make more accurate judgments.\", type='text')],\n",
       " [TextBlock(text='I apologize, but it seems the Top_logits information is not properly formatted or readable in this case. Without clear information about the tokens the neuron predicts, I cannot refine my analysis or make accurate judgments about the neuron\\'s behavior in relation to the given concepts.\\n\\nBased solely on the examples provided in Part 1, the neuron appears to activate on words or phrases that come at the end of quoted speech or thoughts. However, this observation is not sufficient to confidently relate the neuron\\'s behavior to any of the specific concepts listed.\\n\\nGiven the limited and unclear information, I cannot make a definitive judgment about the neuron\\'s relation to any of the listed concepts. Therefore, I will mark all concepts as \"no\" in my decision.\\n\\nyes_or_no_decisions = {\"accountant\": \"no\", \"architect\": \"no\", \"attorney\": \"no\", \"chiropractor\": \"no\", \"comedian\": \"no\", \"composer\": \"no\", \"dentist\": \"no\", \"dietitian\": \"no\", \"dj\": \"no\", \"filmmaker\": \"no\", \"interior_designer\": \"no\", \"journalist\": \"no\", \"model\": \"no\", \"nurse\": \"no\", \"painter\": \"no\", \"paralegal\": \"no\", \"pastor\": \"no\", \"personal_trainer\": \"no\", \"photographer\": \"no\", \"physician\": \"no\", \"poet\": \"no\", \"professor\": \"no\", \"psychologist\": \"no\", \"rapper\": \"no\", \"software_engineer\": \"no\", \"surgeon\": \"no\", \"teacher\": \"no\", \"yoga_teacher\": \"no\", \"profession\": \"no\", \"gender\": \"no\"}', type='text')],\n",
       " [TextBlock(text='I apologize, but the Top_logits provided appear to be corrupted or incorrectly formatted. The tokens contain unusual characters and do not form coherent English words. Without clear and valid information from either the examples or the Top_logits, I cannot make any meaningful analysis or decisions about the neuron\\'s behavior in relation to the given concepts.\\n\\nGiven the lack of usable information, I must conclude that there is insufficient evidence to relate this neuron\\'s behavior to any of the listed concepts.\\n\\nyes_or_no_decisions = {\"accountant\": \"no\", \"architect\": \"no\", \"attorney\": \"no\", \"chiropractor\": \"no\", \"comedian\": \"no\", \"composer\": \"no\", \"dentist\": \"no\", \"dietitian\": \"no\", \"dj\": \"no\", \"filmmaker\": \"no\", \"interior_designer\": \"no\", \"journalist\": \"no\", \"model\": \"no\", \"nurse\": \"no\", \"painter\": \"no\", \"paralegal\": \"no\", \"pastor\": \"no\", \"personal_trainer\": \"no\", \"photographer\": \"no\", \"physician\": \"no\", \"poet\": \"no\", \"professor\": \"no\", \"psychologist\": \"no\", \"rapper\": \"no\", \"software_engineer\": \"no\", \"surgeon\": \"no\", \"teacher\": \"no\", \"yoga_teacher\": \"no\", \"profession\": \"no\", \"gender\": \"no\"}', type='text')],\n",
       " [TextBlock(text='I apologize, but the Top_logits provided are not readable or interpretable. They appear to be corrupted or improperly formatted. Without clear information from the Top_logits, I\\'ll base my analysis solely on the text examples provided.\\n\\nBased on the text examples, this neuron appears to be strongly associated with legal terminology and court case formatting, particularly related to the roles of parties in legal proceedings. The neuron consistently activates on terms like \"Plaintiff\", \"Defendant\", \"Appellant\", and \"Appellee\" in the context of court case citations and summaries.\\n\\nKey observations:\\n\\n1. The neuron activates on party designations in legal cases, such as \"Plaintiff\", \"Defendant\", \"Appellant\", and \"Appellee\".\\n2. It often activates on these terms when they appear in a specific format typical of legal case citations.\\n3. The examples are consistently from legal documents, court cases, or legal proceedings.\\n4. The neuron seems to be sensitive to the structure and formatting of legal case headers.\\n\\nGiven these observations, it appears that this neuron is related to identifying and processing legal party designations in the context of court cases. It\\'s not specifically related to any particular profession, but it is strongly associated with the legal field in general.\\n\\nyes_or_no_decisions = {\"accountant\": \"no\", \"architect\": \"no\", \"attorney\": \"yes\", \"chiropractor\": \"no\", \"comedian\": \"no\", \"composer\": \"no\", \"dentist\": \"no\", \"dietitian\": \"no\", \"dj\": \"no\", \"filmmaker\": \"no\", \"interior_designer\": \"no\", \"journalist\": \"no\", \"model\": \"no\", \"nurse\": \"no\", \"painter\": \"no\", \"paralegal\": \"yes\", \"pastor\": \"no\", \"personal_trainer\": \"no\", \"photographer\": \"no\", \"physician\": \"no\", \"poet\": \"no\", \"professor\": \"no\", \"psychologist\": \"no\", \"rapper\": \"no\", \"software_engineer\": \"no\", \"surgeon\": \"no\", \"teacher\": \"no\", \"yoga_teacher\": \"no\", \"profession\": \"yes\", \"gender\": \"no\"}', type='text')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO prompt caching\n",
    "# TODO parallelize LLM inference\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "llm_outputs_direct_prompt = []\n",
    "for i, messages in tqdm(enumerate(prompts), desc=\"LLM inference\", total=len(prompts)):\n",
    "\n",
    "    # barrier for testing\n",
    "    if i == 5: \n",
    "        print(\"stopping LLM inference early for testing\")\n",
    "        break\n",
    "\n",
    "    llm_out = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=system_prompt,\n",
    "        messages=messages,\n",
    "    )\n",
    "    llm_outputs_direct_prompt.append(llm_out.content)\n",
    "\n",
    "print(f'Total number of LLM outputs: {len(llm_outputs_direct_prompt)}\\n')\n",
    "llm_outputs_direct_prompt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"accountant\": \"no\", \"architect\": \"no\", \"attorney\": \"no\", \"chiropractor\": \"no\", \"comedian\": \"no\", \"composer\": \"no\", \"dentist\": \"no\", \"dietitian\": \"no\", \"dj\": \"no\", \"filmmaker\": \"no\", \"interior_designer\": \"no\", \"journalist\": \"no\", \"model\": \"no\", \"nurse\": \"no\", \"painter\": \"no\", \"paralegal\": \"no\", \"pastor\": \"no\", \"personal_trainer\": \"no\", \"photographer\": \"no\", \"physician\": \"no\", \"poet\": \"no\", \"professor\": \"no\", \"psychologist\": \"no\", \"rapper\": \"no\", \"software_engineer\": \"no\", \"surgeon\": \"no\", \"teacher\": \"no\", \"yoga_teacher\": \"no\", \"profession\": \"no\", \"gender\": \"no\"}\n",
      "\n",
      "I apologize, but it seems the Top_logits information is not properly formatted or is missing in your input. Without this crucial information, I cannot refine my analysis or make accurate judgments about the neuron's behavior in relation to the given concepts.\n",
      "\n",
      "Based solely on the examples provided, I can make some observations:\n",
      "\n",
      "1. The neuron seems to activate on units of measurement, particularly those related to medical and scientific contexts. Examples include:\n",
      "   - Temperature: °C\n",
      "   - Volume: mL\n",
      "   - Weight: kg\n",
      "   - Pressure: mmHg\n",
      "   - Time: sec, min\n",
      "   - Concentration: pmol\n",
      "   - Length/distance: Å (Angstrom)\n",
      "\n",
      "2. The neuron also activates on mathematical operators or symbols used in measurements:\n",
      "   - Division symbol: /\n",
      "   - Percentage: %\n",
      "\n",
      "3. There's a focus on medical and scientific terminology, especially in contexts related to:\n",
      "   - Surgical procedures\n",
      "   - Pharmacology\n",
      "   - Physiology\n",
      "   - Chemistry\n",
      "   - Nutrition\n",
      "\n",
      "4. The neuron seems to activate on numerical values associated with these measurements.\n",
      "\n",
      "However, without the Top_logits information, I cannot make a confident decision about whether this neuron's behavior is specifically related to any of the professions or concepts listed. The examples suggest a potential relation to medical or scientific fields, but this is not conclusive enough to make definitive yes/no decisions for each concept.\n",
      "\n",
      "Given the lack of complete information, I will have to abstain from providing the yes_or_no_decisions as requested. If you can provide the Top_logits information, I'd be happy to refine my analysis and make more accurate judgments.\n",
      "\n",
      "{\"accountant\": \"no\", \"architect\": \"no\", \"attorney\": \"no\", \"chiropractor\": \"no\", \"comedian\": \"no\", \"composer\": \"no\", \"dentist\": \"no\", \"dietitian\": \"no\", \"dj\": \"no\", \"filmmaker\": \"no\", \"interior_designer\": \"no\", \"journalist\": \"no\", \"model\": \"no\", \"nurse\": \"no\", \"painter\": \"no\", \"paralegal\": \"no\", \"pastor\": \"no\", \"personal_trainer\": \"no\", \"photographer\": \"no\", \"physician\": \"no\", \"poet\": \"no\", \"professor\": \"no\", \"psychologist\": \"no\", \"rapper\": \"no\", \"software_engineer\": \"no\", \"surgeon\": \"no\", \"teacher\": \"no\", \"yoga_teacher\": \"no\", \"profession\": \"no\", \"gender\": \"no\"}\n",
      "\n",
      "{\"accountant\": \"no\", \"architect\": \"no\", \"attorney\": \"no\", \"chiropractor\": \"no\", \"comedian\": \"no\", \"composer\": \"no\", \"dentist\": \"no\", \"dietitian\": \"no\", \"dj\": \"no\", \"filmmaker\": \"no\", \"interior_designer\": \"no\", \"journalist\": \"no\", \"model\": \"no\", \"nurse\": \"no\", \"painter\": \"no\", \"paralegal\": \"no\", \"pastor\": \"no\", \"personal_trainer\": \"no\", \"photographer\": \"no\", \"physician\": \"no\", \"poet\": \"no\", \"professor\": \"no\", \"psychologist\": \"no\", \"rapper\": \"no\", \"software_engineer\": \"no\", \"surgeon\": \"no\", \"teacher\": \"no\", \"yoga_teacher\": \"no\", \"profession\": \"no\", \"gender\": \"no\"}\n",
      "\n",
      "{\"accountant\": \"no\", \"architect\": \"no\", \"attorney\": \"yes\", \"chiropractor\": \"no\", \"comedian\": \"no\", \"composer\": \"no\", \"dentist\": \"no\", \"dietitian\": \"no\", \"dj\": \"no\", \"filmmaker\": \"no\", \"interior_designer\": \"no\", \"journalist\": \"no\", \"model\": \"no\", \"nurse\": \"no\", \"painter\": \"no\", \"paralegal\": \"yes\", \"pastor\": \"no\", \"personal_trainer\": \"no\", \"photographer\": \"no\", \"physician\": \"no\", \"poet\": \"no\", \"professor\": \"no\", \"psychologist\": \"no\", \"rapper\": \"no\", \"software_engineer\": \"no\", \"surgeon\": \"no\", \"teacher\": \"no\", \"yoga_teacher\": \"no\", \"profession\": \"yes\", \"gender\": \"no\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for out in llm_outputs_direct_prompt:\n",
    "    print(f'{out[0].text.split('yes_or_no_decisions = ')[-1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM inference:  50%|█████     | 5/10 [00:09<00:09,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping LLM inference early for testing\n",
      "Total number of LLM outputs: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Decisions(gender='no', professors='no', nurses='no'),\n",
       " Decisions(gender='no', professors='no', nurses='no'),\n",
       " Decisions(gender='no', professors='no', nurses='no'),\n",
       " Decisions(gender='no', professors='no', nurses='no'),\n",
       " Decisions(gender='no', professors='no', nurses='no')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data extraction with Instructor\n",
    "\n",
    "import instructor # pip install -U instructor\n",
    "from anthropic import Anthropic\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Decisions(BaseModel):\n",
    "    gender: str\n",
    "    professors: str\n",
    "    nurses: str\n",
    "\n",
    "\n",
    "client = instructor.from_anthropic(Anthropic())\n",
    "\n",
    "llm_outputs_instructor = []\n",
    "for i, messages in tqdm(enumerate(prompts), desc=\"LLM inference\", total=len(prompts)):\n",
    "\n",
    "    # barrier for testing\n",
    "    if i == 5: \n",
    "        print(\"stopping LLM inference early for testing\")\n",
    "        break\n",
    "\n",
    "    resp = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=system_prompt,\n",
    "        messages=messages,\n",
    "        response_model=Decisions,\n",
    "    )\n",
    "    llm_outputs_instructor.append(resp)\n",
    "\n",
    "print(f'Total number of LLM outputs: {len(llm_outputs_instructor)}\\n')\n",
    "llm_outputs_instructor[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note for the simple example of gender, doctor, nurse; Prompting with Instructor yielded a different result than direct prompting! I do not fully trust Instructor and would default to regex, if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No JSON-like dictionary found in the input string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     17\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes_or_no_decisions\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m     yes_or_no_decisions \u001b[38;5;241m=\u001b[39m \u001b[43mextract_and_convert_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     llm_outputs_json\u001b[38;5;241m.\u001b[39mappend(yes_or_no_decisions)\n\u001b[1;32m     21\u001b[0m llm_outputs_json[:\u001b[38;5;241m5\u001b[39m]\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mextract_and_convert_json\u001b[0;34m(input_string)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ast\u001b[38;5;241m.\u001b[39mliteral_eval(json_string)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JSON-like dictionary found in the input string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No JSON-like dictionary found in the input string."
     ]
    }
   ],
   "source": [
    "# Regex pattern matching\n",
    "\n",
    "def extract_and_convert_json(input_string):\n",
    "    # Regular expression to find JSON-like dictionaries\n",
    "    match = re.search(r'\\{.*?\\}', input_string)\n",
    "    \n",
    "    if match:\n",
    "        json_string = match.group(0)\n",
    "        # Convert the extracted string to a dictionary\n",
    "        return ast.literal_eval(json_string)\n",
    "    else:\n",
    "        raise ValueError(\"No JSON-like dictionary found in the input string.\")\n",
    "\n",
    "llm_outputs_json = []\n",
    "for out in llm_outputs_direct_prompt:\n",
    "    out = out[0].text\n",
    "    json_str = out.split('yes_or_no_decisions')[-1]\n",
    "    yes_or_no_decisions = extract_and_convert_json(json_str)\n",
    "    llm_outputs_json.append(yes_or_no_decisions)\n",
    "\n",
    "llm_outputs_json[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE assumes that feature_idx starts from 0\n",
    "# NOTE node_effects does currently not contain tensors\n",
    "# NOTE we currently do not check whether all classes are contained in all llm json outputs, this could lead to feature idx mismatching\n",
    "\n",
    "node_effects = defaultdict(list)\n",
    "for feat_idx, decisions in enumerate(llm_outputs_json):\n",
    "    for class_name, decision in decisions.items():\n",
    "        class_idx = profession_dict[class_name]\n",
    "        decision_bool = evaluate_binary_llm_output(decision)\n",
    "        node_effects[class_idx].append(decision_bool)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects_autointerp.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(node_effects, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [False, False, False, False, False],\n",
       "             1: [False, False, False, False, False],\n",
       "             2: [False, False, False, False, True],\n",
       "             3: [False, False, False, False, False],\n",
       "             4: [False, False, False, False, False],\n",
       "             5: [False, False, False, False, False],\n",
       "             6: [False, False, False, False, False],\n",
       "             7: [False, False, False, False, False],\n",
       "             8: [False, False, False, False, False],\n",
       "             9: [False, False, False, False, False],\n",
       "             10: [False, False, False, False, False],\n",
       "             11: [False, False, False, False, False],\n",
       "             12: [False, False, False, False, False],\n",
       "             13: [False, True, False, False, False],\n",
       "             14: [False, False, False, False, False],\n",
       "             15: [False, False, False, False, True],\n",
       "             16: [False, False, False, False, False],\n",
       "             17: [False, False, False, False, False],\n",
       "             18: [False, False, False, False, False],\n",
       "             19: [False, True, False, False, False],\n",
       "             20: [False, False, False, False, False],\n",
       "             21: [False, False, False, False, False],\n",
       "             22: [False, False, False, False, False],\n",
       "             23: [False, False, False, False, False],\n",
       "             24: [False, False, False, False, False],\n",
       "             25: [False, True, False, False, False],\n",
       "             26: [False, False, False, False, False],\n",
       "             27: [False, False, False, False, False],\n",
       "             -4: [False, False, False, False, True],\n",
       "             -2: [False, False, False, False, False]})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
