{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from experiments.bib_intervention import run_interventions, FeatureSelection\n",
    "import experiments.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_method = FeatureSelection.above_threshold\n",
    "selection_method = FeatureSelection.top_n\n",
    "\n",
    "random_seed = random.randint(0, 1000)\n",
    "num_classes = 5\n",
    "\n",
    "probe_train_set_size = 5000\n",
    "probe_test_set_size = 1000\n",
    "\n",
    "# Load datset and probes\n",
    "train_set_size = 1000\n",
    "test_set_size = 1000\n",
    "probe_batch_size = 50\n",
    "llm_batch_size = 125\n",
    "# llm_batch_size = 10\n",
    "\n",
    "# Attribution patching variables\n",
    "n_eval_batches = 4\n",
    "patching_batch_size = 50\n",
    "# patching_batch_size = 5\n",
    "\n",
    "top_n_features = [5, 10, 20, 50, 100, 500]\n",
    "top_n_features = [5, 500]\n",
    "T_effects_all_classes = [0.1, 0.01, 0.005, 0.001]\n",
    "# T_effects_all_classes = [0.001]\n",
    "T_effects_unique_class = [1e-4, 1e-8]\n",
    "\n",
    "if selection_method == FeatureSelection.top_n:\n",
    "    T_effects = top_n_features\n",
    "elif selection_method == FeatureSelection.above_threshold:\n",
    "    T_effects = T_effects_all_classes\n",
    "elif selection_method == FeatureSelection.unique:\n",
    "    T_effects = T_effects_unique_class\n",
    "else:\n",
    "    raise ValueError(\"Invalid selection method\")\n",
    "\n",
    "T_max_sideeffect = 5e-3\n",
    "\n",
    "dictionaries_path = \"../dictionary_learning/dictionaries\"\n",
    "probes_dir = \"trained_bib_probes\"\n",
    "\n",
    "# Example of sweeping over all SAEs in a sweep\n",
    "ae_sweep_paths = {\"pythia70m_test_sae\": None}\n",
    "\n",
    "# Example of sweeping over all SAEs in a submodule\n",
    "ae_sweep_paths = {\"pythia70m_test_sae\": {\"resid_post_layer_3\": None}}\n",
    "\n",
    "# Example of sweeping over a single SAE\n",
    "ae_sweep_paths = {\"pythia70m_test_sae\": {\"resid_post_layer_3\": {\"trainer_ids\": [0]}}}\n",
    "\n",
    "# This will look for any empty folders in any ae_path and raise an error if it finds any\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "    ae_group_paths = utils.get_ae_group_paths(dictionaries_path, sweep_name, submodule_trainers)\n",
    "\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "\n",
    "    run_interventions(\n",
    "        submodule_trainers,\n",
    "        sweep_name,\n",
    "        dictionaries_path,\n",
    "        probes_dir,\n",
    "        selection_method,\n",
    "        probe_train_set_size,\n",
    "        probe_test_set_size,\n",
    "        train_set_size,\n",
    "        test_set_size,\n",
    "        probe_batch_size,\n",
    "        llm_batch_size,\n",
    "        n_eval_batches,\n",
    "        patching_batch_size,\n",
    "        T_effects,\n",
    "        T_max_sideeffect,\n",
    "        num_classes,\n",
    "        random_seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Probe Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "\n",
    "\n",
    "# dictionaries_path = \"../dictionary_learning/dictionaries\"\n",
    "# device = \"cuda\"\n",
    "\n",
    "\n",
    "# submodule_trainers = {\"resid_post_layer_3\": {\"trainer_ids\": [10]}}\n",
    "# sweep_name = \"pythia70m_sweep0711\"\n",
    "\n",
    "\n",
    "# model_eval_config = utils.ModelEvalConfig.from_sweep_name(sweep_name)\n",
    "# model_name = model_eval_config.full_model_name\n",
    "\n",
    "# model = LanguageModel(model_name, device_map=device, dispatch=True)\n",
    "\n",
    "# ae_group_paths = utils.get_ae_group_paths(dictionaries_path, sweep_name, submodule_trainers)\n",
    "# ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "# submodule, dictionary, config = utils.load_dictionary(model, ae_paths[0], device)\n",
    "# sae_decoder = dictionary.decoder.weight\n",
    "\n",
    "# Load Sam's SAE\n",
    "ae_path = '/share/u/can/sae_eval/dictionary_learning/dictionaries/pythia-70m-deduped/resid_out_layer4/10_32768/ae.pt'\n",
    "dictionary = AutoEncoder.from_pretrained(ae_path, device='cuda')\n",
    "sae_decoder = dictionary.decoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_path = '/share/u/can/sae_eval/experiments/trained_bib_probes/pythia-70m-deduped/probes_ctx_len_128.pkl'\n",
    "\n",
    "with open(probe_path, \"rb\") as f:\n",
    "    probes = pickle.load(f)\n",
    "\n",
    "probe_map = probes[0].net.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple weight matmul\n",
    "direct_feature_attr = probe_map @ sae_decoder\n",
    "direct_feature_attr = direct_feature_attr.squeeze().detach().cpu().numpy()\n",
    "plt.hist(direct_feature_attr, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profession dictionary\n",
    "profession_dict = {\n",
    "    \"accountant\": 0,\n",
    "    \"architect\": 1,\n",
    "    \"attorney\": 2,\n",
    "    \"chiropractor\": 3,\n",
    "    \"comedian\": 4,\n",
    "    \"composer\": 5,\n",
    "    \"dentist\": 6,\n",
    "    \"dietitian\": 7,\n",
    "    \"dj\": 8,\n",
    "    \"filmmaker\": 9,\n",
    "    \"interior_designer\": 10,\n",
    "    \"journalist\": 11,\n",
    "    \"model\": 12,\n",
    "    \"nurse\": 13,\n",
    "    \"painter\": 14,\n",
    "    \"paralegal\": 15,\n",
    "    \"pastor\": 16,\n",
    "    \"personal_trainer\": 17,\n",
    "    \"photographer\": 18,\n",
    "    \"physician\": 19,\n",
    "    \"poet\": 20,\n",
    "    \"professor\": 21,\n",
    "    \"psychologist\": 22,\n",
    "    \"rapper\": 23,\n",
    "    \"software_engineer\": 24,\n",
    "    \"surgeon\": 25,\n",
    "    \"teacher\": 26,\n",
    "    \"yoga_teacher\": 27,\n",
    "}\n",
    "profession_dict_rev = {v: k for k, v in profession_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature to probe direction\n",
    "for i, probe in probes.items():\n",
    "    print(f'probe direction for profession {profession_dict_rev[i]}')\n",
    "    probe_map = probe.net.weight\n",
    "    probe_map_norm = probe_map / probe_map.norm(dim=1, keepdim=True)\n",
    "    sae_decoder_norm = sae_decoder / sae_decoder.norm(dim=0, keepdim=True)\n",
    "\n",
    "    feature_probe_cossim = probe_map_norm @ sae_decoder_norm\n",
    "    feature_probe_cossim = feature_probe_cossim.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    max_cossim = feature_probe_cossim.max()\n",
    "    min_cossim = feature_probe_cossim.min()\n",
    "    print(f\"Max cossim: {max_cossim}, Min cossim: {min_cossim}\")\n",
    "\n",
    "    plt.hist(feature_probe_cossim, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
