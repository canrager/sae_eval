{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "import datasets\n",
    "import anthropic\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import json\n",
    "\n",
    "import experiments.utils as utils\n",
    "from experiments.autointerp import (\n",
    "    get_max_activating_prompts,\n",
    "    highlight_top_activations,\n",
    "    compute_dla,\n",
    "    format_examples,\n",
    "    get_autointerp_inputs_for_all_saes,\n",
    ")\n",
    "import experiments.llm_autointerp.llm_utils as llm_utils\n",
    "\n",
    "DEBUGGING = True\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../anthropic_api_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "os.environ['ANTHROPIC_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic()\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=50,\n",
    "    temperature=0,\n",
    "    system=\"You are a world-class poet. Respond only with short poems.\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Why is the ocean salty?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profession_dict = {\n",
    "    \"accountant\": 0,\n",
    "    \"architect\": 1,\n",
    "    \"attorney\": 2,\n",
    "    \"chiropractor\": 3,\n",
    "    \"comedian\": 4,\n",
    "    \"composer\": 5,\n",
    "    \"dentist\": 6,\n",
    "    \"dietitian\": 7,\n",
    "    \"dj\": 8,\n",
    "    \"filmmaker\": 9,\n",
    "    \"interior_designer\": 10,\n",
    "    \"journalist\": 11,\n",
    "    \"model\": 12,\n",
    "    \"nurse\": 13,\n",
    "    \"painter\": 14,\n",
    "    \"paralegal\": 15,\n",
    "    \"pastor\": 16,\n",
    "    \"personal_trainer\": 17,\n",
    "    \"photographer\": 18,\n",
    "    \"physician\": 19,\n",
    "    \"poet\": 20,\n",
    "    \"professor\": 21,\n",
    "    \"psychologist\": 22,\n",
    "    \"rapper\": 23,\n",
    "    \"software_engineer\": 24,\n",
    "    \"surgeon\": 25,\n",
    "    \"teacher\": 26,\n",
    "    \"yoga_teacher\": 27,\n",
    "    \"male / female\": \"male / female\",\n",
    "    \"professor / nurse\": \"professor / nurse\",\n",
    "    \"male_professor / female_nurse\": \"male_professor / female_nurse\",\n",
    "    \"biased_male / biased_female\": \"biased_male / biased_female\",\n",
    "}\n",
    "\n",
    "chosen_class_names = [\n",
    "    \"gender\",\n",
    "    \"professor\",\n",
    "    \"nurse\",\n",
    "    \"accountant\",\n",
    "    \"architect\",\n",
    "    \"attorney\",\n",
    "    \"dentist\",\n",
    "    \"filmmaker\",\n",
    "]\n",
    "\n",
    "PROMPT_DIR = \"llm_autointerp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scale = 0\n",
    "max_scale = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PROMPT_DIR}manual_labels_few_shot.json\", \"r\") as f:\n",
    "    few_shot_manual_labels = json.load(f)\n",
    "\n",
    "for label in few_shot_manual_labels:\n",
    "    print(label, few_shot_manual_labels[label][\"per_class_scores\"])\n",
    "\n",
    "few_shot_examples = \"Here's a few examples of how to perform the task:\\n\\n\"\n",
    "\n",
    "for i, selected_index in enumerate(few_shot_manual_labels):\n",
    "    example_prompts = few_shot_manual_labels[selected_index][\"example_prompts\"]\n",
    "    tokens_string = few_shot_manual_labels[selected_index][\"tokens_string\"]\n",
    "    per_class_scores = few_shot_manual_labels[selected_index][\"per_class_scores\"]\n",
    "    chain_of_thought = few_shot_manual_labels[selected_index][\"chain_of_thought\"]\n",
    "\n",
    "    example_prompts = example_prompts[0].split(\"Example 4:\")[0]\n",
    "\n",
    "    few_shot_examples += f\"\\n\\n<<BEGIN EXAMPLE FEATURE {i}>>\\n\"\n",
    "    few_shot_examples += f\"Promoted tokens: {tokens_string}\\n\"\n",
    "    few_shot_examples += f\"Example prompts: {example_prompts}\\n\"\n",
    "    few_shot_examples += f\"Chain of thought: {chain_of_thought}\\n\\n\"\n",
    "    few_shot_examples += \"```json\\n\"\n",
    "    few_shot_examples += f\"{per_class_scores}\\n\"\n",
    "    few_shot_examples += \"```\"\n",
    "    few_shot_examples += f\"\\n<<END EXAMPLE FEATURE {i}>>\\n\\n\"\n",
    "\n",
    "print(few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(few_shot_examples)) \n",
    "\n",
    "\n",
    "\n",
    "print(llm_utils.count_tokens(few_shot_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = build_system_prompt(\n",
    "    concepts=chosen_class_names, min_scale=min_scale, max_scale=max_scale\n",
    ")\n",
    "\n",
    "# print(count_tokens(system_prompt))\n",
    "print(system_prompt[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_idx = 0\n",
    "number_of_test_examples = 10\n",
    "\n",
    "displayed_prompts = 10\n",
    "num_top_emphasized_tokens = 5\n",
    "include_activations = True\n",
    "t.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PROMPT_DIR}manual_labels_adam_corr.json\", \"r\") as f:\n",
    "    manual_test_labels = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_prompts = []\n",
    "\n",
    "for example_feature in manual_test_labels:\n",
    "\n",
    "    example_prompts = manual_test_labels[example_feature][\"example_prompts\"]\n",
    "    tokens_string = manual_test_labels[example_feature][\"tokens_string\"]\n",
    "    per_class_scores = manual_test_labels[example_feature][\"per_class_scores\"]\n",
    "    chain_of_thought = manual_test_labels[example_feature][\"chain_of_thought\"]\n",
    "    class_index = manual_test_labels[example_feature][\"class_index\"]\n",
    "\n",
    "    llm_prompt = \"Okay, now here's the real task.\\n\"\n",
    "    llm_prompt += f\"Promoted tokens: {tokens_string}\\n\"\n",
    "    llm_prompt += f\"Example prompts: {example_prompts[0]}\\n\"\n",
    "    llm_prompt += \"Chain of thought:\"\n",
    "\n",
    "    test_prompts.append((llm_prompt, class_index, per_class_scores, chain_of_thought))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 1\n",
    "\n",
    "test_prompt = few_shot_examples + test_prompts[test_idx][0]\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_prompts[test_idx][1])\n",
    "print(test_prompts[test_idx][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original for loop implementation for testing\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(number_of_test_examples):\n",
    "    test_prompt = few_shot_examples + test_prompts[i][0]\n",
    "\n",
    "    message = client.messages.create(\n",
    "        # model=\"claude-3-5-sonnet-20240620\",\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        system=system_prompt[0]['text'],\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": test_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    llm_response = message.content[0].text\n",
    "\n",
    "    json_response = llm_utils.extract_and_validate_json(llm_response)\n",
    "    good_json, verification_message = llm_utils.verify_json_response(json_response, min_scale, max_scale, chosen_class_names)\n",
    "    results.append((llm_response, json_response, good_json, verification_message))\n",
    "    print(i, good_json, verification_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = message.content[0].text\n",
    "\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = llm_utils.extract_and_validate_json(llm_response)\n",
    "good_json, verification_message = llm_utils.verify_json_response(json_response, min_scale, max_scale, chosen_class_names)\n",
    "print(json_response)\n",
    "print(chosen_class_names)\n",
    "print(f\"Good json: {good_json}\")\n",
    "print(verification_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
