{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual feature labelling and prompt building\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "import datasets\n",
    "import anthropic\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "\n",
    "import experiments.utils as utils\n",
    "from experiments.autointerp import (\n",
    "    get_max_activating_prompts, \n",
    "    highlight_top_activations,\n",
    "    compute_dla, \n",
    "    format_examples,\n",
    "    evaluate_binary_llm_output,\n",
    "    get_autointerp_inputs_for_all_saes,\n",
    ")\n",
    "from experiments.explainers.simple.prompt_builder import build_prompt\n",
    "from experiments.explainers.simple.prompts import build_system_prompt\n",
    "\n",
    "DEBUGGING = True\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model_dtype = t.bfloat16\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=DEVICE,\n",
    "    dispatch=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=model_dtype,\n",
    ")\n",
    "model_unembed = model.embed_out # For direct logit attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "num_contexts = 10000\n",
    "context_length = 128\n",
    "batch_size = 250\n",
    "\n",
    "dataset = datasets.load_dataset(\"georgeyw/dsir-pile-100k\", streaming=False)\n",
    "data = model.tokenizer(dataset[\"train\"][\"contents\"][:num_contexts], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=context_length).to(DEVICE).data\n",
    "batched_data = utils.batch_inputs(data, batch_size)\n",
    "\n",
    "\n",
    "# Class specific data\n",
    "\n",
    "profession_dict = {\n",
    "    \"accountant\": 0, \"architect\": 1, \"attorney\": 2, \"chiropractor\": 3,\n",
    "    \"comedian\": 4, \"composer\": 5, \"dentist\": 6, \"dietitian\": 7,\n",
    "    \"dj\": 8, \"filmmaker\": 9, \"interior_designer\": 10, \"journalist\": 11,\n",
    "    \"model\": 12, \"nurse\": 13, \"painter\": 14, \"paralegal\": 15,\n",
    "    \"pastor\": 16, \"personal_trainer\": 17, \"photographer\": 18, \"physician\": 19,\n",
    "    \"poet\": 20, \"professor\": 21, \"psychologist\": 22, \"rapper\": 23,\n",
    "    \"software_engineer\": 24, \"surgeon\": 25, \"teacher\": 26, \"yoga_teacher\": 27,\n",
    "    \"profession\": -4, \"gender\": -2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary\n",
    "\n",
    "dictionaries_path = \"../dictionary_learning/dictionaries\"\n",
    "\n",
    "# Current recommended way to generate graphs. You can copy paste ae_sweep_paths directly from bib_intervention.py\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\"resid_post_layer_3\": {\"trainer_ids\": [6]}},\n",
    "    # \"pythia70m_sweep_gated_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [9]}},\n",
    "    # \"pythia70m_sweep_topk_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [10]}},\n",
    "    # \"gemma-2-2b_sweep_topk_ctx128_0817\": {\"resid_post_layer_12\": {\"trainer_ids\": [2]}}, \n",
    "}\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(dictionaries_path, sweep_name, submodule_trainers)\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "ae_path = ae_paths[0]\n",
    "submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run max activating examples once from all SAEs\n",
    "\n",
    "k_inputs_per_feature = 10\n",
    "\n",
    "get_autointerp_inputs_for_all_saes(\n",
    "    model, \n",
    "    n_inputs=num_contexts,\n",
    "    batch_size=batch_size,\n",
    "    context_length=context_length,\n",
    "    top_k_inputs=k_inputs_per_feature,\n",
    "    ae_paths=ae_paths,\n",
    "    force_rerun=False,\n",
    ")\n",
    "\n",
    "with open(os.path.join(ae_path, \"max_activating_inputs.pkl\"), \"rb\") as f:\n",
    "    file = pickle.load(f)\n",
    "\n",
    "max_token_idxs_FKL = file[\"max_tokens_FKL\"]\n",
    "max_activations_FKL = file[\"max_activations_FKL\"]\n",
    "top_dla_token_idxs_FK = file[\"dla_results_FK\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find features relevant to a profession from the class_probe node_effects.pkl\n",
    "\n",
    "class_name = \"gender\"\n",
    "k_features_per_concept = 3 \n",
    "\n",
    "filename_counter = \"\"\n",
    "class_id = profession_dict[class_name]\n",
    "node_effects_filename = f\"{ae_path}/node_effects{filename_counter}.pkl\"\n",
    "\n",
    "with open(node_effects_filename, \"rb\") as f:\n",
    "    node_effects = pickle.load(f)\n",
    "\n",
    "effects = node_effects[class_id]\n",
    "print(effects.shape)\n",
    "\n",
    "\n",
    "top_k_values, top_k_indices = t.topk(effects, k_features_per_concept)\n",
    "t.set_printoptions(sci_mode=False)\n",
    "print(top_k_values)\n",
    "print(top_k_indices)\n",
    "\n",
    "selected_token_idxs_FKL = max_token_idxs_FKL[top_k_indices]\n",
    "selected_activations_FKL = max_activations_FKL[top_k_indices]\n",
    "top_dla_token_idxs_FK = top_dla_token_idxs_FK[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format max_activating_inputs by << emphasizing>> max act examples\n",
    "\n",
    "num_top_emphasized_tokens = 5\n",
    "\n",
    "example_prompts = format_examples(model, selected_token_idxs_FKL, selected_activations_FKL, num_top_emphasized_tokens)\n",
    "top_dla_tokens_FK = model.tokenizer.batch_decode(top_dla_token_idxs_FK, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = build_system_prompt(\n",
    "    cot=False,\n",
    "    concepts=list(profession_dict.keys()),\n",
    "    logits=True\n",
    ")\n",
    "\n",
    "prompts = []\n",
    "for example_prompt, top_dla_tokens_K in zip(example_prompts, top_dla_tokens_FK):\n",
    "    message = build_prompt(\n",
    "        examples=example_prompt,\n",
    "        cot=False,\n",
    "        top_logits=top_dla_tokens_K,\n",
    "    )\n",
    "    prompts.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feat_examples in enumerate(example_prompts):\n",
    "    print(f'############### Max act examples for Feature {top_k_indices[i]}:')\n",
    "    print(feat_examples)\n",
    "    print(f'############### Top dla tokens for Feature {top_k_indices[i]}:')\n",
    "    print(top_dla_tokens_FK[i])\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render prompt for human labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or alternatively view with circuitvis\n",
    "def _list_decode(x):\n",
    "    if len(x.shape) == 0:\n",
    "        return model.tokenizer.decode(x, skip_special_tokens=True)\n",
    "    else:\n",
    "        return [_list_decode(y) for y in x]\n",
    "\n",
    "selected_token_strs_FKL = _list_decode(selected_token_idxs_FKL)\n",
    "for i in range(selected_activations_FKL.shape[0]):\n",
    "    feat_idx = top_k_indices[i]\n",
    "    print(f\"Feature {feat_idx}:\")\n",
    "    selected_activations_KL11 = [selected_activations_FKL[i, k, :, None, None] for k in range(k_inputs_per_feature)]\n",
    "    html_activations = text_neuron_activations(selected_token_strs_FKL[i], selected_activations_KL11)\n",
    "    display(html_activations)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
