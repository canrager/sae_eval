{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import einops\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "import experiments.utils as utils\n",
    "from experiments.probe_training import *\n",
    "from experiments.pipeline_config import PipelineConfig\n",
    "from experiments.probe_training import get_all_activations\n",
    "\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "\n",
    "# Configuration\n",
    "DEBUGGING = False\n",
    "SEED = 42\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set up paths and model\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "tracer_kwargs = dict(scan=DEBUGGING, validate=DEBUGGING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "llm_model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "device = \"cuda\"\n",
    "train_set_size = 4000\n",
    "test_set_size = 1000\n",
    "context_length = 128\n",
    "model_dtype = torch.bfloat16 # TODO get from p_config\n",
    "\n",
    "# Probe config\n",
    "epochs = 1\n",
    "probe_batch_size = 500\n",
    "llm_batch_size = 500\n",
    "save_results = True\n",
    "\n",
    "# TODO: I think there may be a scoping issue with model and get_acts(), but we currently aren't using get_acts()\n",
    "model = LanguageModel(llm_model_name, device_map=device, dispatch=True, torch_dtype=model_dtype)\n",
    "probe_dir = \"latent_probing/probes\"\n",
    "only_model_name = llm_model_name.split(\"/\")[-1]\n",
    "\n",
    "model_eval_config = utils.ModelEvalConfig.from_full_model_name(llm_model_name)\n",
    "probe_layer = model_eval_config.probe_layer\n",
    "\n",
    "probe_output_filename = (\n",
    "    f\"{probe_dir}/{only_model_name}/probes_ctx_len_{context_length}_layer_{probe_layer}.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available paths: ['../../dictionary_learning/dictionaries/autointerp_test_data/pythia70m_sweep_topk_ctx128_0730/resid_post_layer_3/trainer_2']\n",
      "\n",
      "Loading dictionary from ../../dictionary_learning/dictionaries/autointerp_test_data/pythia70m_sweep_topk_ctx128_0730/resid_post_layer_3/trainer_2\n"
     ]
    }
   ],
   "source": [
    "# Select dictionaries\n",
    "\n",
    "dictionary_dir = \"../../dictionary_learning/dictionaries/autointerp_test_data\" # TODO: get from p_config\n",
    "\n",
    "trainer_ids = [2]\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_topk_ctx128_0730\": {\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Format paths\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "ae_paths = []\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "\n",
    "    ae_group_paths = utils.get_ae_group_paths(\n",
    "        dictionary_dir, sweep_name, submodule_trainers\n",
    "    )\n",
    "    ae_paths.extend(utils.get_ae_paths(ae_group_paths))\n",
    "\n",
    "print(f'available paths: {ae_paths}\\n')\n",
    "\n",
    "# Load dictionaries\n",
    "submodules = []\n",
    "dictionaries = {}\n",
    "sae_configs = {}\n",
    "for ae_path in ae_paths:\n",
    "    submodule, dictionary, sae_config = utils.load_dictionary(model, ae_path, device)\n",
    "    dictionary = dictionary.to(dtype=model_dtype)\n",
    "    submodules.append(submodule)\n",
    "    dictionaries[submodule] = dictionary\n",
    "    sae_configs[submodule] = sae_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trainer': {'trainer_class': 'TrainerTopK',\n",
       "  'dict_class': 'AutoEncoderTopK',\n",
       "  'lr': 0.0002,\n",
       "  'steps': 48828,\n",
       "  'seed': 0,\n",
       "  'activation_dim': 512,\n",
       "  'dict_size': 16384,\n",
       "  'k': 20,\n",
       "  'device': 'cuda:0',\n",
       "  'layer': 3,\n",
       "  'lm_name': 'EleutherAI/pythia-70m-deduped',\n",
       "  'wandb_name': 'TopKTrainer-EleutherAI/pythia-70m-deduped-resid_post_layer_3',\n",
       "  'submodule_name': 'resid_post_layer_3'},\n",
       " 'buffer': {'d_submodule': 512,\n",
       "  'io': 'out',\n",
       "  'n_ctxs': 10000,\n",
       "  'ctx_len': 128,\n",
       "  'refresh_batch_size': 32,\n",
       "  'out_batch_size': 4096,\n",
       "  'device': 'cuda:0'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_configs[submodules[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_and_prepare_dataset() missing 1 required positional argument: 'dataset_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_eval_config \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mModelEvalConfig\u001b[38;5;241m.\u001b[39mfrom_full_model_name(llm_model_name)\n\u001b[1;32m      5\u001b[0m d_model \u001b[38;5;241m=\u001b[39m model_eval_config\u001b[38;5;241m.\u001b[39mactivation_dim\n\u001b[0;32m----> 7\u001b[0m dataset, df \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m train_bios, test_bios \u001b[38;5;241m=\u001b[39m get_train_test_data(\n\u001b[1;32m     10\u001b[0m     dataset,\n\u001b[1;32m     11\u001b[0m     train_set_size,\n\u001b[1;32m     12\u001b[0m     test_set_size,\n\u001b[1;32m     13\u001b[0m     include_paired_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m train_bios \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtokenize_data(train_bios, model\u001b[38;5;241m.\u001b[39mtokenizer, context_length, device)\n",
      "\u001b[0;31mTypeError\u001b[0m: load_and_prepare_dataset() missing 1 required positional argument: 'dataset_name'"
     ]
    }
   ],
   "source": [
    "\"\"\"Because we save the probes, we always train them on all classes to avoid potential issues with missing classes. It's only a one-time cost.\"\"\"\n",
    "\n",
    "\n",
    "model_eval_config = utils.ModelEvalConfig.from_full_model_name(llm_model_name)\n",
    "d_model = model_eval_config.activation_dim\n",
    "\n",
    "dataset, df = load_and_prepare_dataset()\n",
    "\n",
    "train_bios, test_bios = get_train_test_data(\n",
    "    dataset,\n",
    "    train_set_size,\n",
    "    test_set_size,\n",
    "    include_paired_classes=False,\n",
    ")\n",
    "\n",
    "train_bios = utils.tokenize_data(train_bios, model.tokenizer, context_length, device)\n",
    "test_bios = utils.tokenize_data(test_bios, model.tokenizer, context_length, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 6, 9, 11, 13, 14, 18, 19, 20, 21, 22, 25, 26])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bios.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations for profession: 0\n",
      "Collecting activations for profession: 1\n",
      "Collecting activations for profession: 2\n",
      "Collecting activations for profession: 6\n",
      "Collecting activations for profession: 9\n",
      "Collecting activations for profession: 11\n",
      "Collecting activations for profession: 13\n",
      "Collecting activations for profession: 14\n",
      "Collecting activations for profession: 18\n",
      "Collecting activations for profession: 19\n",
      "Collecting activations for profession: 20\n",
      "Collecting activations for profession: 21\n",
      "Collecting activations for profession: 22\n",
      "Collecting activations for profession: 25\n",
      "Collecting activations for profession: 26\n"
     ]
    }
   ],
   "source": [
    "all_train_model_acts = {}\n",
    "all_test_model_acts = {}\n",
    "all_train_sae_acts = {}\n",
    "all_test_sae_acts = {}\n",
    "\n",
    "# TODO iterate over submodules\n",
    "submodule = submodules[0]\n",
    "with torch.no_grad():\n",
    "    for i, profession in enumerate(train_bios.keys()):\n",
    "\n",
    "        print(f\"Collecting activations for profession: {profession}\")\n",
    "        all_train_model_acts[profession] = get_all_activations(\n",
    "            train_bios[profession], model, llm_batch_size, submodule\n",
    "        )\n",
    "        all_test_model_acts[profession] = get_all_activations(\n",
    "            test_bios[profession], model, llm_batch_size, submodule\n",
    "        )\n",
    "        all_train_sae_acts[profession] = dictionaries[submodule].encode(\n",
    "            all_train_model_acts[profession]\n",
    "        )\n",
    "        all_test_sae_acts[profession] = dictionaries[submodule].encode(\n",
    "            all_test_model_acts[profession]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train k-sparse probe\n",
    "# Select the features in the first place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 16384])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_sae_acts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tensor_memory_usage(tensor: torch.Tensor):\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        print(\"Input is not a tensor. Cannot calculate memory usage.\")\n",
    "        return\n",
    "    \n",
    "    memory = tensor.element_size() * tensor.nelement()\n",
    "    print(f\"Tensor Shape: {tensor.shape}\")\n",
    "    print(f\"Tensor Type: {tensor.dtype}\")\n",
    "    print(f\"Memory usage: {memory / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_acts = all_train_acts[0]\n",
    "sae_acts = all_train_sae_acts[0]\n",
    "print_tensor_memory_usage(model_acts)\n",
    "print_tensor_memory_usage(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe model and training\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim: int, dtype: torch.dtype):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True, dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "def train_probe(\n",
    "    train_input_batches: list,\n",
    "    train_label_batches: list[torch.Tensor],\n",
    "    test_input_batches: list,\n",
    "    test_label_batches: list[torch.Tensor],\n",
    "    get_acts: Callable,\n",
    "    precomputed_acts: bool,\n",
    "    dim: int,\n",
    "    epochs: int,\n",
    "    device: str,\n",
    "    model_dtype: torch.dtype,\n",
    "    lr: float = 1e-2,\n",
    "    seed: int = SEED,\n",
    "    verbose: bool = False,\n",
    ") -> tuple[Probe, float]:\n",
    "    \"\"\"input_batches can be a list of tensors or strings. If strings, get_acts must be provided.\"\"\"\n",
    "\n",
    "    if type(train_input_batches[0]) == str or type(test_input_batches[0]) == str:\n",
    "        assert precomputed_acts == False\n",
    "    elif type(train_input_batches[0]) == torch.Tensor or type(test_input_batches[0]) == torch.Tensor:\n",
    "        assert precomputed_acts == True\n",
    "\n",
    "    probe = Probe(dim, model_dtype).to(device)\n",
    "    optimizer = torch.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_idx = 0\n",
    "        for inputs, labels in zip(train_input_batches, train_label_batches):\n",
    "            if precomputed_acts:\n",
    "                acts_BD = inputs\n",
    "            else:\n",
    "                acts_BD = get_acts(inputs)\n",
    "            logits_B = probe(acts_BD)\n",
    "            loss = criterion(logits_B, labels.clone().detach().to(device=device, dtype=model_dtype))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_idx += 1\n",
    "\n",
    "        \n",
    "\n",
    "        train_accuracy = test_probe(\n",
    "            train_input_batches[:30], train_label_batches[:30], probe, get_acts, precomputed_acts\n",
    "        )\n",
    "\n",
    "\n",
    "        test_accuracy = test_probe(\n",
    "            test_input_batches, test_label_batches, probe, get_acts, precomputed_acts\n",
    "        )\n",
    "\n",
    "        if epoch == epochs - 1 and verbose:\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs} Loss: {loss.item()}, train accuracy: {train_accuracy}, test accuracy: {test_accuracy}\\n\")\n",
    "    \n",
    "    return probe, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.probe_training import prepare_probe_data\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def train_probe_on_activations(\n",
    "    train_activations: dict[str | int : torch.Tensor],\n",
    "    test_activations: dict[str | int : torch.Tensor],\n",
    "    select_top_k: Optional[int] = None,\n",
    ") -> tuple[dict[str | int : Probe], dict[str | int : float]]:\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    probes, test_accuracies = {}, {}\n",
    "\n",
    "    for profession in train_activations.keys():\n",
    "        if profession in utils.PAIRED_CLASS_KEYS.values():\n",
    "            continue\n",
    "\n",
    "        train_acts, train_labels = prepare_probe_data(train_activations, profession, probe_batch_size, select_top_k)\n",
    "\n",
    "        test_acts, test_labels = prepare_probe_data(test_activations, profession, probe_batch_size, select_top_k)\n",
    "\n",
    "        if profession == \"biased_male / biased_female\" or profession == \"male / female\":\n",
    "            probe_epochs = 1\n",
    "        else:\n",
    "            probe_epochs = epochs\n",
    "\n",
    "        activation_dim = train_acts[0].shape[1]\n",
    "\n",
    "        probe, test_accuracy = train_probe(\n",
    "            train_acts,\n",
    "            train_labels,\n",
    "            test_acts,\n",
    "            test_labels,\n",
    "            get_acts,\n",
    "            precomputed_acts=True,\n",
    "            epochs=probe_epochs,\n",
    "            dim=activation_dim,\n",
    "            device=device,\n",
    "            model_dtype=model_dtype,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        probes[profession] = probe\n",
    "        test_accuracies[profession] = test_accuracy\n",
    "\n",
    "    return probes, test_accuracies\n",
    "\n",
    "    # if save_results:\n",
    "    #     only_model_name = llm_model_name.split(\"/\")[-1]\n",
    "    #     os.makedirs(f\"{probe_dir}\", exist_ok=True)\n",
    "    #     os.makedirs(f\"{probe_dir}/{only_model_name}\", exist_ok=True)\n",
    "\n",
    "    #     with open(probe_output_filename, \"wb\") as f:\n",
    "    #         pickle.dump(probes, f)\n",
    "\n",
    "\n",
    "model_activation_probes, model_activation_accuracies = train_probe_on_activations(all_train_acts, all_test_acts)\n",
    "sae_activation_probes, sae_activation_accuracies = train_probe_on_activations(all_train_sae_acts, all_test_sae_acts)\n",
    "sae_activation_top_1_probes, sae_activation_top_1_accuracies = train_probe_on_activations(all_train_sae_acts, all_test_sae_acts, select_top_k=1)\n",
    "sae_activation_top_5_probes, sae_activation_top_5_accuracies = train_probe_on_activations(all_train_sae_acts, all_test_sae_acts, select_top_k=5)\n",
    "sae_activation_top_10_probes, sae_activation_top_10_accuracies = train_probe_on_activations(all_train_sae_acts, all_test_sae_acts, select_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diffs = []\n",
    "int_diffs = []\n",
    "str_diffs = []\n",
    "\n",
    "for class_name in model_activation_accuracies.keys():\n",
    "    model_acc = model_activation_accuracies[class_name][0]\n",
    "    sae_acc = sae_activation_accuracies[class_name][0]\n",
    "    # sae_acc = sae_activation_top_1_accuracies[class_name][0]\n",
    "    # sae_acc = sae_activation_top_5_accuracies[class_name][0]\n",
    "    # sae_acc = sae_activation_top_10_accuracies[class_name][0]\n",
    "\n",
    "    diff = model_acc - sae_acc\n",
    "    print(f\"Class: {class_name}, Model Acc: {model_acc}, SAE Acc: {sae_acc}, Diff: {diff}\")\n",
    "\n",
    "    if isinstance(class_name, int):\n",
    "        int_diffs.append(diff)\n",
    "    if isinstance(class_name, str):\n",
    "        str_diffs.append(diff)\n",
    "    all_diffs.append(diff)\n",
    "\n",
    "print(f\"\\nAverage difference: {np.mean(all_diffs)}\")\n",
    "print(f\"Average difference for int classes: {np.mean(int_diffs)}\")\n",
    "print(f\"Average difference for str classes: {np.mean(str_diffs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.probe_training import get_activation_distribution_diff\n",
    "\n",
    "sae_feature_distribution_differences = {}\n",
    "for profession in all_train_sae_acts.keys():\n",
    "    if profession in utils.PAIRED_CLASS_KEYS.values():\n",
    "        continue\n",
    "\n",
    "    sae_feature_distribution_differences[profession] = get_activation_distribution_diff(all_train_sae_acts, profession)\n",
    "\n",
    "print(sae_feature_distribution_differences[0].sum())\n",
    "print(sae_feature_distribution_differences[\"male / female\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_class_indices = [\n",
    "    \"male / female\",\n",
    "    \"professor / nurse\",\n",
    "    \"male_professor / female_nurse\",\n",
    "    \"biased_male / biased_female\",\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    6,\n",
    "]\n",
    "\n",
    "saved_sae_feature_distribution_differences = {}\n",
    "\n",
    "for class_index in chosen_class_indices:\n",
    "    saved_sae_feature_distribution_differences[class_index] = sae_feature_distribution_differences[class_index]\n",
    "\n",
    "print(saved_sae_feature_distribution_differences.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ae_path}/node_effects.pkl\", \"rb\") as f:\n",
    "    node_effects_attrib_patching = pickle.load(f)\n",
    "\n",
    "with open(f\"{ae_path}/node_effects_dist_diff.pkl\", \"wb\") as f:\n",
    "    pickle.dump(saved_sae_feature_distribution_differences, f)\n",
    "\n",
    "class_idx = \"male / female\"\n",
    "# class_idx = 0\n",
    "\n",
    "node_effect = node_effects_attrib_patching[class_idx]\n",
    "sae_distribution = sae_feature_distribution_differences[class_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"node effect stats: {node_effect.mean():.4f}, {node_effect.std().item():.4f}, {node_effect.max().item():.4f}, {node_effect.min().item():.4f}\")\n",
    "print(f\"sae distribution stats: {sae_distribution.mean():.4f}, {sae_distribution.std().item():.4f}, {sae_distribution.max().item():.4f}, {sae_distribution.min().item():.4f}\")\n",
    "import torch\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    return (tensor - tensor.mean()) / tensor.std()\n",
    "\n",
    "def compare_top_values(tensor1, tensor2, top_n=10):\n",
    "    # Ensure tensors are on the same device\n",
    "    device = tensor1.device\n",
    "    tensor2 = tensor2.to(device)\n",
    "\n",
    "    # Convert to float32 for calculations\n",
    "    tensor1 = tensor1.to(torch.float32)\n",
    "    tensor2 = tensor2.to(torch.float32)\n",
    "\n",
    "    # Normalize tensors\n",
    "    norm_tensor1 = normalize_tensor(tensor1)\n",
    "    norm_tensor2 = normalize_tensor(tensor2)\n",
    "    \n",
    "    # Get indices of top N values\n",
    "    top_indices1 = torch.argsort(norm_tensor1, descending=True)[:top_n]\n",
    "    top_indices2 = torch.argsort(norm_tensor2, descending=True)[:top_n]\n",
    "    \n",
    "    print(f\"Top {top_n} indices in normalized node_effect:\")\n",
    "    for i, idx in enumerate(top_indices1):\n",
    "        print(f\"  {i+1}. Index {idx.item()}: {norm_tensor1[idx].item():.4f} (original: {tensor1[idx].item():.4f})\")\n",
    "    \n",
    "    print(f\"\\nTop {top_n} indices in normalized sae_distribution:\")\n",
    "    for i, idx in enumerate(top_indices2):\n",
    "        print(f\"  {i+1}. Index {idx.item()}: {norm_tensor2[idx].item():.4f} (original: {tensor2[idx].item():.4f})\")\n",
    "    \n",
    "    # Compare common indices\n",
    "    common_indices = set(top_indices1.tolist()) & set(top_indices2.tolist())\n",
    "    print(f\"\\nCommon indices in top {top_n}: {common_indices}\")\n",
    "    \n",
    "    if common_indices:\n",
    "        print(\"\\nValues at common indices:\")\n",
    "        for idx in common_indices:\n",
    "            print(f\"  Index {idx}: node_effect = {norm_tensor1[idx].item():.4f}, sae_distribution = {norm_tensor2[idx].item():.4f}\")\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = torch.corrcoef(torch.stack([norm_tensor1, norm_tensor2]))[0, 1]\n",
    "    print(f\"\\nCorrelation between normalized tensors: {correlation.item():.4f}\")\n",
    "\n",
    "# Usage:\n",
    "compare_top_values(node_effect, sae_distribution, top_n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
