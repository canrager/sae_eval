{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probes across all layers\n",
    "\n",
    "We score SAEs by their ability to \"recover\" supervised concepts from the residual stream? To which degree are those concepts detectable by linear probes at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "import experiments.utils as utils\n",
    "from experiments.probe_training import train_probes\n",
    "from experiments.pipeline_config import PipelineConfig\n",
    "from experiments.dataset_info import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = PipelineConfig()\n",
    "\n",
    "cfg.device = 'cuda'\n",
    "\n",
    "# llm_model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "llm_model_name = \"google/gemma-2-2b\"\n",
    "cfg.model_dtype = torch.bfloat16\n",
    "\n",
    "cfg.spurious_corr = False\n",
    "cfg.probe_train_set_size = 1000\n",
    "cfg.probe_test_set_size = 250\n",
    "cfg.probe_context_length = 128\n",
    "cfg.probe_batch_size = 250\n",
    "cfg.probe_epochs = 10\n",
    "cfg.probes_dir = 'probes'\n",
    "\n",
    "\n",
    "# cfg.dataset_name = 'bias_in_bios'\n",
    "# cfg.chosen_class_indices = [0, 1,]\n",
    "\n",
    "cfg.dataset_name ='amazon_reviews_1and5'\n",
    "\n",
    "chosen_classes = [\n",
    "    \"Beauty_and_Personal_Care\",\n",
    "    \"Books\",\n",
    "    \"Automotive\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Software\",\n",
    "    \"Sports_and_Outdoors\",\n",
    "]\n",
    "cfg.chosen_class_indices = [\n",
    "    amazon_category_dict[c] for c in chosen_classes\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# TODO: I think there may be a scoping issue with model and get_acts(), but we currently aren't using get_acts()\n",
    "model = LanguageModel(llm_model_name, device_map=cfg.device, dispatch=True, torch_dtype=cfg.model_dtype)\n",
    "only_model_name = llm_model_name.split(\"/\")[-1]\n",
    "model_eval_config = utils.ModelEvalConfig.from_full_model_name(llm_model_name)\n",
    "\n",
    "num_layers = len(model.model.layers) # TODO Make model agnostic\n",
    "# num_layers = len(model.gpt_neox.layers) # TODO Make model agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs_all_layers_scr = []\n",
    "# for layer in range(num_layers):\n",
    "for layer in [19]:\n",
    "    print(f\"Training probes for layer {layer}\")\n",
    "\n",
    "    date = \"0909\"\n",
    "    probe_path = f\"probes/{only_model_name}/{cfg.dataset_name}_scr{cfg.spurious_corr}_probes_layer{layer}_date{date}.pkl\"\n",
    "\n",
    "    test_accs_all_layers_scr.append(\n",
    "        # TODO adapt train_probes to reuse tokenized datasets\n",
    "        train_probes(\n",
    "            cfg.probe_train_set_size,\n",
    "            cfg.probe_test_set_size,\n",
    "            model,\n",
    "            context_length=cfg.probe_context_length,\n",
    "            probe_batch_size=cfg.probe_batch_size,\n",
    "            llm_batch_size=model_eval_config.llm_batch_size,\n",
    "            device=cfg.device,\n",
    "            probe_output_filename=probe_path,\n",
    "            dataset_name=cfg.dataset_name,\n",
    "            probe_dir=cfg.probes_dir,\n",
    "            llm_model_name=llm_model_name,\n",
    "            epochs=cfg.probe_epochs,\n",
    "            model_dtype=cfg.model_dtype,\n",
    "            spurious_correlation_removal=cfg.spurious_corr,\n",
    "            # column1_vals=cfg.column1_vals,\n",
    "            # column2_vals=cfg.column2_vals,\n",
    "            probe_layer=layer,\n",
    "            chosen_class_indices=cfg.chosen_class_indices,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Save test accuracies as json\n",
    "test_accs_all_layers_path_scr = f\"probes/{cfg.dataset_name}_scr{cfg.spurious_corr}_test_accs_date{date}.pkl\"\n",
    "with open(test_accs_all_layers_path_scr, \"wb\") as f:\n",
    "    pickle.dump(test_accs_all_layers_scr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = list(test_accs_all_layers_scr[0].keys())\n",
    "\n",
    "test_accs_per_class = {}\n",
    "for layer in range(len(test_accs_all_layers_scr)):\n",
    "    assert all_classes == list(test_accs_all_layers_scr[layer].keys())\n",
    "    for c, accs in test_accs_all_layers_scr[layer].items():\n",
    "        if c not in test_accs_per_class:\n",
    "            test_accs_per_class[c] = []\n",
    "        test_accs_per_class[c].append(accs[0])\n",
    "test_accs_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for c, accs in test_accs_per_class.items():\n",
    "    plt.plot(accs, label=f'{c}. {full_amazon_int_to_str[c]}')\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.legend(title='Class')\n",
    "plt.title(f\"Test accuracy of class probes across residual stream\\nfor {only_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_class_accuracies(test_accs_per_class, full_amazon_int_to_str, only_model_name, threshold=None):\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each class, optionally filtering by threshold\n",
    "    for c, accs in test_accs_per_class.items():\n",
    "        if threshold is None or max(accs) > threshold:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[11, 15, 19],  # Assuming layer numbers start from 0\n",
    "                y=accs,\n",
    "                mode='lines',\n",
    "                name=f'{c}. {full_amazon_int_to_str[c]}'\n",
    "            ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Probe test acc, {only_model_name},\" +\n",
    "              (f\" (max accuracy > {threshold}),\" if threshold is not None else \"\") +\n",
    "              f\" data: {cfg.dataset_name}\",\n",
    "        xaxis_title=\"Layer\",\n",
    "        yaxis_title=\"Test Accuracy\",\n",
    "        legend_title=\"Class\",\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "\n",
    "    # Update x-axis to show integer values\n",
    "    fig.update_xaxes(tick0=0, dtick=1)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# only display class if max acc above thresh\n",
    "display_thresh = 0.8\n",
    "plot_class_accuracies(test_accs_per_class, full_amazon_int_to_str, only_model_name, threshold=display_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
