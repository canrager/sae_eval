{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "DICTIONARIES_PATH = \"../dictionary_learning/dictionaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity_penalty(config: dict, trainer_class: str) -> float:\n",
    "    if trainer_class == \"TrainerTopK\":\n",
    "        return config[\"trainer\"][\"k\"]\n",
    "    elif trainer_class == \"PAnnealTrainer\":\n",
    "        return config[\"trainer\"][\"sparsity_penalty\"]\n",
    "    else:\n",
    "        return config[\"trainer\"][\"l1_penalty\"]\n",
    "\n",
    "\n",
    "def ae_config_results(ae_paths: list[str], dictionaries_path: str) -> dict[str, dict[str, float]]:\n",
    "    results = {}\n",
    "    for ae_path in ae_paths:\n",
    "        config_file = f\"{ae_path}/config.json\"\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        ae_name = ae_path.split(dictionaries_path)[1]\n",
    "\n",
    "        results[ae_name] = {}\n",
    "\n",
    "        trainer_class = config[\"trainer\"][\"trainer_class\"]\n",
    "        results[ae_name][\"trainer_class\"] = trainer_class\n",
    "        results[ae_name][\"l1_penalty\"] = get_sparsity_penalty(config, trainer_class)\n",
    "\n",
    "        results[ae_name][\"lr\"] = config[\"trainer\"][\"lr\"]\n",
    "        results[ae_name][\"dict_size\"] = config[\"trainer\"][\"dict_size\"]\n",
    "        if \"steps\" in config[\"trainer\"]:\n",
    "            results[ae_name][\"steps\"] = config[\"trainer\"][\"steps\"]\n",
    "        else:\n",
    "            results[ae_name][\"steps\"] = -1\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def add_custom_metric_results(\n",
    "    ae_paths: list[str],\n",
    "    results: dict[str, dict[str, float]],\n",
    "    metric_filename: str,\n",
    "    dictionaries_path: str,\n",
    "    metric_dict_key: Optional[str] = None,\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    for ae_path in ae_paths:\n",
    "        config_file = f\"{ae_path}/{metric_filename}\"\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            custom_metric_results = json.load(f)\n",
    "\n",
    "        ae_name = ae_path.split(dictionaries_path)[1]\n",
    "\n",
    "        if metric_dict_key:\n",
    "            results[ae_name][\"custom_metric\"] = custom_metric_results[metric_dict_key]\n",
    "        else:\n",
    "            for key, value in custom_metric_results.items():\n",
    "                results[ae_name][key] = value\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_l0_threshold(results: dict, l0_threshold: Optional[int]) -> dict:\n",
    "    if l0_threshold is not None:\n",
    "        filtered_results = {\n",
    "            path: data for path, data in results.items() if data[\"l0\"] <= l0_threshold\n",
    "        }\n",
    "\n",
    "        # Optional: Print how many results were filtered out\n",
    "        filtered_count = len(results) - len(filtered_results)\n",
    "        print(f\"Filtered out {filtered_count} results with L0 > {l0_threshold}\")\n",
    "\n",
    "        # Replace the original results with the filtered results\n",
    "        results = filtered_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Gated SAE”, “Gated SAE w/ p-annealing”, “Standard”, “Standard w/ p-annealing”\n",
    "label_lookup = {\n",
    "    \"StandardTrainer\": \"Standard\",\n",
    "    # \"PAnnealTrainer\": \"Standard w/ p-annealing\",\n",
    "    # \"GatedSAETrainer\": \"Gated SAE\",\n",
    "    \"TrainerJumpRelu\": \"JumpReLU\",\n",
    "    # \"GatedAnnealTrainer\": \"Gated SAE w/ p-annealing\",\n",
    "    \"TrainerTopK\": \"Top K\",\n",
    "    # \"Identity\": \"Identity\",\n",
    "}\n",
    "\n",
    "unique_trainers = list(label_lookup.keys())\n",
    "\n",
    "# create a dictionary mapping trainer types to marker shapes\n",
    "\n",
    "trainer_markers = {\n",
    "    \"StandardTrainer\": \"o\",\n",
    "    \"TrainerJumpRelu\": \"X\",\n",
    "    \"TrainerTopK\": \"^\",\n",
    "    \"GatedSAETrainer\": \"d\",\n",
    "}\n",
    "\n",
    "\n",
    "# default text size\n",
    "plt.rcParams.update({\"font.size\": 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3var_graph(\n",
    "    results: dict[str, dict[str, float]],\n",
    "    title: str,\n",
    "    custom_metric: str,\n",
    "    xlims: Optional[tuple[float, float]] = None,\n",
    "    ylims: Optional[tuple[float, float]] = None,\n",
    "    colorbar_label: str = \"Average Diff\",\n",
    "    output_filename: Optional[str] = None,\n",
    "    legend_location: str = \"lower right\",\n",
    "    x_axis_key: str = \"l0\",\n",
    "    y_axis_key: str = \"frac_recovered\",\n",
    "):\n",
    "    # Extract data from results\n",
    "    l0_values = [data[x_axis_key] for data in results.values()]\n",
    "    frac_recovered_values = [data[y_axis_key] for data in results.values()]\n",
    "    average_diff_values = [data[custom_metric] for data in results.values()]\n",
    "\n",
    "    # Create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Create a normalize object for color scaling\n",
    "    norm = Normalize(vmin=min(average_diff_values), vmax=max(average_diff_values))\n",
    "\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for trainer, marker in trainer_markers.items():\n",
    "        # Filter data for this trainer\n",
    "        trainer_data = {k: v for k, v in results.items() if v[\"trainer_class\"] == trainer}\n",
    "\n",
    "        if not trainer_data:\n",
    "            continue  # Skip this trainer if no data points\n",
    "\n",
    "        l0_values = [data[x_axis_key] for data in trainer_data.values()]\n",
    "        frac_recovered_values = [data[y_axis_key] for data in trainer_data.values()]\n",
    "        average_diff_values = [data[custom_metric] for data in trainer_data.values()]\n",
    "\n",
    "        # Plot data points\n",
    "        scatter = ax.scatter(\n",
    "            l0_values,\n",
    "            frac_recovered_values,\n",
    "            c=average_diff_values,\n",
    "            cmap=\"viridis\",\n",
    "            marker=marker,\n",
    "            s=100,\n",
    "            label=label_lookup[trainer],\n",
    "            norm=norm,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "\n",
    "        # custom legend stuff\n",
    "        _handle, _ = scatter.legend_elements(prop=\"sizes\")\n",
    "        _handle[0].set_markeredgecolor(\"black\")\n",
    "        _handle[0].set_markerfacecolor(\"white\")\n",
    "        _handle[0].set_markersize(10)\n",
    "        if marker == \"d\":\n",
    "            _handle[0].set_markersize(13)\n",
    "        handles += _handle\n",
    "        labels.append(label_lookup[trainer])\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(scatter, ax=ax, label=colorbar_label)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"L0 (Sparsity)\")\n",
    "    ax.set_ylabel(\"Loss Recovered (Fidelity)\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.legend(handles, labels, loc=legend_location)\n",
    "\n",
    "    # Set axis limits\n",
    "    if xlims:\n",
    "        ax.set_xlim(*xlims)\n",
    "    if ylims:\n",
    "        ax.set_ylim(*ylims)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# print(results)\n",
    "# if include_diff:\n",
    "# plot_3var_graph(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "\n",
    "def plot_interactive_3var_graph(\n",
    "    results: dict[str, dict[str, float]],\n",
    "    custom_color_metric: str,\n",
    "    xlims: Optional[tuple[float, float]] = None,\n",
    "    y_lims: Optional[tuple[float, float]] = None,\n",
    "    output_filename: Optional[str] = None,\n",
    "    x_axis_key: str = \"l0\",\n",
    "    y_axis_key: str = \"frac_recovered\",\n",
    "):\n",
    "    # Extract data from results\n",
    "    ae_paths = list(results.keys())\n",
    "    l0_values = [data[x_axis_key] for data in results.values()]\n",
    "    frac_recovered_values = [data[y_axis_key] for data in results.values()]\n",
    "\n",
    "    custom_metric_value = [data[custom_color_metric] for data in results.values()]\n",
    "\n",
    "    dict_size = [data[\"dict_size\"] for data in results.values()]\n",
    "    lr = [data[\"lr\"] for data in results.values()]\n",
    "    l1_penalty = [data[\"l1_penalty\"] for data in results.values()]\n",
    "\n",
    "    # Create the scatter plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=l0_values,\n",
    "            y=frac_recovered_values,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=custom_metric_value,  # Color points based on frac_recovered\n",
    "                colorscale=\"Viridis\",  # You can change this colorscale\n",
    "                showscale=True,\n",
    "            ),\n",
    "            text=[\n",
    "                f\"AE Path: {ae}<br>L0: {l0:.4f}<br>Frac Recovered: {fr:.4f}<br>Custom Metric: {ad:.4f}<br>Dict Size: {d:.4f}<br>LR: {l:.4f}<br>L1 Penalty: {l1:.4f}\"\n",
    "                for ae, l0, fr, ad, d, l, l1 in zip(\n",
    "                    ae_paths,\n",
    "                    l0_values,\n",
    "                    frac_recovered_values,\n",
    "                    custom_metric_value,\n",
    "                    dict_size,\n",
    "                    lr,\n",
    "                    l1_penalty,\n",
    "                )\n",
    "            ],\n",
    "            hoverinfo=\"text\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"L0 vs Loss Recovered\",\n",
    "        xaxis_title=\"L0 (Sparsity)\",\n",
    "        yaxis_title=\"Loss Recovered (Fidelity)\",\n",
    "        hovermode=\"closest\",\n",
    "    )\n",
    "\n",
    "    # Set axis limits\n",
    "    if xlims:\n",
    "        fig.update_xaxes(range=xlims)\n",
    "    if y_lims:\n",
    "        fig.update_yaxes(range=y_lims)\n",
    "\n",
    "    # Save and show the plot\n",
    "    if output_filename:\n",
    "        fig.write_html(output_filename)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# plot_interactive_3var_graph(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2var_graph(\n",
    "    results: dict[str, dict[str, float]],\n",
    "    custom_metric: str,\n",
    "    title: str = \"L0 vs Custom Metric\",\n",
    "    y_label: str = \"Custom Metric\",\n",
    "    xlims: Optional[tuple[float, float]] = None,\n",
    "    ylims: Optional[tuple[float, float]] = None,\n",
    "    output_filename: Optional[str] = None,\n",
    "    legend_location: str = \"lower right\",\n",
    "    original_acc: Optional[float] = None,\n",
    "    x_axis_key: str = \"l0\",\n",
    "):\n",
    "    # Extract data from results\n",
    "    l0_values = [data[x_axis_key] for data in results.values()]\n",
    "    custom_metric_values = [data[custom_metric] for data in results.values()]\n",
    "\n",
    "    # Create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for trainer, marker in trainer_markers.items():\n",
    "        # Filter data for this trainer\n",
    "        trainer_data = {k: v for k, v in results.items() if v[\"trainer_class\"] == trainer}\n",
    "\n",
    "        if not trainer_data:\n",
    "            continue  # Skip this trainer if no data points\n",
    "\n",
    "        l0_values = [data[x_axis_key] for data in trainer_data.values()]\n",
    "        custom_metric_values = [data[custom_metric] for data in trainer_data.values()]\n",
    "\n",
    "        # Plot data points\n",
    "        scatter = ax.scatter(\n",
    "            l0_values,\n",
    "            custom_metric_values,\n",
    "            marker=marker,\n",
    "            s=100,\n",
    "            label=label_lookup[trainer],\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "\n",
    "        # custom legend stuff\n",
    "        _handle, _ = scatter.legend_elements(prop=\"sizes\")\n",
    "        _handle[0].set_markeredgecolor(\"black\")\n",
    "        _handle[0].set_markerfacecolor(\"white\")\n",
    "        _handle[0].set_markersize(10)\n",
    "        if marker == \"d\":\n",
    "            _handle[0].set_markersize(13)\n",
    "        handles += _handle\n",
    "        labels.append(label_lookup[trainer])\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"L0 (Sparsity)\")\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    if original_acc:\n",
    "        ax.axhline(original_acc, color=\"red\", linestyle=\"--\", label=\"Original Probe Accuracy\")\n",
    "\n",
    "    ax.legend(handles, labels, loc=legend_location)\n",
    "\n",
    "    # Set axis limits\n",
    "    if xlims:\n",
    "        ax.set_xlim(*xlims)\n",
    "    if ylims:\n",
    "        ax.set_ylim(*ylims)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def plot_steps_vs_average_diff(\n",
    "    results_dict: dict,\n",
    "    steps_key: str = \"steps\",\n",
    "    avg_diff_key: str = \"average_diff\",\n",
    "    title: Optional[str] = None,\n",
    "    y_label: Optional[str] = None,\n",
    "    output_filename: Optional[str] = None,\n",
    "):\n",
    "    # Initialize a defaultdict to store data for each trainer\n",
    "    trainer_data = defaultdict(lambda: {\"steps\": [], \"average_diffs\": []})\n",
    "\n",
    "    all_steps = set()\n",
    "\n",
    "    # Extract data from the dictionary\n",
    "    for key, value in results_dict.items():\n",
    "        # Extract trainer number from the key\n",
    "        trainer = key.split(\"/\")[-1].split(\"_\")[1]  # Assuming format like \"trainer_1_...\"\n",
    "        layer = key.split(\"/\")[-2].split(\"_\")[-2]\n",
    "\n",
    "        if \"topk_ctx128\" in key:\n",
    "            trainer_type = \"TopK SAE\"\n",
    "        elif \"standard_ctx128\" in key:\n",
    "            trainer_type = \"Standard SAE\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown trainer type in key: {key}\")\n",
    "\n",
    "        step = int(value[steps_key])\n",
    "        avg_diff = value[avg_diff_key]\n",
    "\n",
    "        trainer_key = f\"{trainer_type} Layer {layer} Trainer {trainer}\"\n",
    "\n",
    "        trainer_data[trainer_key][\"steps\"].append(step)\n",
    "        trainer_data[trainer_key][\"average_diffs\"].append(avg_diff)\n",
    "        all_steps.add(step)\n",
    "\n",
    "    # Calculate average across all trainers\n",
    "    average_trainer_data = {\"steps\": [], \"average_diffs\": []}\n",
    "    for step in sorted(all_steps):\n",
    "        step_diffs = []\n",
    "        for data in trainer_data.values():\n",
    "            if step in data[\"steps\"]:\n",
    "                idx = data[\"steps\"].index(step)\n",
    "                step_diffs.append(data[\"average_diffs\"][idx])\n",
    "        if step_diffs:\n",
    "            average_trainer_data[\"steps\"].append(step)\n",
    "            average_trainer_data[\"average_diffs\"].append(np.mean(step_diffs))\n",
    "\n",
    "    # Add average_trainer_data to trainer_data\n",
    "    trainer_data[\"Average\"] = average_trainer_data\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot data for each trainer\n",
    "    for trainer_key, data in trainer_data.items():\n",
    "        steps = data[\"steps\"]\n",
    "        average_diffs = data[\"average_diffs\"]\n",
    "\n",
    "        # Sort the data by steps to ensure proper ordering\n",
    "        sorted_data = sorted(zip(steps, average_diffs))\n",
    "        steps, average_diffs = zip(*sorted_data)\n",
    "\n",
    "        # Find the maximum step value for this trainer\n",
    "        max_step = max(steps)\n",
    "\n",
    "        # Convert steps to percentages of max_step\n",
    "        step_percentages = [step / max_step * 100 for step in steps]\n",
    "\n",
    "        # Plot the line for this trainer\n",
    "        if trainer_key == \"Average\":\n",
    "            plt.plot(\n",
    "                step_percentages,\n",
    "                average_diffs,\n",
    "                marker=\"o\",\n",
    "                label=trainer_key,\n",
    "                linewidth=3,\n",
    "                color=\"red\",\n",
    "                zorder=10,\n",
    "            )  # Emphasized average line\n",
    "        else:\n",
    "            plt.plot(\n",
    "                step_percentages,\n",
    "                average_diffs,\n",
    "                marker=\"o\",\n",
    "                label=trainer_key,\n",
    "                alpha=0.3,\n",
    "                linewidth=1,\n",
    "            )  # More transparent individual lines\n",
    "\n",
    "    # log scale\n",
    "    # plt.xscale(\"log\")\n",
    "\n",
    "    # if not title:\n",
    "    #     title = f'{steps_key.capitalize()} vs {avg_diff_key.replace(\"_\", \" \").capitalize()}'\n",
    "\n",
    "    if not y_label:\n",
    "        y_label = avg_diff_key.replace(\"_\", \" \").capitalize()\n",
    "\n",
    "    plt.xlabel(\"Training Progess (%)\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)  # More transparent grid\n",
    "\n",
    "    if len(trainer_data) < 50 and False:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "\n",
    "    # Adjust layout to prevent clipping of tick-labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_folders(path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Recursively get a list of folders that contain an ae.pt file, starting the search from the given path\n",
    "    \"\"\"\n",
    "    folder_names = []\n",
    "\n",
    "    # We use config.json so it also works for data folders\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if \"config.json\" in files:\n",
    "            folder_names.append(root)\n",
    "\n",
    "    return folder_names\n",
    "\n",
    "\n",
    "def check_for_empty_folders(ae_group_paths: list[str]) -> bool:\n",
    "    \"\"\"So your run doesn't crash / do nothing interesting because folder 13 is empty.\"\"\"\n",
    "    for ae_group_path in ae_group_paths:\n",
    "        if len(get_nested_folders(ae_group_path)) == 0:\n",
    "            raise ValueError(f\"No folders found in {ae_group_path}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_ae_group_paths(\n",
    "    dictionaries_path: str, sweep_name: str, submodule_trainers: Optional[dict]\n",
    ") -> list[str]:\n",
    "    if submodule_trainers is None:\n",
    "        return [f\"{dictionaries_path}/{sweep_name}\"]\n",
    "\n",
    "    ae_group_paths = []\n",
    "\n",
    "    for submodule in submodule_trainers.keys():\n",
    "        trainer_ids = submodule_trainers[submodule][\"trainer_ids\"]\n",
    "\n",
    "        base_filename = f\"{dictionaries_path}/{sweep_name}/{submodule}\"\n",
    "\n",
    "        if trainer_ids is None:\n",
    "            ae_group_paths.append(base_filename)\n",
    "        else:\n",
    "            for trainer_id in trainer_ids:\n",
    "                ae_group_paths.append(f\"{base_filename}/trainer_{trainer_id}\")\n",
    "\n",
    "    check_for_empty_folders(ae_group_paths)\n",
    "\n",
    "    return ae_group_paths\n",
    "\n",
    "\n",
    "def get_ae_paths(ae_group_paths: list[str]) -> list[str]:\n",
    "    ae_paths = []\n",
    "    for ae_group_path in ae_group_paths:\n",
    "        ae_paths.extend(get_nested_folders(ae_group_path))\n",
    "    return ae_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be used to load autoencoders from the nested folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "# from dictionary_learning.dictionary import (\n",
    "#     IdentityDict,\n",
    "#     GatedAutoEncoder,\n",
    "#     AutoEncoderNew,\n",
    "#     JumpReluAutoEncoder,\n",
    "# )\n",
    "# from dictionary_learning.trainers.top_k import AutoEncoderTopK\n",
    "\n",
    "# def load_dictionary(model, base_path: str, device: str, verbose: bool = True):\n",
    "#     if verbose:\n",
    "#         print(f\"Loading dictionary from {base_path}\")\n",
    "#     ae_path = f\"{base_path}/ae.pt\"\n",
    "#     config_path = f\"{base_path}/config.json\"\n",
    "\n",
    "#     with open(config_path, \"r\") as f:\n",
    "#         config = json.load(f)\n",
    "\n",
    "#     submodule_str = config[\"trainer\"][\"submodule_name\"]\n",
    "#     layer = config[\"trainer\"][\"layer\"]\n",
    "#     model_name = config[\"trainer\"][\"lm_name\"]\n",
    "#     dict_class = config[\"trainer\"][\"dict_class\"]\n",
    "\n",
    "#     first_model_name = model.config._name_or_path\n",
    "#     assert type(first_model_name) == str, \"Model name must be a string\"\n",
    "\n",
    "#     assert (\n",
    "#         model_name == first_model_name\n",
    "#     ), f\"Model name {model_name} does not match first model name {first_model_name}\"\n",
    "\n",
    "#     submodule = get_submodule(model, submodule_str, layer)\n",
    "\n",
    "#     if dict_class == \"AutoEncoder\":\n",
    "#         dictionary = AutoEncoder.from_pretrained(ae_path, device=device)\n",
    "#     # elif dict_class == \"IdentityDict\":\n",
    "#     #     dictionary = IdentityDict.from_pretrained(ae_path, device=device)\n",
    "#     elif dict_class == \"GatedAutoEncoder\":\n",
    "#         dictionary = GatedAutoEncoder.from_pretrained(ae_path, device=device)\n",
    "#     elif dict_class == \"AutoEncoderNew\":\n",
    "#         dictionary = AutoEncoderNew.from_pretrained(ae_path, device=device)\n",
    "#     elif dict_class == \"AutoEncoderTopK\":\n",
    "#         k = config[\"trainer\"][\"k\"]\n",
    "#         dictionary = AutoEncoderTopK.from_pretrained(ae_path, k=k, device=device)\n",
    "#     elif dict_class == \"JumpReluAutoEncoder\":\n",
    "#         dictionary = JumpReluAutoEncoder.from_pretrained(ae_path, device=device)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Dictionary class {dict_class} not supported\")\n",
    "\n",
    "#     return submodule, dictionary, config\n",
    "\n",
    "\n",
    "# def get_submodule(model, submodule_str: str, layer: int):\n",
    "#     allowed_submodules = [\"attention_out\", \"mlp_out\", \"resid_post\", \"unembed\"]\n",
    "\n",
    "#     model_architecture = model.config.architectures[0]\n",
    "\n",
    "#     assert type(model_architecture) == str, \"Model architecture must be a string\"\n",
    "\n",
    "#     if model_architecture == \"GPTNeoXForCausalLM\":\n",
    "#         if \"attention_out\" in submodule_str:\n",
    "#             submodule = model.gpt_neox.layers[layer].attention\n",
    "#         elif \"mlp_out\" in submodule_str:\n",
    "#             submodule = model.gpt_neox.layers[layer].mlp\n",
    "#         elif \"resid_post\" in submodule_str:\n",
    "#             submodule = model.gpt_neox.layers[layer]\n",
    "#         elif \"unembed\" in submodule_str:\n",
    "#             submodule = model.embed_out\n",
    "#         else:\n",
    "#             raise ValueError(f\"submodule_str must contain one of {allowed_submodules}\")\n",
    "#     elif model_architecture == \"Gemma2ForCausalLM\":\n",
    "#         if \"resid_post\" in submodule_str:\n",
    "#             submodule = model.model.layers[layer]\n",
    "#         elif \"unembed\" in submodule_str:\n",
    "#             submodule = model.lm_head\n",
    "#         else:\n",
    "#             raise ValueError(f\"submodule_str must contain one of {allowed_submodules}\")\n",
    "#     else:\n",
    "#         raise ValueError(f\"Model architecture {model_architecture} not supported\")\n",
    "\n",
    "#     return submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename_prefix = \"images/\"\n",
    "\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.makedirs(\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is an optional way to create `plotting_results` from Can / Adam's nested folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If trainer_ids is None, all trainers will be plotted\n",
    "# trainer_ids = None\n",
    "\n",
    "# # To only graph e.g. trainer_ids 1 and 2, set trainer_ids = [1, 2]\n",
    "# # To graph all layers at once, do something like this:\n",
    "\n",
    "# # ae_sweep_paths = {\n",
    "# #     \"gemma-2-2b_sweep_standard_ctx128_ef8_0824\": None,\n",
    "# # }\n",
    "\n",
    "# ae_sweep_paths = {\n",
    "#     \"gemma-2-2b_sweep_standard_ctx128_ef8_0824\": {\n",
    "#         \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "#     \"gemma-2-2b_sweep_topk_ctx128_ef8_0824\": {\n",
    "#         \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "#     \"gemma-2-2b_sweep_jumprelu_0902\": {\n",
    "#         \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# DICTIONARIES_PATH = \"../dictionary_learning/dictionaries/09_20_gemma_tpp_results\"\n",
    "\n",
    "\n",
    "# l0_threshold = 500\n",
    "\n",
    "# model = \"Gemma-2-2B\"\n",
    "\n",
    "# l0_threshold = None\n",
    "\n",
    "# no_title = True\n",
    "\n",
    "\n",
    "# ae_paths = []\n",
    "\n",
    "# for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "#     ae_group_paths = get_ae_group_paths(DICTIONARIES_PATH, sweep_name, submodule_trainers)\n",
    "#     ae_paths.extend(get_ae_paths(ae_group_paths))\n",
    "\n",
    "\n",
    "# plotting_results = ae_config_results(ae_paths, DICTIONARIES_PATH + \"/\")\n",
    "# plotting_results = add_custom_metric_results(\n",
    "#     ae_paths, plotting_results, \"eval_results.json\", DICTIONARIES_PATH + \"/\"\n",
    "# )\n",
    "# print(plotting_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of how to update `plotting_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting_results = add_custom_metric_results(\n",
    "#     ae_paths, plotting_results, \"tpp_results.json\", DICTIONARIES_PATH + \"/\", metric_dict_key=None\n",
    "# )\n",
    "\n",
    "# print(plotting_results.keys())\n",
    "\n",
    "# first_key = next(iter(plotting_results.keys()))\n",
    "# print(plotting_results[first_key].keys())\n",
    "\n",
    "# scr_filename = \"all_scr_results.json\"\n",
    "\n",
    "# with open(scr_filename, \"r\") as f:\n",
    "#     scr_results = json.load(f)\n",
    "\n",
    "# for ae_name in scr_results.keys():\n",
    "#     assert ae_name in plotting_results, f\"{ae_name} not found in plotting_results\"\n",
    "#     plotting_results[ae_name].update(scr_results[ae_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = \"all_scr_tpp_gemma_results.json\"\n",
    "\n",
    "with open(results_filename, \"r\") as f:\n",
    "    plotting_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plotting_results = filter_by_l0_threshold(plotting_results, l0_threshold)\n",
    "\n",
    "custom_metric = \"l1_penalty\"\n",
    "custom_metric = \"cossim\"\n",
    "\n",
    "title = f\"L0 vs Loss Recovered vs {custom_metric.replace('_', ' ').capitalize()}\"\n",
    "\n",
    "plot_3var_graph(plotting_results, title, custom_metric, output_filename=f\"{image_filename_prefix}{custom_metric}_3var.png\")\n",
    "plot_2var_graph(plotting_results, custom_metric, title=title, output_filename=f\"{image_filename_prefix}{custom_metric}_2var.png\")\n",
    "plot_interactive_3var_graph(plotting_results, custom_metric)\n",
    "\n",
    "# At this point, if there's any additional .json files located alongside the ae.pt and eval_results.json\n",
    "# You can easily adapt them to be included in the plotting_results dictionary by using something similar to add_ae_config_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric = \"tpp_auto_interp_threshold_50_total_metric\"\n",
    "\n",
    "title = f\"L0 vs Loss Recovered vs {custom_metric}\"\n",
    "\n",
    "plot_3var_graph(plotting_results, title, custom_metric, output_filename=f\"{image_filename_prefix}{custom_metric}_3var.png\")\n",
    "plot_2var_graph(plotting_results, custom_metric, title=title, y_label=\"Custom Metric\", output_filename=f\"{image_filename_prefix}{custom_metric}_2var.png\")\n",
    "plot_interactive_3var_graph(plotting_results, custom_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(\n",
    "    plotting_results: dict[str, dict[str, float]],\n",
    "    metric_names: list[str],\n",
    "    ae_names: Optional[list[str]] = None,\n",
    "    title: str = \"Metric Correlation Heatmap\",\n",
    "    output_filename: str = None,\n",
    "    figsize: tuple = (12, 10),\n",
    "    cmap: str = \"coolwarm\",\n",
    "    annot: bool = True,\n",
    "):\n",
    "    # If ae_names is not provided, use all ae_names from plotting_results\n",
    "    if ae_names is None:\n",
    "        ae_names = list(plotting_results.keys())\n",
    "\n",
    "    # If metric_names is not provided, use all metric names from the first ae_name\n",
    "    # if metric_names is None:\n",
    "    #     metric_names = list(plotting_results[ae_names[0]].keys())\n",
    "\n",
    "    # Create a DataFrame from the plotting_results\n",
    "    data = []\n",
    "    for ae in ae_names:\n",
    "        row = [plotting_results[ae].get(metric, np.nan) for metric in metric_names]\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data, index=ae_names, columns=metric_names)\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(corr_matrix, annot=annot, cmap=cmap, vmin=-1, vmax=1, center=0)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if output_filename is provided\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_10_total_metric\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_attrib_threshold_500_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_10_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "]\n",
    "\n",
    "plot_correlation_heatmap(plotting_results, metric_names=metric_keys, ae_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from typing import Optional\n",
    "\n",
    "def plot_metric_scatter(\n",
    "    plotting_results: dict[str, dict[str, float]],\n",
    "    metric_x: str,\n",
    "    metric_y: str,\n",
    "    ae_names: Optional[list[str]] = None,\n",
    "    title: str = \"Metric Comparison Scatter Plot\",\n",
    "    output_filename: Optional[str] = None,\n",
    "    figsize: tuple = (10, 8),\n",
    "):\n",
    "    # If ae_names is not provided, use all ae_names from plotting_results\n",
    "    if ae_names is None:\n",
    "        ae_names = list(plotting_results.keys())\n",
    "\n",
    "    # Extract x and y values for the specified metrics\n",
    "    x_values = [plotting_results[ae].get(metric_x, float('nan')) for ae in ae_names]\n",
    "    y_values = [plotting_results[ae].get(metric_y, float('nan')) for ae in ae_names]\n",
    "\n",
    "    # Remove any NaN values\n",
    "    valid_data = [(x, y, ae) for x, y, ae in zip(x_values, y_values, ae_names) if not (np.isnan(x) or np.isnan(y))]\n",
    "    if not valid_data:\n",
    "        print(\"No valid data points after removing NaN values.\")\n",
    "        return\n",
    "\n",
    "    x_values, y_values, valid_ae_names = zip(*valid_data)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x_values = np.array(x_values)\n",
    "    y_values = np.array(y_values)\n",
    "\n",
    "    # Calculate correlation coefficients\n",
    "    r, p_value = stats.pearsonr(x_values, y_values)\n",
    "    r_squared = r ** 2\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.scatterplot(x=x_values, y=y_values)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(metric_x)\n",
    "    plt.ylabel(metric_y)\n",
    "    plt.title(f\"{title}\\nr = {r:.4f}, r² = {r_squared:.4f}, p = {p_value:.4f}\")\n",
    "\n",
    "    # Add annotations for each point\n",
    "    # for i, ae in enumerate(valid_ae_names):\n",
    "    #     plt.annotate(ae, (x_values[i], y_values[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "    # Add a trend line\n",
    "    sns.regplot(x=x_values, y=y_values, scatter=False, color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if output_filename is provided\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print correlation coefficients\n",
    "    print(f\"Pearson correlation coefficient (r): {r:.4f}\")\n",
    "    print(f\"Coefficient of determination (r²): {r_squared:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# plot_metric_scatter(plotting_results, metric_x=\"l0\", metric_y=\"frac_recovered\", title=\"L0 vs Fraction Recovered\")\n",
    "\n",
    "metric1 = \"tpp_attrib_threshold_50_total_metric\"\n",
    "metric2 = \"tpp_auto_interp_threshold_50_total_metric\"\n",
    "title = f\"{metric1} vs {metric2}\"\n",
    "\n",
    "plot_metric_scatter(plotting_results, metric_x=metric1, metric_y=metric2, title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_key = next(iter(plotting_results.keys()))\n",
    "print(plotting_results[first_key].keys())\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_20_total_metric\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_attrib_threshold_500_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_20_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "    \"scr_bias_shift_dir2_threshold_20\",\n",
    "    \"scr_bias_shift_dir2_threshold_50\",\n",
    "    \"scr_bias_shift_dir1_threshold_20\",\n",
    "    \"scr_bias_shift_dir1_threshold_50\",\n",
    "    \"scr_attrib_dir2_threshold_20\",\n",
    "    \"scr_attrib_dir2_threshold_50\",\n",
    "    \"scr_attrib_dir1_threshold_20\",\n",
    "    \"scr_attrib_dir1_threshold_50\",\n",
    "]\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "    \"scr_bias_shift_dir2_threshold_50\",\n",
    "    # \"scr_bias_shift_dir1_threshold_50\",\n",
    "    \"scr_attrib_dir2_threshold_50\",\n",
    "    # \"scr_attrib_dir1_threshold_50\",\n",
    "]\n",
    "\n",
    "plot_correlation_heatmap(plotting_results, metric_names=metric_keys, ae_names=None, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric = \"scr_bias_shift_dir1_threshold_50\"\n",
    "custom_metric = \"scr_attrib_dir2_threshold_50\"\n",
    "\n",
    "title = f\"L0 vs Loss Recovered vs {custom_metric}\"\n",
    "\n",
    "plot_3var_graph(plotting_results, title, custom_metric)\n",
    "plot_2var_graph(plotting_results, custom_metric, title=title, y_label=\"Custom Metric\")\n",
    "plot_interactive_3var_graph(plotting_results, custom_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plotting_results[first_key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = \"scr_bias_shift_dir2_threshold_50\"\n",
    "metric2 = \"scr_attrib_dir2_threshold_50\"\n",
    "title = f\"{metric1} vs {metric2}\"\n",
    "\n",
    "plot_metric_scatter(plotting_results, metric_x=metric1, metric_y=metric2, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = \"tpp_auto_interp_threshold_50_total_metric\"\n",
    "metric2 = \"scr_attrib_dir2_threshold_50\"\n",
    "title = f\"{metric1} vs {metric2}\"\n",
    "plot_metric_scatter(plotting_results, metric_x=metric1, metric_y=metric2, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
