{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import einops\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "from datasets import load_dataset\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "# Configuration\n",
    "DEBUGGING = False\n",
    "DEVICE = \"cuda:0\"\n",
    "SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "ACTIVATION_DIM = 512\n",
    "LAYER = 4\n",
    "MIN_SAMPLES_PER_GROUP = 1024\n",
    "\n",
    "# Set up paths and model\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "tracer_kwargs = dict(scan=DEBUGGING, validate=DEBUGGING)\n",
    "model = LanguageModel(\"EleutherAI/pythia-70m-deduped\", device_map=DEVICE, dispatch=True)\n",
    "\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_and_prepare_dataset():\n",
    "    dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "    df = pd.DataFrame(dataset[\"train\"])\n",
    "    df[\"combined_label\"] = df[\"profession\"].astype(str) + \"_\" + df[\"gender\"].astype(str)\n",
    "    return dataset, df\n",
    "\n",
    "\n",
    "# Profession dictionary\n",
    "profession_dict = {\n",
    "    \"accountant\": 0,\n",
    "    \"architect\": 1,\n",
    "    \"attorney\": 2,\n",
    "    \"chiropractor\": 3,\n",
    "    \"comedian\": 4,\n",
    "    \"composer\": 5,\n",
    "    \"dentist\": 6,\n",
    "    \"dietitian\": 7,\n",
    "    \"dj\": 8,\n",
    "    \"filmmaker\": 9,\n",
    "    \"interior_designer\": 10,\n",
    "    \"journalist\": 11,\n",
    "    \"model\": 12,\n",
    "    \"nurse\": 13,\n",
    "    \"painter\": 14,\n",
    "    \"paralegal\": 15,\n",
    "    \"pastor\": 16,\n",
    "    \"personal_trainer\": 17,\n",
    "    \"photographer\": 18,\n",
    "    \"physician\": 19,\n",
    "    \"poet\": 20,\n",
    "    \"professor\": 21,\n",
    "    \"psychologist\": 22,\n",
    "    \"rapper\": 23,\n",
    "    \"software_engineer\": 24,\n",
    "    \"surgeon\": 25,\n",
    "    \"teacher\": 26,\n",
    "    \"yoga_teacher\": 27,\n",
    "}\n",
    "profession_dict_rev = {v: k for k, v in profession_dict.items()}\n",
    "\n",
    "\n",
    "# Visualization\n",
    "def plot_label_distribution(df):\n",
    "    label_counts = df[\"combined_label\"].value_counts().sort_index()\n",
    "    labels = [\n",
    "        f\"{profession_dict_rev[int(label.split('_')[0])]} ({'Male' if label.split('_')[1] == '0' else 'Female'})\"\n",
    "        for label in label_counts.index\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(labels, label_counts)\n",
    "    plt.xlabel(\"(Profession x Gender) Label\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.title(\"Number of Samples per (Profession x Gender) Label\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Dataset balancing and preparation\n",
    "def get_balanced_dataset(dataset, min_samples_per_group: int, train: bool, random_seed: int = SEED):\n",
    "    df = pd.DataFrame(dataset[\"train\" if train else \"test\"])\n",
    "    balanced_df_list = []\n",
    "\n",
    "    for profession in df[\"profession\"].unique():\n",
    "        prof_df = df[df[\"profession\"] == profession]\n",
    "        min_count = prof_df[\"gender\"].value_counts().min()\n",
    "\n",
    "        if min_samples_per_group and min_count < min_samples_per_group:\n",
    "            continue\n",
    "\n",
    "        cutoff = min_samples_per_group or min_count\n",
    "        balanced_prof_df = pd.concat([\n",
    "            group.sample(n=cutoff, random_state=random_seed)\n",
    "            for _, group in prof_df.groupby(\"gender\")\n",
    "        ]).reset_index(drop=True)\n",
    "        balanced_df_list.append(balanced_prof_df)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_df_list).reset_index(drop=True)\n",
    "    grouped = balanced_df.groupby(\"profession\")[\"hard_text\"].apply(list)\n",
    "    return {label: shuffle(texts) for label, texts in grouped.items()}\n",
    "\n",
    "\n",
    "def sample_from_classes(data_dict, chosen_class):\n",
    "    total_samples = len(data_dict[chosen_class])\n",
    "    all_classes = list(data_dict.keys())\n",
    "    all_classes.remove(chosen_class)\n",
    "    random_class_indices = random.choices(all_classes, k=total_samples)\n",
    "\n",
    "    samples_count = defaultdict(int)\n",
    "    for class_idx in random_class_indices:\n",
    "        samples_count[class_idx] += 1\n",
    "\n",
    "    sampled_data = []\n",
    "    for class_idx, count in samples_count.items():\n",
    "        sampled_data.extend(random.sample(data_dict[class_idx], count))\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def create_labeled_dataset(data_dict, chosen_class, batch_size):\n",
    "    in_class_data = data_dict[chosen_class]\n",
    "    other_class_data = sample_from_classes(data_dict, chosen_class)\n",
    "\n",
    "    combined_dataset = [(sample, 0) for sample in in_class_data] + [\n",
    "        (sample, 1) for sample in other_class_data\n",
    "    ]\n",
    "    random.shuffle(combined_dataset)\n",
    "\n",
    "    bio_texts, bio_labels = zip(*combined_dataset)\n",
    "    text_batches = [\n",
    "        bio_texts[i : i + batch_size] for i in range(0, len(combined_dataset), batch_size)\n",
    "    ]\n",
    "    label_batches = [\n",
    "        t.tensor(bio_labels[i : i + batch_size], device=DEVICE)\n",
    "        for i in range(0, len(combined_dataset), batch_size)\n",
    "    ]\n",
    "\n",
    "    return text_batches, label_batches\n",
    "\n",
    "\n",
    "# Probe model and training\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "def get_acts(text):\n",
    "    with t.no_grad():\n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.input[1][\"attention_mask\"]\n",
    "            acts = model.gpt_neox.layers[LAYER].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value\n",
    "\n",
    "\n",
    "@t.no_grad()\n",
    "def get_all_activations(text_batches: list[list[str]]) -> t.Tensor:\n",
    "    all_acts_list_BD = []\n",
    "    for text_batch_BL in tqdm(text_batches, desc=\"Getting activations\"):\n",
    "        with model.trace(text_batch_BL, **tracer_kwargs):\n",
    "            attn_mask = model.input[1][\"attention_mask\"]\n",
    "            acts_BLD = model.gpt_neox.layers[LAYER].output[0]\n",
    "            acts_BLD = acts_BLD * attn_mask[:, :, None]\n",
    "            acts_BD = acts_BLD.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts_BD = acts_BD.save()\n",
    "        all_acts_list_BD.append(acts_BD.value)\n",
    "\n",
    "    all_acts_bD = t.cat(all_acts_list_BD, dim=0)\n",
    "    return all_acts_bD\n",
    "\n",
    "\n",
    "def train_probe(\n",
    "    train_input_batches: list,\n",
    "    train_label_batches: list[t.Tensor],\n",
    "    test_input_batches: list,\n",
    "    test_label_batches: list[t.Tensor],\n",
    "    get_acts: Callable,\n",
    "    precomputed_acts: bool,\n",
    "    lr: float = 1e-2,\n",
    "    epochs: int = 1,\n",
    "    dim: int = ACTIVATION_DIM,\n",
    "    seed: int = SEED,\n",
    "):\n",
    "    \"\"\"input_batches can be a list of tensors or strings. If strings, get_acts must be provided.\"\"\"\n",
    "\n",
    "    if type(train_input_batches[0]) == str or type(test_input_batches[0]) == str:\n",
    "        assert precomputed_acts == False\n",
    "    elif type(train_input_batches[0]) == t.Tensor or type(test_input_batches[0]) == t.Tensor:\n",
    "        assert precomputed_acts == True\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to(DEVICE)\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = t.zeros(epochs, len(train_input_batches))\n",
    "    for epoch in range(epochs):\n",
    "        batch_idx = 0\n",
    "        for inputs, labels in zip(train_input_batches, train_label_batches):\n",
    "            if precomputed_acts:\n",
    "                acts_BD = inputs\n",
    "            else:\n",
    "                acts_BD = get_acts(inputs)\n",
    "            logits_B = probe(acts_BD)\n",
    "            loss = criterion(logits_B, t.tensor(labels, device=DEVICE, dtype=t.float32))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses[epoch, batch_idx] = loss\n",
    "            batch_idx += 1\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} Loss: {loss.item()}\")\n",
    "\n",
    "        train_accuracy = test_probe(train_input_batches[:30], train_label_batches[:30], probe, get_acts, precomputed_acts)\n",
    "\n",
    "        print(f\"Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "        test_accuracy = test_probe(\n",
    "            test_input_batches, test_label_batches, probe, get_acts, precomputed_acts\n",
    "        )\n",
    "        print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    return probe, losses\n",
    "\n",
    "\n",
    "def test_probe(\n",
    "    input_batches: list,\n",
    "    label_batches: list[t.Tensor],\n",
    "    probe: Probe,\n",
    "    get_acts: Callable,\n",
    "    precomputed_acts: bool,\n",
    "):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "        for input_batch, labels_B in zip(input_batches, label_batches):\n",
    "            if precomputed_acts:\n",
    "                acts_BD = input_batch\n",
    "            else:\n",
    "                acts_BD = get_acts(input_batch)\n",
    "            logits_B = probe(acts_BD)\n",
    "            preds_B = (logits_B > 0.0).long()\n",
    "            corrects.append((preds_B == labels_B).float())\n",
    "        return t.cat(corrects).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, df = load_and_prepare_dataset()\n",
    "plot_label_distribution(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bios_gender_balanced = get_balanced_dataset(dataset, 1250, train=True)\n",
    "test_bios_gender_balanced = get_balanced_dataset(dataset, 250, train=False)\n",
    "\n",
    "\n",
    "\n",
    "def ensure_shared_keys(train_data: dict, test_data: dict) -> tuple[dict, dict]:\n",
    "    # Find keys that are in test but not in train\n",
    "    test_only_keys = set(test_data.keys()) - set(train_data.keys())\n",
    "\n",
    "    # Find keys that are in train but not in test\n",
    "    train_only_keys = set(train_data.keys()) - set(test_data.keys())\n",
    "\n",
    "    # Remove keys from test that are not in train\n",
    "    for key in test_only_keys:\n",
    "        print(f\"Removing {key} from test set\")\n",
    "        del test_data[key]\n",
    "\n",
    "    # Remove keys from train that are not in test\n",
    "    for key in train_only_keys:\n",
    "        print(f\"Removing {key} from train set\")\n",
    "        del train_data[key]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "train_bios_gender_balanced, test_bios_gender_balanced = ensure_shared_keys(\n",
    "    train_bios_gender_balanced, test_bios_gender_balanced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_bios_gender_balanced.keys())\n",
    "print(train_bios_gender_balanced[0])\n",
    "print(len(train_bios_gender_balanced[22]))\n",
    "\n",
    "def batch_list(input_list, batch_size):\n",
    "    return [input_list[i:i + batch_size] for i in range(0, len(input_list), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes, losses = {}, {}\n",
    "\n",
    "all_train_acts = {}\n",
    "all_test_acts = {}\n",
    "\n",
    "for i, profession in enumerate(train_bios_gender_balanced.keys()):\n",
    "    t.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"Training probe for profession: {profession}\")\n",
    "    train_input_batches = batch_list(train_bios_gender_balanced[profession], BATCH_SIZE)\n",
    "\n",
    "    test_input_batches = batch_list(test_bios_gender_balanced[profession], BATCH_SIZE)\n",
    "\n",
    "    all_train_acts[profession] = get_all_activations(train_input_batches)\n",
    "\n",
    "    all_test_acts[profession] = get_all_activations(test_input_batches)\n",
    "\n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "    # probe, loss = train_probe(text_batches, label_batches, get_acts, epochs=1)\n",
    "    # probes[profession] = probe\n",
    "    # losses[profession] = loss\n",
    "\n",
    "# os.makedirs('trained_bib_probes', exist_ok=True)\n",
    "# t.save(probes, 'trained_bib_probes/probes_0705.pt')\n",
    "# t.save(losses, 'trained_bib_probes/losses_0705.pt')\n",
    "\n",
    "# bios_test = get_balanced_dataset(dataset, min_samples_per_group=50, train=False)\n",
    "# test_accuracies = {}\n",
    "# for profession, probe in probes.items():\n",
    "#     text_batches, label_batches = create_labeled_dataset(bios_test, profession, BATCH_SIZE)\n",
    "#     accuracy = test_probe(text_batches, label_batches, probe, get_acts)\n",
    "#     print(f'Profession: {profession}, Accuracy: {accuracy}')\n",
    "#     test_accuracies[profession] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_probe_data(\n",
    "    all_activations: dict[int, t.Tensor],\n",
    "    class_idx: int,\n",
    "    batch_size: int,\n",
    ") -> tuple[t.Tensor, t.Tensor]:\n",
    "    positive_acts = all_activations[class_idx]\n",
    "\n",
    "    num_positive = len(positive_acts)\n",
    "\n",
    "    # Collect all negative class activations and labels\n",
    "    negative_acts = []\n",
    "    for idx, batched_acts in all_activations.items():\n",
    "        if idx != class_idx:\n",
    "            negative_acts.append(batched_acts)\n",
    "\n",
    "    negative_acts = t.cat(negative_acts)\n",
    "\n",
    "    # Randomly select num_positive samples from negative class\n",
    "    indices = t.randperm(len(negative_acts))[:num_positive]\n",
    "    selected_negative_acts = negative_acts[indices]\n",
    "\n",
    "    assert selected_negative_acts.shape == positive_acts.shape\n",
    "\n",
    "    # Combine positive and negative samples\n",
    "    combined_acts = t.cat([positive_acts, selected_negative_acts])\n",
    "    combined_labels = t.zeros(len(combined_acts), device=DEVICE)\n",
    "    combined_labels[num_positive:] = 1\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    shuffle_indices = t.randperm(len(combined_acts))\n",
    "    shuffled_acts = combined_acts[shuffle_indices]\n",
    "    shuffled_labels = combined_labels[shuffle_indices]\n",
    "\n",
    "    # Reshape into lists of tensors with specified batch_size\n",
    "    num_samples = len(shuffled_acts)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "\n",
    "    batched_acts = [\n",
    "        shuffled_acts[i * batch_size : (i + 1) * batch_size] for i in range(num_batches)\n",
    "    ]\n",
    "    batched_labels = [\n",
    "        shuffled_labels[i * batch_size : (i + 1) * batch_size] for i in range(num_batches)\n",
    "    ]\n",
    "\n",
    "    return batched_acts, batched_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(True)\n",
    "\n",
    "probe_batch_size = 32\n",
    "\n",
    "for profession in all_train_acts.keys():\n",
    "    train_acts, train_labels = prepare_probe_data(all_train_acts, profession, probe_batch_size)\n",
    "\n",
    "    test_acts, test_labels = prepare_probe_data(all_test_acts, profession, probe_batch_size)\n",
    "    \n",
    "    probe, loss = train_probe(train_acts, train_labels, test_acts, test_labels, get_acts, precomputed_acts=True, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
