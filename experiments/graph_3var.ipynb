{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import experiments.utils as utils\n",
    "\n",
    "\n",
    "\n",
    "DICTIONARIES_PATH = \"../dictionary_learning/dictionaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a potential issue as we currently assume that all SAEs have the same classes.\n",
    "\n",
    "\n",
    "def get_classes(first_path: str) -> list[int]:\n",
    "    class_accuracies_file = f\"{first_path}/class_accuracies.pkl\"\n",
    "    with open(class_accuracies_file, \"rb\") as f:\n",
    "        class_accuracies = pickle.load(f)\n",
    "    return list(class_accuracies[-1].keys())\n",
    "\n",
    "def get_sparsity_penalty(config: dict, trainer_class: str) -> float:\n",
    "    if trainer_class == \"TrainerTopK\":\n",
    "        return config[\"trainer\"][\"k\"]\n",
    "    elif trainer_class == \"PAnnealTrainer\":\n",
    "        return config[\"trainer\"][\"sparsity_penalty\"]\n",
    "    else:\n",
    "        return config[\"trainer\"][\"l1_penalty\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_plotting_dict(\n",
    "    ae_paths: list[str],\n",
    "    threshold: float,\n",
    "    intended_filter_class_ids: list[int],\n",
    "    unintended_filter_class_ids: list[int],\n",
    "    include_ablation_diff: bool,\n",
    "    filename_counter: str,\n",
    ") -> dict:\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for ae_path in ae_paths:\n",
    "\n",
    "        eval_results_file = f\"{ae_path}/eval_results.json\"\n",
    "\n",
    "        if not os.path.exists(eval_results_file):\n",
    "            print(f\"Warning: {eval_results_file} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        with open(eval_results_file, \"r\") as f:\n",
    "            eval_results = json.load(f)\n",
    "\n",
    "        l0 = eval_results[\"l0\"]\n",
    "        frac_recovered = eval_results[\"frac_recovered\"]\n",
    "\n",
    "        results[ae_path] = {\n",
    "            \"l0\": l0,\n",
    "            \"frac_recovered\": frac_recovered,\n",
    "        }\n",
    "\n",
    "        config_file = f\"{ae_path}/config.json\"\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        trainer_class = config[\"trainer\"][\"trainer_class\"]\n",
    "        results[ae_path][\"trainer_class\"] = trainer_class\n",
    "        results[ae_path][\"l1_penalty\"] = get_sparsity_penalty(config, trainer_class)\n",
    "\n",
    "        results[ae_path][\"lr\"] = config[\"trainer\"][\"lr\"]\n",
    "        results[ae_path][\"dict_size\"] = config[\"trainer\"][\"dict_size\"]\n",
    "\n",
    "        # Use these to analyze hyperparameters vs L0 / frac recovered\n",
    "        results[ae_path][\"average_diff\"] = results[ae_path][\"l1_penalty\"]\n",
    "        # results[ae_path][\"average_diff\"] = results[ae_path][\"lr\"]\n",
    "        # results[ae_path][\"average_diff\"] = results[ae_path][\"dict_size\"]\n",
    "\n",
    "        if not include_ablation_diff:\n",
    "            continue\n",
    "\n",
    "        intended_diffs = []\n",
    "        unintended_diffs = []\n",
    "\n",
    "        class_accuracies_file = f\"{ae_path}/class_accuracies{filename_counter}.pkl\"\n",
    "\n",
    "        if not os.path.exists(class_accuracies_file):\n",
    "            print(\n",
    "                f\"Warning: {class_accuracies_file} does not exist. Removing this path from results.\"\n",
    "            )\n",
    "            del results[ae_path]\n",
    "            continue\n",
    "\n",
    "        with open(class_accuracies_file, \"rb\") as f:\n",
    "            class_accuracies = pickle.load(f)\n",
    "\n",
    "        classes = list(class_accuracies[-1].keys())\n",
    "\n",
    "        # print(class_accuracies)\n",
    "        # for class_id in classes:\n",
    "        #     print(class_accuracies[-1][class_id])\n",
    "\n",
    "        for class_id in classes:\n",
    "\n",
    "            if intended_filter_class_ids and class_id not in intended_filter_class_ids:\n",
    "                continue\n",
    "\n",
    "            clean = class_accuracies[-1][class_id]\n",
    "            # print(ae_path)\n",
    "            # print(class_accuracies[class_id])\n",
    "            patched = class_accuracies[class_id][threshold][class_id]\n",
    "\n",
    "            diff = clean - patched\n",
    "            intended_diffs.append(diff)\n",
    "\n",
    "        for intended_class_id in classes:\n",
    "\n",
    "            if intended_filter_class_ids and intended_class_id not in intended_filter_class_ids:\n",
    "                continue\n",
    "\n",
    "            for unintended_class_id in classes:\n",
    "                if intended_class_id == unintended_class_id:\n",
    "                    continue\n",
    "\n",
    "                if (\n",
    "                    unintended_filter_class_ids\n",
    "                    and unintended_class_id not in unintended_filter_class_ids\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                clean = class_accuracies[-1][unintended_class_id]\n",
    "                patched = class_accuracies[intended_class_id][threshold][unintended_class_id]\n",
    "                diff = clean - patched\n",
    "                unintended_diffs.append(diff)\n",
    "\n",
    "        average_intended_diff = sum(intended_diffs) / len(intended_diffs)\n",
    "        average_unintended_diff = sum(unintended_diffs) / len(unintended_diffs)\n",
    "        average_diff = average_intended_diff - average_unintended_diff\n",
    "\n",
    "        results[ae_path][\"average_diff\"] = average_diff\n",
    "        results[ae_path][\"average_intended_diff\"] = average_intended_diff\n",
    "        results[ae_path][\"average_unintended_diff\"] = average_unintended_diff\n",
    "\n",
    "    # results[ae_path][\"average_diff\"] = average_intended_diff\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_l0_threshold(results: dict, l0_threshold: Optional[int]) -> dict:\n",
    "    if l0_threshold is not None:\n",
    "        filtered_results = {\n",
    "            path: data for path, data in results.items() \n",
    "            if data['l0'] <= l0_threshold\n",
    "        }\n",
    "        \n",
    "        # Optional: Print how many results were filtered out\n",
    "        filtered_count = len(results) - len(filtered_results)\n",
    "        print(f\"Filtered out {filtered_count} results with L0 > {l0_threshold}\")\n",
    "        \n",
    "        # Replace the original results with the filtered results\n",
    "        results = filtered_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Gated SAE”, “Gated SAE w/ p-annealing”, “Standard”, “Standard w/ p-annealing”\n",
    "label_lookup = {\n",
    "    \"StandardTrainer\": \"Standard\",\n",
    "    \"PAnnealTrainer\": \"Standard w/ p-annealing\",\n",
    "    \"GatedSAETrainer\": \"Gated SAE\",\n",
    "    # \"GatedAnnealTrainer\": \"Gated SAE w/ p-annealing\",\n",
    "    \"TrainerTopK\": \"Top K\",\n",
    "    # \"Identity\": \"Identity\",\n",
    "}\n",
    "\n",
    "unique_trainers = list(label_lookup.keys())\n",
    "\n",
    "# create a dictionary mapping trainer types to marker shapes\n",
    "trainer_markers = dict(zip(unique_trainers, [\"o\", \"X\", \"^\", \"d\"]))\n",
    "\n",
    "\n",
    "\n",
    "def plot_3var_graph(\n",
    "    results: dict,\n",
    "    threshold: float,\n",
    "    custom_metric: str,\n",
    "    xlims: Optional[tuple[float, float]] = None,\n",
    "    ylims: Optional[tuple[float, float]] = None,\n",
    "    colorbar_label: str = \"Average Diff\",\n",
    "    output_filename: Optional[str] = None,\n",
    "    legend_location: str = \"lower right\",\n",
    "):\n",
    "    # Extract data from results\n",
    "    l0_values = [data['l0'] for data in results.values()]\n",
    "    frac_recovered_values = [data['frac_recovered'] for data in results.values()]\n",
    "    average_diff_values = [data[custom_metric] for data in results.values()]\n",
    "\n",
    "    # Create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Create a normalize object for color scaling\n",
    "    norm = Normalize(vmin=min(average_diff_values), vmax=max(average_diff_values))\n",
    "\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for trainer, marker in trainer_markers.items():\n",
    "\n",
    "        # Filter data for this trainer\n",
    "        trainer_data = {k: v for k, v in results.items() if v['trainer_class'] == trainer}\n",
    "        \n",
    "        if not trainer_data:\n",
    "            continue  # Skip this trainer if no data points\n",
    "\n",
    "        l0_values = [data['l0'] for data in trainer_data.values()]\n",
    "        frac_recovered_values = [data['frac_recovered'] for data in trainer_data.values()]\n",
    "        average_diff_values = [data[custom_metric] for data in trainer_data.values()]\n",
    "\n",
    "        # Plot data points\n",
    "        scatter = ax.scatter(\n",
    "            l0_values,\n",
    "            frac_recovered_values,\n",
    "            c=average_diff_values,\n",
    "            cmap=\"viridis\",\n",
    "            marker=marker,\n",
    "            s=100,\n",
    "            label=label_lookup[trainer],\n",
    "            norm=norm,\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "\n",
    "        # custom legend stuff\n",
    "        _handle, _ = scatter.legend_elements(prop=\"sizes\")\n",
    "        _handle[0].set_markeredgecolor(\"black\")\n",
    "        _handle[0].set_markerfacecolor(\"white\")\n",
    "        _handle[0].set_markersize(10)\n",
    "        if marker == \"d\":\n",
    "            _handle[0].set_markersize(13)\n",
    "        handles += _handle\n",
    "        labels.append(label_lookup[trainer])\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(scatter, ax=ax, label=colorbar_label)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"L0\")\n",
    "    ax.set_ylabel(\"Fraction Recovered\")\n",
    "    ax.set_title(f\"\"\"Ablating Top {threshold} features from attribution patching.\n",
    "                 Color is (intended class difference - unintended class difference).\n",
    "                 intended class = gender probe, unintended class = profession probe\"\"\")\n",
    "\n",
    "    ax.legend(handles, labels, loc=legend_location)\n",
    "\n",
    "    # Set axis limits\n",
    "    if xlims:\n",
    "        ax.set_xlim(*xlims)\n",
    "    if ylims:\n",
    "        ax.set_ylim(*ylims)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# print(results)\n",
    "# if include_diff:\n",
    "# plot_3var_graph(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def plot_interactive_3var_graph(\n",
    "    results: Dict[str, Dict[str, float]],\n",
    "    xlims: Optional[tuple[float, float]] = None,\n",
    "    y_lims: Optional[tuple[float, float]] = None,\n",
    "    output_filename: Optional[str] = None,\n",
    "):\n",
    "    # Extract data from results\n",
    "    ae_paths = list(results.keys())\n",
    "    l0_values = [data['l0'] for data in results.values()]\n",
    "    frac_recovered_values = [data['frac_recovered'] for data in results.values()]\n",
    "\n",
    "    custom_metric_value = [data['average_diff'] for data in results.values()]\n",
    "\n",
    "    dict_size = [data['dict_size'] for data in results.values()]\n",
    "    lr = [data['lr'] for data in results.values()]\n",
    "    l1_penalty = [data['l1_penalty'] for data in results.values()]\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add trace\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=l0_values,\n",
    "        y=frac_recovered_values,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=custom_metric_value,  # Color points based on frac_recovered\n",
    "            colorscale='Viridis',  # You can change this colorscale\n",
    "            showscale=True\n",
    "        ),\n",
    "        text=[f'AE Path: {ae}<br>L0: {l0:.4f}<br>Frac Recovered: {fr:.4f}<br>Average Diff: {ad:.4f}<br>Dict Size: {d:.4f}<br>LR: {l:.4f}<br>L1 Penalty: {l1:.4f}' \n",
    "              for ae, l0, fr, ad, d, l, l1 in zip(ae_paths, l0_values, frac_recovered_values, custom_metric_value, dict_size, lr, l1_penalty)],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='L0 vs Fraction Recovered',\n",
    "        xaxis_title='L0',\n",
    "        yaxis_title='Fraction Recovered',\n",
    "        hovermode='closest'\n",
    "    )\n",
    "\n",
    "    # Set axis limits\n",
    "    if xlims:\n",
    "        fig.update_xaxes(range=xlims)\n",
    "    if y_lims:\n",
    "        fig.update_yaxes(range=y_lims)\n",
    "\n",
    "    # Save and show the plot\n",
    "    if output_filename:\n",
    "        fig.write_html(output_filename)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_interactive_3var_graph(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2var_graph(\n",
    "    results: dict,\n",
    "    xlims: Optional[tuple[float, float]] = None,\n",
    "    y_lims: Optional[tuple[float, float]] = None,\n",
    "    output_filename: Optional[str] = None,\n",
    "):\n",
    "    # Extract data from results\n",
    "    l0_values = [data['l0'] for data in results.values()]\n",
    "    frac_recovered_values = [data['frac_recovered'] for data in results.values()]\n",
    "\n",
    "    # Create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot data points\n",
    "    ax.scatter(\n",
    "        l0_values,\n",
    "        frac_recovered_values,\n",
    "        s=100,\n",
    "        edgecolor=\"black\",\n",
    "        c=\"blue\"  # You can change this color as needed\n",
    "    )\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"L0\")\n",
    "    ax.set_ylabel(\"Fraction Recovered\")\n",
    "    ax.set_title(\"L0 vs Fraction Recovered\")\n",
    "\n",
    "    # Set axis limits\n",
    "    if xlims:\n",
    "        ax.set_xlim(*xlims)\n",
    "    if y_lims:\n",
    "        ax.set_ylim(*y_lims)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_2var_graph(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to generate graphs, where you manually populate sweep_name and submodule_trainers\n",
    "sweep_name = \"pythia70m_test_sae\"\n",
    "submodule_trainers = {\"resid_post_layer_3\": {\"trainer_ids\": [0]}}\n",
    "\n",
    "# Current recommended way to generate graphs. You can copy paste ae_sweep_paths directly from bib_intervention.py\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": [1, 7, 11, 18]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\n",
    "        # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": [2, 6, 10, 14, 18]},\n",
    "        # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "    },\n",
    "    \"pythia70m_sweep_topk_ctx128_0730\": {\n",
    "        # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_3\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": [2, 6, 10, 14, 18]},\n",
    "    },        \n",
    "    \"pythia70m_sweep_gated_ctx128_0730\": {\n",
    "        # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_3\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": [2, 6, 10, 14, 18]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# ae_sweep_paths = {\n",
    "#     \"pythia70m_sweep_topk_ctx128_0730\": {\n",
    "#         # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "#         # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "#         # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "#         \"resid_post_layer_3\": {\"trainer_ids\": None},\n",
    "#         # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "# If not empty, this will filter to only include the specified class ids\n",
    "\n",
    "intended_filter_class_ids = []\n",
    "unintended_filter_class_ids = []\n",
    "intended_filter_class_ids = [0, 1, 2]\n",
    "# unintended_filter_class_ids = [-4]\n",
    "\n",
    "threshold = 0.1\n",
    "threshold = 20\n",
    "\n",
    "ae_paths = []\n",
    "\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "\n",
    "    ae_group_paths = utils.get_ae_group_paths(\n",
    "        DICTIONARIES_PATH, sweep_name, submodule_trainers\n",
    "    )\n",
    "    ae_paths.extend(utils.get_ae_paths(ae_group_paths))\n",
    "\n",
    "\n",
    "# If you haven't ran bib_intervenion.py before and there's no class accuracies file\n",
    "# Set include_diff to False. You can still view the L0 / Loss recovered curve\n",
    "# And you can view various SAE hyperparameters like sparsity penalty in the 3 variable plot\n",
    "include_ablation_diff = True\n",
    "filename_counter = \"1\"\n",
    "\n",
    "print(ae_paths)\n",
    "\n",
    "results = get_plotting_dict(\n",
    "    ae_paths,\n",
    "    threshold,\n",
    "    intended_filter_class_ids,\n",
    "    unintended_filter_class_ids,\n",
    "    include_ablation_diff,\n",
    "    filename_counter,\n",
    ")\n",
    "\n",
    "l0_threshold = None\n",
    "# l0_threshold = 500\n",
    "\n",
    "custom_metric1 = \"average_diff\"\n",
    "custom_metric2 = \"average_intended_diff\"\n",
    "custom_metric3 = \"average_unintended_diff\"\n",
    "\n",
    "results = filter_by_l0_threshold(results, l0_threshold)\n",
    "\n",
    "plot_3var_graph(results, threshold, custom_metric1)\n",
    "plot_interactive_3var_graph(results)\n",
    "plot_3var_graph(results, threshold, custom_metric2)\n",
    "plot_3var_graph(results, threshold, custom_metric3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
