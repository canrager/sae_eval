{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import datasets\n",
    "\n",
    "import einops\n",
    "import dictionary_learning.interp as interp\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from collections import namedtuple\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import experiments.utils as utils\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "\n",
    "from experiments.autointerp import highlight_top_activations\n",
    "\n",
    "DEBUGGING = True\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model_dtype = torch.bfloat16\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=DEVICE,\n",
    "    dispatch=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=model_dtype,\n",
    ")\n",
    "\n",
    "\n",
    "context_length = 128\n",
    "\n",
    "dataset = datasets.load_dataset(\"georgeyw/dsir-pile-100k\", streaming=False)\n",
    "\n",
    "data = model.tokenizer(dataset[\"train\"][\"contents\"][:10000], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=context_length).to(DEVICE).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "batch_size = 250\n",
    "\n",
    "batched_data = utils.batch_inputs(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary from ../dictionary_learning/dictionaries/pythia70m_sweep_standard_ctx128_0712/resid_post_layer_3/trainer_6\n",
      "torch.Size([16384])\n",
      "tensor([    1.3203,     0.3594,     0.2402,     0.1455,     0.0762,     0.0605,\n",
      "            0.0503,     0.0464,     0.0403,     0.0337,     0.0198,     0.0150,\n",
      "            0.0128,     0.0124,     0.0119,     0.0107,     0.0101,     0.0099,\n",
      "            0.0093,     0.0091,     0.0085,     0.0085,     0.0084,     0.0079,\n",
      "            0.0076,     0.0075,     0.0073,     0.0072,     0.0069,     0.0069,\n",
      "            0.0069,     0.0068,     0.0067,     0.0067,     0.0065,     0.0056,\n",
      "            0.0055,     0.0055,     0.0053,     0.0052,     0.0050,     0.0049,\n",
      "            0.0049,     0.0048,     0.0047,     0.0047,     0.0046,     0.0045,\n",
      "            0.0045,     0.0045,     0.0042,     0.0041,     0.0040,     0.0040,\n",
      "            0.0039,     0.0039,     0.0039,     0.0038,     0.0038,     0.0038,\n",
      "            0.0038,     0.0038,     0.0037,     0.0036,     0.0035,     0.0034,\n",
      "            0.0034,     0.0034,     0.0034,     0.0034,     0.0033,     0.0033,\n",
      "            0.0033,     0.0032,     0.0032,     0.0032,     0.0031,     0.0031,\n",
      "            0.0031,     0.0030,     0.0030,     0.0030,     0.0029,     0.0029,\n",
      "            0.0029,     0.0029,     0.0028,     0.0028,     0.0028,     0.0028,\n",
      "            0.0028,     0.0028,     0.0027,     0.0027,     0.0027,     0.0026,\n",
      "            0.0026,     0.0026,     0.0026,     0.0026,     0.0026,     0.0025,\n",
      "            0.0025,     0.0025,     0.0025,     0.0025,     0.0024,     0.0024,\n",
      "            0.0024,     0.0024,     0.0024,     0.0024,     0.0024,     0.0023,\n",
      "            0.0023,     0.0023,     0.0023,     0.0023,     0.0023,     0.0022,\n",
      "            0.0022,     0.0022,     0.0022,     0.0022,     0.0022,     0.0022,\n",
      "            0.0022,     0.0022,     0.0022,     0.0022,     0.0022,     0.0022,\n",
      "            0.0022,     0.0021,     0.0021,     0.0021,     0.0021,     0.0021,\n",
      "            0.0021,     0.0021,     0.0020,     0.0020,     0.0020,     0.0020,\n",
      "            0.0020,     0.0020,     0.0020,     0.0020,     0.0020,     0.0020,\n",
      "            0.0020,     0.0020,     0.0019,     0.0019,     0.0019,     0.0019,\n",
      "            0.0019,     0.0019,     0.0019,     0.0019,     0.0019,     0.0019,\n",
      "            0.0019,     0.0019,     0.0018,     0.0018,     0.0018,     0.0018,\n",
      "            0.0018,     0.0018,     0.0018,     0.0018,     0.0018,     0.0018,\n",
      "            0.0018,     0.0018,     0.0018,     0.0017,     0.0017,     0.0017,\n",
      "            0.0017,     0.0017,     0.0017,     0.0017,     0.0017,     0.0016,\n",
      "            0.0016,     0.0016,     0.0016,     0.0016,     0.0016,     0.0016,\n",
      "            0.0016,     0.0016,     0.0016,     0.0016,     0.0015,     0.0015,\n",
      "            0.0015,     0.0015,     0.0015,     0.0015,     0.0015,     0.0015,\n",
      "            0.0015,     0.0015,     0.0015,     0.0015,     0.0015,     0.0015,\n",
      "            0.0015,     0.0014,     0.0014,     0.0014,     0.0014,     0.0014,\n",
      "            0.0014,     0.0014,     0.0014,     0.0014,     0.0014,     0.0014,\n",
      "            0.0014,     0.0014,     0.0014,     0.0014,     0.0014,     0.0014,\n",
      "            0.0014,     0.0014,     0.0014,     0.0014,     0.0013,     0.0013,\n",
      "            0.0013,     0.0013,     0.0013,     0.0013,     0.0013,     0.0013,\n",
      "            0.0013,     0.0013,     0.0013,     0.0013,     0.0013,     0.0013,\n",
      "            0.0013,     0.0013,     0.0012,     0.0012,     0.0012,     0.0012,\n",
      "            0.0012,     0.0012,     0.0012,     0.0012,     0.0012,     0.0012,\n",
      "            0.0012,     0.0012,     0.0012,     0.0012,     0.0012,     0.0012,\n",
      "            0.0012,     0.0012,     0.0012,     0.0012,     0.0012,     0.0012,\n",
      "            0.0012,     0.0012,     0.0012,     0.0012,     0.0012,     0.0011,\n",
      "            0.0011,     0.0011,     0.0011,     0.0011,     0.0011,     0.0011,\n",
      "            0.0011,     0.0011,     0.0011,     0.0011,     0.0011,     0.0011,\n",
      "            0.0011,     0.0011,     0.0011,     0.0011,     0.0011,     0.0011,\n",
      "            0.0011,     0.0011,     0.0011,     0.0011,     0.0011,     0.0011,\n",
      "            0.0011,     0.0011,     0.0011,     0.0011,     0.0011,     0.0011,\n",
      "            0.0011,     0.0011,     0.0011,     0.0011,     0.0011,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0010,\n",
      "            0.0010,     0.0010,     0.0010,     0.0010,     0.0010,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0009,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0009,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0009,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0009,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0009,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0009,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0009,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0009,     0.0009,\n",
      "            0.0009,     0.0009,     0.0009,     0.0009,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0008,     0.0008,     0.0008,     0.0008,     0.0008,\n",
      "            0.0008,     0.0007,     0.0007,     0.0007,     0.0007,     0.0007,\n",
      "            0.0007,     0.0007,     0.0007,     0.0007,     0.0007,     0.0007,\n",
      "            0.0007,     0.0007], dtype=torch.bfloat16)\n",
      "tensor([ 7265,  3597,  4648,  5923,  1104, 10316,  3767, 11238, 11797, 10602,\n",
      "         9857,  1885,  7226,  9051, 13803, 16363, 12167, 11559, 12024,  9813,\n",
      "        14264,  6716, 10862,  2622,  8046,  3915,  5687, 12045, 11784,  3159,\n",
      "        14838, 10703,  7336,  1135,  2960,  2774, 15313, 15437,  2247,  2003,\n",
      "        10998,  6302,  2876,  4591,   700,  8777,  9898, 10178,  2744,  1790,\n",
      "         2850, 15718,  4133,  6625, 13500,  7643,  5698, 13484,  8594, 12543,\n",
      "         5206,  5281, 16321,  1215,  8151, 13284,  2771, 13473, 14665,  9068,\n",
      "        13655, 15906,  8136, 14749,  2406, 14962,  3057,  2828,  4478,   201,\n",
      "         6198, 12066,  2326,  9064,  9285,  6651, 14135,  4259,  5573,  4715,\n",
      "         9356,  4096,  9909, 15200,  8139,  9586, 11861,  3295, 14822,  9945,\n",
      "        15799,  6096,  5805, 10246, 15832,  5855, 13742,  9399,  7622,  9323,\n",
      "          944,  1449,  4555,  2271,  5945,  6179,   662,  9314,  5032, 15827,\n",
      "         1967,  4977, 13043,  7853, 11599, 13945,  6382, 13397,  6855,  1298,\n",
      "         1264,  8495, 10827,  6435, 14848,  8761,  2591,   463,  9463,  5774,\n",
      "        14396, 10243,  3179, 11769, 12892, 15370,  3920,  2302, 14134,   238,\n",
      "         1436,  4378,  1437,   298, 11573, 13279,  6730, 16245,  3105, 11753,\n",
      "        15300,  9306,   484,  2063, 10113,  7038,  6399,  1600, 14146, 11090,\n",
      "         1099,  2270,  9768,  2076,  8412,  6099,  3203, 10608,  9317,  8138,\n",
      "         6926, 13035,  9783,  4004,  8523,  2870,  8716,  7747,  8667,  3255,\n",
      "        10875,   836,  8787, 11274, 12917,  9063,  3713,  7191,     5,  3545,\n",
      "         2705,  5272,  9584, 10419, 11002, 15650,  1621, 12597, 13708,  6429,\n",
      "        12948, 14106,  7796,   336,  7330,  7668, 11188,  3097,  6151,  2504,\n",
      "         5457,  6107,  6199,  8137,  1634, 13494,  4957, 11985,  3124, 11139,\n",
      "         3758,  4837, 10785,  3432, 13125,  6490,  9284,   791,  3640, 15761,\n",
      "         8012, 11592, 12317,  1344,  8162, 12177,  6698,  3426,  6841, 13270,\n",
      "         8358,  1614,  1182, 13172, 13164, 12260,  7412,  2959,  7159,   871,\n",
      "         3528,  4953, 11082,  5172, 14422,  3463, 12337,  1586,  8155,  2098,\n",
      "        14282,  8034, 16171, 15451,  8437,  6670, 12303, 11516, 10835, 11105,\n",
      "         3864,  8576, 14338, 10494, 14942,  8239, 13553, 11607,   402, 10156,\n",
      "         9191,  6823,  7723, 16214, 12983, 10006,  4635,  1020, 13686, 10236,\n",
      "         3032,  9375, 13150, 10105, 14241, 10928,  4319,    96,  2109, 11945,\n",
      "         7560, 14520,   131,  5776,  5401, 14092,  7448,  2544,  2835,  6894,\n",
      "        10668, 12267,  1719, 14568,  7004,  5704,  7831, 11440,  8211, 13023,\n",
      "        11991, 15603, 10222,  2123,  1927, 10009,  3788,  4659,  8632,  2663,\n",
      "         3202,  4141,  5720, 15356, 13539, 12273, 10448, 14658,  8096,  5908,\n",
      "        14939,  6689, 11637,   539,  9272,   739,  9686,  9789,  8061,  3398,\n",
      "         5205, 15161, 12756, 14757, 13540,  5956,  9955, 12146,  5913,  7778,\n",
      "        15218,  3562,  5677, 11758, 10276, 15720,  9780,  3103,  2839,  9378,\n",
      "        13089, 11980,  5875,  6916,  2032,  6287, 13707,  8689, 12780, 12913,\n",
      "         2777,  1701, 15797, 10802,  7264,  9815, 12520,  6904,  1557, 11242,\n",
      "          888,  2301, 11923,   436,  1162,  5049, 13163,  9315,  3860,   328,\n",
      "         8454,  2147,  8456,   371, 11591, 15412, 14040,  1167, 13111,  1754,\n",
      "           93,  2502, 12042, 15009,  5370, 15646, 16080,  7546, 13413,  7154,\n",
      "        11463,  5376, 14676,  9808, 13146,  8368,  4047,  5586, 10365,  5181,\n",
      "         6711, 15000,    74, 14839, 10947,  3048, 15106,  4517, 14414, 11847,\n",
      "         4911,  3657, 16113, 10455, 15108,  7783, 12508,  3332, 12518,  5295,\n",
      "        12935,  3039,  7679,  7173, 14205,   355,  9722,  4129, 12430,  9229,\n",
      "         7277, 11183, 14031,  7146,  2840,  7631,  2750,  1961,  4503,  7541,\n",
      "        11361,  4973,   697,  3221,  2397, 16324,  4933, 15582,  9256,  3592,\n",
      "        13286,  2698,  1549, 11676,   457,  5182,  3085, 11369, 14109,  5119])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 40/40 [00:07<00:00,  5.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "import dictionary_learning.interp as interp\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from collections import namedtuple\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import experiments.utils as utils\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "\n",
    "\n",
    "def get_max_activating_prompts(\n",
    "    model,\n",
    "    submodule,\n",
    "    tokenized_inputs_bL: list[list[dict]],\n",
    "    dim_indices: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    dictionary=None,\n",
    "    n_inputs: int = 512,\n",
    "    k: int = 30,\n",
    "):\n",
    "\n",
    "    assert n_inputs % batch_size == 0\n",
    "\n",
    "    feature_count = dim_indices.shape[0]\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    max_activating_indices_FK = torch.zeros((feature_count, k), device=device, dtype=torch.int)\n",
    "    max_activations_FK = torch.zeros((feature_count, k), device=device, dtype=torch.float32)\n",
    "    max_tokens_FKL = torch.zeros((feature_count, k, context_length), device=device, dtype=torch.int)\n",
    "    max_activations_FKL = torch.zeros((feature_count, k, context_length), device=device, dtype=torch.float32)\n",
    "\n",
    "    for i, inputs in tqdm(enumerate(tokenized_inputs_bL), total=len(tokenized_inputs_bL)):\n",
    "\n",
    "        batch_offset = i * batch_size\n",
    "        inputs_BL = inputs['input_ids']\n",
    "\n",
    "        with torch.no_grad(), model.trace(inputs, **tracer_kwargs):\n",
    "            activations_BLD = submodule.output\n",
    "            if type(activations_BLD.shape) == tuple:\n",
    "                activations_BLD = activations_BLD[0]\n",
    "            activations_BLF = dictionary.encode(activations_BLD)\n",
    "            activations_BLF = activations_BLF[:, :, dim_indices].save()\n",
    "\n",
    "        activations_FBL = einops.rearrange(activations_BLF.value, 'B L F -> F B L')\n",
    "        # Use einops to find the max activation per input\n",
    "        activations_FB = einops.reduce(activations_FBL, 'F B L -> F B', 'max')\n",
    "        tokens_FBL = einops.repeat(inputs_BL, 'B L -> F B L', F=feature_count)\n",
    "        \n",
    "        # Keep track of input indices\n",
    "        indices_B = torch.arange(batch_offset, batch_offset + batch_size, device=device)\n",
    "        indices_FB = einops.repeat(indices_B, 'B -> F B', F=feature_count)\n",
    "\n",
    "        # Concatenate current batch activations and indices with the previous ones\n",
    "        combined_activations_FB = torch.cat([max_activations_FK, activations_FB], dim=1)\n",
    "        combined_indices_FB = torch.cat([max_activating_indices_FK, indices_FB], dim=1)\n",
    "        combined_activations_FBL = torch.cat([max_activations_FKL, activations_FBL], dim=1)\n",
    "        combined_tokens_FBL = torch.cat([max_tokens_FKL, tokens_FBL], dim=1)\n",
    "\n",
    "        # Sort and keep top k activations for each dimension\n",
    "        topk_activations_FK, topk_indices_FK = torch.topk(combined_activations_FB, k, dim=1)\n",
    "        max_activations_FK = topk_activations_FK\n",
    "\n",
    "        feature_indices_F1 = torch.arange(feature_count, device=device)[:, None]\n",
    "        max_activating_indices_FK = combined_indices_FB[feature_indices_F1, topk_indices_FK]\n",
    "        max_activations_FKL = combined_activations_FBL[feature_indices_F1, topk_indices_FK]\n",
    "        max_tokens_FKL = combined_tokens_FBL[feature_indices_F1, topk_indices_FK]\n",
    "            \n",
    "\n",
    "    return max_tokens_FKL, max_activations_FKL\n",
    "\n",
    "dictionaries_path = \"../dictionary_learning/dictionaries\"\n",
    "\n",
    "# Current recommended way to generate graphs. You can copy paste ae_sweep_paths directly from bib_intervention.py\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\"resid_post_layer_3\": {\"trainer_ids\": [6]}},\n",
    "    # \"pythia70m_sweep_gated_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [9]}},\n",
    "    # \"pythia70m_sweep_topk_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [10]}},\n",
    "    # \"gemma-2-2b_sweep_topk_ctx128_0817\": {\"resid_post_layer_12\": {\"trainer_ids\": [2]}}, \n",
    "}\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "filter_class_ids = []\n",
    "# filter_class_ids = [-4, -2]\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(dictionaries_path, sweep_name, submodule_trainers)\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "# TODO\n",
    "# Add bias_in_bios dataset option\n",
    "# Cosine sim with probes\n",
    "# Vector per class probe\n",
    "\n",
    "ae_path = ae_paths[0]\n",
    "submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)\n",
    "\n",
    "filename_counter = \"\"\n",
    "class_id = -2\n",
    "\n",
    "node_effects_filename = f\"{ae_path}/node_effects{filename_counter}.pkl\"\n",
    "\n",
    "with open(node_effects_filename, \"rb\") as f:\n",
    "    node_effects = pickle.load(f)\n",
    "\n",
    "effects = node_effects[class_id]\n",
    "\n",
    "print(effects.shape)\n",
    "\n",
    "k = 500\n",
    "top_k_values, top_k_indices = torch.topk(effects, k)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(top_k_values)\n",
    "print(top_k_indices)\n",
    "\n",
    "all_indices = torch.arange(0, effects.shape[0])\n",
    "all_indices = top_k_indices\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "max_tokens_FKL, max_activations_FKL = get_max_activating_prompts(model, submodule, batched_data, all_indices, batch_size, dictionary, 1000, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "Feature 7265 has cosine similarity 0.6078369617462158\n",
      "Feature 3597 has cosine similarity -0.5437126159667969\n",
      "Feature 4648 has cosine similarity 0.6239863038063049\n",
      "Feature 5923 has cosine similarity 0.15908387303352356\n",
      "Feature 1104 has cosine similarity 0.2630188465118408\n",
      "Feature 10316 has cosine similarity 0.13465353846549988\n",
      "Feature 3767 has cosine similarity -0.1064731627702713\n",
      "Feature 11238 has cosine similarity -0.2627173662185669\n",
      "Feature 11797 has cosine similarity 0.3990252614021301\n",
      "Feature 10602 has cosine similarity -0.602030873298645\n"
     ]
    }
   ],
   "source": [
    "with open(\"trained_bib_probes/pythia-70m-deduped/probes_ctx_len_128.pkl\", \"rb\") as f:\n",
    "    probes = pickle.load(f)\n",
    "\n",
    "probe_vec = probes[class_id].net.weight.squeeze()\n",
    "print(probe_vec.shape)\n",
    "\n",
    "for i in range(10):\n",
    "    sae_feat_idx = top_k_indices[i]\n",
    "    decoder_vec = dictionary.decoder.weight[: ,sae_feat_idx].squeeze()\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(probe_vec, decoder_vec, dim=0)\n",
    "    print(f\"Feature {sae_feat_idx} has cosine similarity {cos_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXModel(\n",
       "  (embed_in): Embedding(50304, 512)\n",
       "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x GPTNeoXLayer(\n",
       "      (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (attention): GPTNeoXAttention(\n",
       "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "        (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp): GPTNeoXMLP(\n",
       "        (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (act): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gpt_neox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed_out.weight.norm(dim=1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Direct Logit Attribution\n",
    "\n",
    "from experiments.autointerp import compute_dla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  233, 18713,   109, 15911, 15533,   349,   477,   234,     1,   126],\n",
       "        [   99,   110,   211,   228,   117,  5980,  7633,  5808,  1760,  3059],\n",
       "        [  113, 24384,   243,    96,   117,   220,   112,   116,    99,   120]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dla(torch.tensor([0, 1, 2]), dictionary.decoder.weight, model.embed_out.weight, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index: 0\n",
      "7265\n",
      "Feature index: 1\n",
      "3597\n",
      "Feature index: 2\n",
      "4648\n",
      "Feature index: 3\n",
      "5923\n",
      "Feature index: 4\n",
      "1104\n",
      "Feature index: 5\n",
      "10316\n",
      "Feature index: 6\n",
      "3767\n",
      "Feature index: 7\n",
      "11238\n",
      "Feature index: 8\n",
      "11797\n",
      "Feature index: 9\n",
      "10602\n"
     ]
    }
   ],
   "source": [
    "# D is top k dla values\n",
    "\n",
    "\n",
    "formatted_tokens = {}\n",
    "top_dla_FD = compute_dla(top_k_indices, dictionary.decoder.weight, model.embed_out.weight, return_topk_tokens=3)\n",
    "\n",
    "for feat_idx in range(10):\n",
    "    sae_feat_idx = top_k_indices[feat_idx].item()\n",
    "    top_dla_D = top_dla_FD[feat_idx].tolist()\n",
    "    print(f\"Feature index: {feat_idx}\")\n",
    "    print(sae_feat_idx)\n",
    "\n",
    "    encoded_tokens_KL = max_tokens_FKL[feat_idx].tolist()\n",
    "    activations_KL = max_activations_FKL[feat_idx]\n",
    "\n",
    "    activations_KL11 = [activations_KL[k, :, None, None] for k in range(activations_KL.shape[0])]\n",
    "    def _list_decode(x):\n",
    "        if isinstance(x, int):\n",
    "            return model.tokenizer.decode(x)\n",
    "        else:\n",
    "            return [_list_decode(y) for y in x]\n",
    "\n",
    "    decoded_tokens_KL = _list_decode(encoded_tokens_KL)\n",
    "    decoded_dla_D = _list_decode(top_dla_D)\n",
    "    # print(decoded_dla_D)\n",
    "    # print(decoded_tokens_KL)\n",
    "\n",
    "    formatted_tokens_KL = highlight_top_activations(decoded_tokens_KL, activations_KL, top_n=5, include_activations=False)\n",
    "    formatted_tokens[feat_idx] = (formatted_tokens_KL, decoded_dla_D)\n",
    "    top_contexts = text_neuron_activations(decoded_tokens_KL, activations_KL11)\n",
    "    # display(top_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the broker and said yes.\n",
      "\n",
      "She << told>> her younger sister she was going to America for work, << but>> << to>> << keep>> it a secret from her parents, who would never grant her permission to work abroad. You Mi told her parents she was going to Seoul to be a golf caddy -- one of the few legal women's jobs that bring hefty tips from rich men.\n",
      "\n",
      "She planned to << tell>> them the truth after she paid off her debts.\n",
      "\n",
      "You Mi was instructed to take passport photos and give them to a man named Kevin in Seoul. The broker drove her to the city, and two days later, You Mi had\n",
      "\n",
      "Top logits: [' her', ' herself', ' she']\n"
     ]
    }
   ],
   "source": [
    "s, dla = formatted_tokens[0]\n",
    "# dla_tokens = \", \".join([f\"<< {t}>>\" for t in dla])\n",
    "for sentence in s:\n",
    "    print(\"\".join(sentence))\n",
    "    print(f'\\nTop logits: {dla}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.explainers.simple.prompt_builder import build_prompt\n",
    "system_prompt, messages = build_prompt(\n",
    "    examples=\"\".join(s[0]),\n",
    "    cot=False,\n",
    "    top_logits=dla,\n",
    "    concept=\"gender\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n\\n[yes/no DECISION]: no\\n'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n\\n[yes/no DECISION]: no\\n'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n\\n[yes/no DECISION]: no\\n'},\n",
       " {'role': 'user',\n",
       "  'content': \"\\nthe broker and said yes.\\n\\nShe << told>> her younger sister she was going to America for work, << but>> << to>> << keep>> it a secret from her parents, who would never grant her permission to work abroad. You Mi told her parents she was going to Seoul to be a golf caddy -- one of the few legal women's jobs that bring hefty tips from rich men.\\n\\nShe planned to << tell>> them the truth after she paid off her debts.\\n\\nYou Mi was instructed to take passport photos and give them to a man named Kevin in Seoul. The broker drove her to the city, and two days later, You Mi had\\n\\nTop_logits: [' her', ' herself', ' she']\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Securely input the API key\n",
    "api_key = input(\"Enter your API key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ['ANTHROPIC_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='After analyzing the examples and the Top_logits, I can provide the following assessment:\\n\\nThe neuron seems to activate on verbs of communication, particularly \"told\" and \"tell,\" when they are used in the context of sharing information, especially secretive or important information. The activation appears to occur just before these verbs.\\n\\nThe Top_logits list provides additional insight:\\nSIMILAR TOKENS: \"her\", \"herself\", \"she\"\\n\\nThese tokens are all feminine pronouns, which suggests that the neuron might be predicting feminine subjects or objects following the communication verbs.\\n\\nConsidering both the activation examples and the Top_logits, it appears that this neuron might be related to the concept of gender, specifically focusing on female subjects or objects in the context of communication or information sharing.\\n\\nThe neuron seems to activate on scenarios where a woman (indicated by \"She\" or \"her\") is telling something, often in a context that implies secrecy, importance, or personal information. The Top_logits reinforce this by suggesting that the neuron predicts feminine pronouns following these communication verbs.\\n\\nGiven this analysis, I believe the neuron\\'s behavior is related to gender, specifically focusing on female subjects in communication contexts.\\n\\n[yes/no DECISION]: yes', type='text')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    system=system_prompt,\n",
    "    messages=messages,\n",
    ")\n",
    "print(message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'text', 'text': \"You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and decide whether its behavior is related to the concept of gender.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nYou will also be shown a list called Top_logits. The logits promoted by the neuron shed light on how the neuron's activation influences the model's predictions or outputs. Look at this list of Top_logits and refine your hypotheses from part 1. It is possible that this list is more informative than the examples from part 1.\\n\\nPay close attention to the words in this list and write down what they have in common. Then look at what they have in common, as well as patterns in the tokens you found in Part 1, to produce a single explanation for what features of text cause the neuron to activate. Propose your explanation in the following format:\\n[yes/no DECISION]: <your decision>\\n\\nGuidelines:\\n\\nYou will be given a list of text examples on which the neuron activates. The specific tokens which cause the neuron to activate will appear between delimiters like <<this>>. The activation value of the token is given after each token in parentheses like <<this>>(3).\\n\\n- Try to judge whether the neurons behavior is related to gender. Simply make a choice based on the text features that activate the neuron, and what its role might be based on the tokens it predicts.\\n- The last line of your response must be your binary decision, yes or no.\"}]\n",
      "[{'role': 'user', 'content': '\\nExample 1:  and he was <<over the moon>> to find\\nExample 2:  we\\'ll be laughing <<till the cows come home>>! Pro\\nExample 3:  thought Scotland was boring, but really there\\'s more <<than meets the eye>>! I\\'d\\n\\nTop_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\\n'}, {'role': 'assistant', 'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\\n- The top logits list contains words that are strongly associated with positive emotions.\\n\\n[yes/no DECISION]: no\\n'}, {'role': 'user', 'content': '\\nExample 1:  a river is wide but the ocean is wid<<er>>. The ocean\\nExample 2:  every year you get tall<<er>>,\" she\\nExample 3:  the hole was small<<er>> but deep<<er>> than the\\n\\nTop_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\\n'}, {'role': 'assistant', 'content': '\\n(Part 2)\\nSIMILAR TOKENS: None\\n- The top logits list contains unrelated nouns and adverbs.\\n\\n[yes/no DECISION]: no\\n'}, {'role': 'user', 'content': '\\nExample 1:  something happening inside my <<house>>\", he\\nExample 2:  presumably was always contained in <<a box>>\", according\\nExample 3:  people were coming into the <<smoking area>>\".\\n\\nHowever he\\nExample 4:  Patrick: \"why are you getting in the << way?>>\" Later,\\n\\nTop_logits: [\"room\", \"end\", \"container, \"space\", \"plane\"]\\n'}, {'role': 'assistant', 'content': '\\n(Part 2)\\nSIMILAR TOKENS: \"room\", \"container\", \"space\".\\n- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\\n\\n[yes/no DECISION]: no\\n'}, {'role': 'user', 'content': \"\\nthe broker and said yes.\\n\\nShe << told>> her younger sister she was going to America for work, << but>> << to>> << keep>> it a secret from her parents, who would never grant her permission to work abroad. You Mi told her parents she was going to Seoul to be a golf caddy -- one of the few legal women's jobs that bring hefty tips from rich men.\\n\\nShe planned to << tell>> them the truth after she paid off her debts.\\n\\nYou Mi was instructed to take passport photos and give them to a man named Kevin in Seoul. The broker drove her to the city, and two days later, You Mi had\\n\\nTop_logits: [' her', ' herself', ' she']\"}]\n",
      "[TextBlock(text='After analyzing the examples and the Top_logits, I can provide the following assessment:\\n\\nThe neuron seems to activate on verbs of communication, particularly \"told\" and \"tell\", especially when they are followed by pronouns or references to people. The context often involves sharing information, sometimes secretive or important.\\n\\nLooking at the Top_logits, we see:\\nSIMILAR TOKENS: \"her\", \"herself\", \"she\"\\n\\nThese tokens are all feminine pronouns, which is interesting given the context of the examples. The neuron seems to be predicting feminine pronouns following verbs of communication.\\n\\nCombining the information from the examples and the Top_logits, it appears that this neuron might be related to the concept of gender, specifically focusing on female subjects in the context of communication or information sharing.\\n\\nThe neuron activates on verbs like \"told\" and \"tell\", and then predicts feminine pronouns to follow. This suggests a pattern of women being the subjects or objects of communication acts in the text.\\n\\nGiven this analysis, I believe the neuron\\'s behavior is related to gender.\\n\\n[yes/no DECISION]: yes', type='text')]\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)\n",
    "\n",
    "print(messages)\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "llm_out = message.content[0].text.lower()[-10:]\n",
    "\n",
    "if 'yes' in llm_out and 'no' in llm_out:\n",
    "    decision = -1\n",
    "elif 'yes' in llm_out:\n",
    "    decision = 1\n",
    "elif 'no' in llm_out:\n",
    "    decision = 0\n",
    "else:\n",
    "    decision = -1\n",
    "\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(part 1)\\nactivating tokens: \"told\", \"but\", \"to\", \"keep\", \"tell\".\\nprevious tokens: no interesting patterns.\\n\\nstep 1:\\n- the activating tokens are mostly verbs related to communication (\"told\", \"tell\") and function words (\"but\", \"to\", \"keep\").\\n- the previous tokens don\\'t show any particular pattern.\\n\\nstep 2:\\n- the text examples involve communication, particularly secretive or deceptive communication.\\n- there\\'s a narrative about a woman (you mi) planning to work abroad and keeping it secret from her parents.\\n\\n(part 2)\\nsimilar tokens: \"her\", \"herself\", \"she\".\\n- the top logits list contains exclusively feminine pronouns.\\n\\n[explanation]: this neuron appears to activate on tokens related to communication, particularly in the context of secrets or deception. it also seems to have a strong association with feminine pronouns in its predictions. this suggests the neuron may be capturing some aspect of narrative or dialogue involving women, especially in situations where information is being withheld or carefully managed.\\n\\ngiven the context of the passage (about a woman keeping secrets and making plans) and the neuron\\'s tendency to predict feminine pronouns, it seems that this neuron\\'s behavior is indeed related to the concept of gender, specifically focusing on female subjects in narrative contexts.\\n\\nyes, the neuron\\'s behavior is related to the concept '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-4, -2, 0, 1, 2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     node_effects \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(node_effects\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 13\u001b[0m effects \u001b[38;5;241m=\u001b[39m \u001b[43mnode_effects\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mae_path\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEffects for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mae_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m theshold \u001b[38;5;129;01min\u001b[39;00m thresholds:\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "thresholds = [0.1, 0.05, 0.025, 0.01, 0.001]\n",
    "top_ns = [1, 10, 100, 500]\n",
    "\n",
    "\n",
    "for i, ae_path in enumerate(ae_paths):\n",
    "    node_effects_filename = f\"{ae_path}/node_effects.pkl\"\n",
    "\n",
    "    with open(node_effects_filename, \"rb\") as f:\n",
    "        node_effects = pickle.load(f)\n",
    "\n",
    "    print(node_effects.keys())\n",
    "\n",
    "    effects = node_effects[-2][ae_path]\n",
    "\n",
    "    print(f\"\\nEffects for {ae_path}\")\n",
    "    for theshold in thresholds:\n",
    "        above_threshold = effects[effects > theshold]\n",
    "        count_above_threshold = above_threshold.shape[0]\n",
    "        avg_above_threshold = above_threshold.mean().item()\n",
    "        print(\n",
    "            f\"Threshold {theshold}: {count_above_threshold} nodes above threshold, {avg_above_threshold:.3f} average\"\n",
    "        )\n",
    "\n",
    "    for top_n in top_ns:\n",
    "        top_k = torch.topk(effects, top_n)\n",
    "        avg_top_k = top_k.values.mean().item()\n",
    "        print(f\"Top {top_n}: {avg_top_k:.3f} average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ae_path in enumerate(ae_paths):\n",
    "    node_effects_filename = f\"{ae_path}/node_effects.pkl\"\n",
    "\n",
    "    with open(node_effects_filename, \"rb\") as f:\n",
    "        node_effects = pickle.load(f)\n",
    "\n",
    "    print(node_effects.keys())\n",
    "    print(node_effects[-2].keys())\n",
    "\n",
    "    effects = node_effects[-4][ae_path]\n",
    "    print(effects.shape)\n",
    "\n",
    "    # Create histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(effects, bins=100)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title(f'Histogram for {ae_path.split(\"/\")[-3]}')\n",
    "    plt.xlabel(\"Effect Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def examine_dimension(\n",
    "#     model,\n",
    "#     submodule,\n",
    "#     buffer,\n",
    "#     feat_idx: int,\n",
    "#     n_inputs: int,\n",
    "#     context_length: int,\n",
    "#     batch_size: int,\n",
    "#     dictionary=None,\n",
    "#     max_length: int = 128,\n",
    "#     k: int = 30,\n",
    "# ):\n",
    "\n",
    "\n",
    "#     def _list_decode(x):\n",
    "#         if isinstance(x, int):\n",
    "#             return model.tokenizer.decode(x)\n",
    "#         else:\n",
    "#             return [_list_decode(y) for y in x]\n",
    "\n",
    "#     # if dim_indices is None:\n",
    "#         # dim_indices = random.randint(0, activations.shape[-1] - 1)\n",
    "\n",
    "#     assert n_inputs % batch_size == 0\n",
    "#     n_iters = n_inputs // batch_size\n",
    "\n",
    "#     device = model.device\n",
    "\n",
    "#     activations = torch.zeros((n_inputs, context_length), device=device)\n",
    "#     tokens = torch.zeros((n_inputs, context_length), dtype=torch.long, device=device)\n",
    "\n",
    "#     for i in tqdm(range(n_iters), desc=\"Collecting activations\"):\n",
    "#         inputs_BL = buffer.tokenized_batch(batch_size=batch_size)\n",
    "\n",
    "#         with torch.no_grad(), model.trace(inputs_BL, **tracer_kwargs):\n",
    "#             tokens_BL = model.input[1][\n",
    "#                 \"input_ids\"\n",
    "#             ].save()  # if you're getting errors, check here; might only work for pythia models\n",
    "#             activations_BLD = submodule.output\n",
    "#             if type(activations_BLD.shape) == tuple:\n",
    "#                 activations_BLD = activations_BLD[0]\n",
    "#             if dictionary is not None:\n",
    "#                 activations_BLF = dictionary.encode(activations_BLD)\n",
    "#             activations_BL = activations_BLF[:, :, feat_idx].save()\n",
    "\n",
    "#         activations[i * batch_size : (i + 1) * batch_size] = activations_BL.value\n",
    "#         tokens[i * batch_size : (i + 1) * batch_size] = tokens_BL.value\n",
    "\n",
    "#     token_mean_acts = {}\n",
    "#     for ctx in tokens:\n",
    "#         for tok in ctx:\n",
    "#             if tok.item() in token_mean_acts:\n",
    "#                 continue\n",
    "#             idxs = (tokens == tok).nonzero(as_tuple=True)\n",
    "#             token_mean_acts[tok.item()] = activations[idxs].mean().item()\n",
    "#     top_tokens = sorted(token_mean_acts.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "#     top_tokens = [(model.tokenizer.decode(tok), act) for tok, act in top_tokens]\n",
    "\n",
    "#     flattened_acts = einops.rearrange(activations, \"b n -> (b n)\")\n",
    "#     topk_indices = torch.argsort(flattened_acts, dim=0, descending=True)[:k]\n",
    "#     batch_indices = topk_indices // activations.shape[1]\n",
    "#     token_indices = topk_indices % activations.shape[1]\n",
    "#     tokens = [\n",
    "#         tokens[batch_idx, : token_idx + 1].tolist()\n",
    "#         for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "#     ]\n",
    "#     activations = [\n",
    "#         activations[batch_idx, : token_id + 1, None, None]\n",
    "#         for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "#     ]\n",
    "#     decoded_tokens = _list_decode(tokens)\n",
    "#     top_contexts = text_neuron_activations(decoded_tokens, activations)\n",
    "\n",
    "#     top_affected = interp.feature_effect(\n",
    "#         model, submodule, dictionary, feat_idx, tokens, max_length=max_length, k=k\n",
    "#     )\n",
    "#     top_affected = [(model.tokenizer.decode(tok), prob.item()) for tok, prob in zip(*top_affected)]\n",
    "\n",
    "#     return namedtuple(\"featureProfile\", [\"top_contexts\", \"top_tokens\", \"top_affected\"])(\n",
    "#         top_contexts, top_tokens, top_affected\n",
    "#     )\n",
    "\n",
    "# # DEVICE = \"cuda\"\n",
    "# # model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# # model = LanguageModel(model_name, device_map=DEVICE, dispatch=True)\n",
    "\n",
    "# # ae_path = ae_paths[2]\n",
    "# # submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)\n",
    "\n",
    "# # context_length = config['buffer']['ctx_len']\n",
    "\n",
    "# # data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "# # buffer = ActivationBuffer(\n",
    "# #     data,\n",
    "# #     model,\n",
    "# #     submodule,\n",
    "# #     d_submodule=512,\n",
    "# #     ctx_len=context_length,\n",
    "# #     refresh_batch_size=128, # decrease to fit on smaller GPUs\n",
    "# #     n_ctxs=512, # decrease to fit on smaller GPUs\n",
    "# #     device=DEVICE\n",
    "# # )\n",
    "\n",
    "# feat_idx = 0\n",
    "# sae_feat_idx = top_k_indices[feat_idx].item()\n",
    "# print(sae_feat_idx)\n",
    "\n",
    "# n_inputs = 1024\n",
    "# batch_size = 256\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.set_grad_enabled(False)\n",
    "\n",
    "# out = examine_dimension(\n",
    "#     model,\n",
    "#     submodule,\n",
    "#     buffer,\n",
    "#     sae_feat_idx,\n",
    "#     n_inputs,\n",
    "#     context_length,\n",
    "#     batch_size,\n",
    "#     dictionary,\n",
    "#     max_length=context_length,\n",
    "#     k=30,\n",
    "# )\n",
    "\n",
    "# print(f'\\n\\ntop activating tokens for feature {sae_feat_idx}')\n",
    "# for token in out.top_tokens:\n",
    "#     print(token)\n",
    "# print(f'\\n\\ntop affected tokens for feature {sae_feat_idx}')\n",
    "# for token in out.top_affected:\n",
    "#     print(token)\n",
    "\n",
    "# out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_activating_prompts_old(\n",
    "    model,\n",
    "    submodule,\n",
    "    inputs_bL: list[str],\n",
    "    dim_indices: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    dictionary=None,\n",
    "    n_inputs: int = 512,\n",
    "    k: int = 30,\n",
    "):\n",
    "\n",
    "    assert n_inputs % batch_size == 0\n",
    "    n_iters = n_inputs // batch_size\n",
    "\n",
    "    dim_count = dim_indices.shape[0]\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    max_activating_indices_FK = torch.zeros((dim_count, k), device=device, dtype=torch.int)\n",
    "    max_activations_FK = torch.zeros((dim_count, k), device=device, dtype=torch.float32)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "\n",
    "        batch_offset = i * batch_size\n",
    "\n",
    "        inputs_BL = inputs_bL[batch_offset : batch_offset + batch_size]\n",
    "\n",
    "        with torch.no_grad(), model.trace(inputs_BL, **tracer_kwargs):\n",
    "            activations_BLD = submodule.output\n",
    "            if type(activations_BLD.shape) == tuple:\n",
    "                activations_BLD = activations_BLD[0]\n",
    "            activations_BLF = dictionary.encode(activations_BLD)\n",
    "            activations_BLF = activations_BLF[:, :, dim_indices].save()\n",
    "\n",
    "        # Use einops to find the max activation per input\n",
    "        activations_FB = einops.reduce(activations_BLF.value, 'B L F _> F B', 'max')\n",
    "        \n",
    "        # Keep track of input indices\n",
    "        indices_B = torch.arange(batch_offset, batch_offset + batch_size, device=device)\n",
    "        indices_FB = einops.repeat(indices_B, 'B -> F B', F=dim_count)\n",
    "\n",
    "        # Concatenate current batch activations and indices with the previous ones\n",
    "        combined_activations_FK = torch.cat([max_activations_FK, activations_FB], dim=1)\n",
    "        combined_indices_FK = torch.cat([max_activating_indices_FK, indices_FB], dim=1)\n",
    "\n",
    "        # Sort and keep top k activations for each dimension\n",
    "        topk_activations_FK, topk_indices = torch.topk(combined_activations_FK, k, dim=1)\n",
    "        max_activations_FK = topk_activations_FK\n",
    "        max_activating_indices_FK = torch.gather(combined_indices_FK, 1, topk_indices)\n",
    "\n",
    "    return max_activating_indices_FK, max_activations_FK\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
