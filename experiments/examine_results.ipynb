{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import datasets\n",
    "\n",
    "import einops\n",
    "import dictionary_learning.interp as interp\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from collections import namedtuple\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import experiments.utils as utils\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "\n",
    "DEBUGGING = True\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model = LanguageModel(model_name, device_map=DEVICE, dispatch=True)\n",
    "\n",
    "\n",
    "context_length = 128\n",
    "\n",
    "dataset = datasets.load_dataset(\"georgeyw/dsir-pile-100k\", streaming=False)\n",
    "\n",
    "data = model.tokenizer(dataset[\"train\"][\"contents\"][:10000], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=context_length).to(DEVICE).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))\n",
    "batch_size = 250\n",
    "\n",
    "batched_data = utils.batch_inputs(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import dictionary_learning.interp as interp\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from collections import namedtuple\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import experiments.utils as utils\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "\n",
    "\n",
    "DEBUGGING = True\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "\n",
    "def get_max_activating_prompts(\n",
    "    model,\n",
    "    submodule,\n",
    "    tokenized_inputs_bL: list[list[dict]],\n",
    "    dim_indices: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    dictionary=None,\n",
    "    n_inputs: int = 512,\n",
    "    k: int = 30,\n",
    "):\n",
    "\n",
    "    assert n_inputs % batch_size == 0\n",
    "\n",
    "    feature_count = dim_indices.shape[0]\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    max_activating_indices_FK = torch.zeros((feature_count, k), device=device, dtype=torch.int)\n",
    "    max_activations_FK = torch.zeros((feature_count, k), device=device, dtype=torch.float32)\n",
    "    max_tokens_FKL = torch.zeros((feature_count, k, context_length), device=device, dtype=torch.int)\n",
    "    max_activations_FKL = torch.zeros((feature_count, k, context_length), device=device, dtype=torch.float32)\n",
    "\n",
    "    for i, inputs in tqdm(tokenized_inputs_bL):\n",
    "\n",
    "        batch_offset = i * batch_size\n",
    "        inputs_BL = inputs['input_ids']\n",
    "\n",
    "        with torch.no_grad(), model.trace(inputs, **tracer_kwargs):\n",
    "            activations_BLD = submodule.output\n",
    "            if type(activations_BLD.shape) == tuple:\n",
    "                activations_BLD = activations_BLD[0]\n",
    "            activations_BLF = dictionary.encode(activations_BLD)\n",
    "            activations_BLF = activations_BLF[:, :, dim_indices].save()\n",
    "\n",
    "        activations_FBL = einops.rearrange(activations_BLF.value, 'B L F -> F B L')\n",
    "        # Use einops to find the max activation per input\n",
    "        activations_FB = einops.reduce(activations_FBL, 'F B L -> F B', 'max')\n",
    "        tokens_FBL = einops.repeat(inputs_BL, 'B L -> F B L', F=feature_count)\n",
    "        \n",
    "        # Keep track of input indices\n",
    "        indices_B = torch.arange(batch_offset, batch_offset + batch_size, device=device)\n",
    "        indices_FB = einops.repeat(indices_B, 'B -> F B', F=feature_count)\n",
    "\n",
    "        # Concatenate current batch activations and indices with the previous ones\n",
    "        combined_activations_FB = torch.cat([max_activations_FK, activations_FB], dim=1)\n",
    "        combined_indices_FB = torch.cat([max_activating_indices_FK, indices_FB], dim=1)\n",
    "        combined_activations_FBL = torch.cat([max_activations_FKL, activations_FBL], dim=1)\n",
    "        combined_tokens_FBL = torch.cat([max_tokens_FKL, tokens_FBL], dim=1)\n",
    "\n",
    "        # Sort and keep top k activations for each dimension\n",
    "        topk_activations_FK, topk_indices_FK = torch.topk(combined_activations_FB, k, dim=1)\n",
    "        max_activations_FK = topk_activations_FK\n",
    "\n",
    "        feature_indices_F1 = torch.arange(feature_count, device=device)[:, None]\n",
    "        max_activating_indices_FK = combined_indices_FB[feature_indices_F1, topk_indices_FK]\n",
    "        max_activations_FKL = combined_activations_FBL[feature_indices_F1, topk_indices_FK]\n",
    "        max_tokens_FKL = combined_tokens_FBL[feature_indices_F1, topk_indices_FK]\n",
    "            \n",
    "\n",
    "    return max_tokens_FKL, max_activations_FKL\n",
    "\n",
    "dictionaries_path = \"../dictionary_learning/dictionaries\"\n",
    "\n",
    "# Current recommended way to generate graphs. You can copy paste ae_sweep_paths directly from bib_intervention.py\n",
    "ae_sweep_paths = {\n",
    "    # \"pythia70m_sweep_standard_ctx128_0712\": {\"resid_post_layer_3\": {\"trainer_ids\": [11]}},\n",
    "    # \"pythia70m_sweep_gated_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [9]}},\n",
    "    \"pythia70m_sweep_topk_ctx128_0730\": {\"resid_post_layer_3\": {\"trainer_ids\": [18]}},\n",
    "}\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "filter_class_ids = []\n",
    "# filter_class_ids = [-4, -2]\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(dictionaries_path, sweep_name, submodule_trainers)\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "# TODO\n",
    "# Add bias_in_bios dataset option\n",
    "# Cosine sim with probes\n",
    "# Vector per class probe\n",
    "\n",
    "ae_path = ae_paths[0]\n",
    "submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)\n",
    "\n",
    "node_effects_filename = f\"{ae_path}/node_effects.pkl\"\n",
    "\n",
    "with open(node_effects_filename, \"rb\") as f:\n",
    "    node_effects = pickle.load(f)\n",
    "\n",
    "effects = node_effects[-2][ae_path]\n",
    "\n",
    "print(effects.shape)\n",
    "\n",
    "k = 500\n",
    "top_k_values, top_k_indices = torch.topk(effects, k)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(top_k_values)\n",
    "print(top_k_indices)\n",
    "\n",
    "all_indices = torch.arange(0, effects.shape[0])\n",
    "all_indices = top_k_indices\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "max_tokens_FKL, max_activations_FKL = get_max_activating_prompts(model, submodule, batched_data, all_indices, batch_size, dictionary, 10000, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_idx in range(10):\n",
    "    sae_feat_idx = top_k_indices[feat_idx].item()\n",
    "    print(f\"Feature index: {feat_idx}\")\n",
    "    print(sae_feat_idx)\n",
    "\n",
    "    encoded_tokens_KL = max_tokens_FKL[feat_idx].tolist()\n",
    "    activations_KL = max_activations_FKL[feat_idx]\n",
    "\n",
    "    activations_KL11 = [activations_KL[k, :, None, None] for k in range(activations_KL.shape[0])]\n",
    "    def _list_decode(x):\n",
    "        if isinstance(x, int):\n",
    "            return model.tokenizer.decode(x)\n",
    "        else:\n",
    "            return [_list_decode(y) for y in x]\n",
    "\n",
    "    decoded_tokens_KL = _list_decode(encoded_tokens_KL)\n",
    "    top_contexts = text_neuron_activations(decoded_tokens_KL, activations_KL11)\n",
    "    display(top_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholds = [0.1, 0.05, 0.025, 0.01, 0.001]\n",
    "top_ns = [1, 10, 100, 500]\n",
    "\n",
    "\n",
    "for i, ae_path in enumerate(ae_paths):\n",
    "    node_effects_filename = f\"{ae_path}/node_effects.pkl\"\n",
    "\n",
    "    with open(node_effects_filename, \"rb\") as f:\n",
    "        node_effects = pickle.load(f)\n",
    "\n",
    "    print(node_effects.keys())\n",
    "\n",
    "    effects = node_effects[-4][ae_path]\n",
    "\n",
    "    print(f\"\\nEffects for {ae_path}\")\n",
    "    for theshold in thresholds:\n",
    "        above_threshold = effects[effects > theshold]\n",
    "        count_above_threshold = above_threshold.shape[0]\n",
    "        avg_above_threshold = above_threshold.mean().item()\n",
    "        print(\n",
    "            f\"Threshold {theshold}: {count_above_threshold} nodes above threshold, {avg_above_threshold:.3f} average\"\n",
    "        )\n",
    "\n",
    "    for top_n in top_ns:\n",
    "        top_k = torch.topk(effects, top_n)\n",
    "        avg_top_k = top_k.values.mean().item()\n",
    "        print(f\"Top {top_n}: {avg_top_k:.3f} average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ae_path in enumerate(ae_paths):\n",
    "    node_effects_filename = f\"{ae_path}/node_effects.pkl\"\n",
    "\n",
    "    with open(node_effects_filename, \"rb\") as f:\n",
    "        node_effects = pickle.load(f)\n",
    "\n",
    "    print(node_effects.keys())\n",
    "    print(node_effects[-2].keys())\n",
    "\n",
    "    effects = node_effects[-4][ae_path]\n",
    "    print(effects.shape)\n",
    "\n",
    "    # Create histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(effects, bins=100)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title(f'Histogram for {ae_path.split(\"/\")[-3]}')\n",
    "    plt.xlabel(\"Effect Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def examine_dimension(\n",
    "#     model,\n",
    "#     submodule,\n",
    "#     buffer,\n",
    "#     feat_idx: int,\n",
    "#     n_inputs: int,\n",
    "#     context_length: int,\n",
    "#     batch_size: int,\n",
    "#     dictionary=None,\n",
    "#     max_length: int = 128,\n",
    "#     k: int = 30,\n",
    "# ):\n",
    "\n",
    "\n",
    "#     def _list_decode(x):\n",
    "#         if isinstance(x, int):\n",
    "#             return model.tokenizer.decode(x)\n",
    "#         else:\n",
    "#             return [_list_decode(y) for y in x]\n",
    "\n",
    "#     # if dim_indices is None:\n",
    "#         # dim_indices = random.randint(0, activations.shape[-1] - 1)\n",
    "\n",
    "#     assert n_inputs % batch_size == 0\n",
    "#     n_iters = n_inputs // batch_size\n",
    "\n",
    "#     device = model.device\n",
    "\n",
    "#     activations = torch.zeros((n_inputs, context_length), device=device)\n",
    "#     tokens = torch.zeros((n_inputs, context_length), dtype=torch.long, device=device)\n",
    "\n",
    "#     for i in tqdm(range(n_iters), desc=\"Collecting activations\"):\n",
    "#         inputs_BL = buffer.tokenized_batch(batch_size=batch_size)\n",
    "\n",
    "#         with torch.no_grad(), model.trace(inputs_BL, **tracer_kwargs):\n",
    "#             tokens_BL = model.input[1][\n",
    "#                 \"input_ids\"\n",
    "#             ].save()  # if you're getting errors, check here; might only work for pythia models\n",
    "#             activations_BLD = submodule.output\n",
    "#             if type(activations_BLD.shape) == tuple:\n",
    "#                 activations_BLD = activations_BLD[0]\n",
    "#             if dictionary is not None:\n",
    "#                 activations_BLF = dictionary.encode(activations_BLD)\n",
    "#             activations_BL = activations_BLF[:, :, feat_idx].save()\n",
    "\n",
    "#         activations[i * batch_size : (i + 1) * batch_size] = activations_BL.value\n",
    "#         tokens[i * batch_size : (i + 1) * batch_size] = tokens_BL.value\n",
    "\n",
    "#     token_mean_acts = {}\n",
    "#     for ctx in tokens:\n",
    "#         for tok in ctx:\n",
    "#             if tok.item() in token_mean_acts:\n",
    "#                 continue\n",
    "#             idxs = (tokens == tok).nonzero(as_tuple=True)\n",
    "#             token_mean_acts[tok.item()] = activations[idxs].mean().item()\n",
    "#     top_tokens = sorted(token_mean_acts.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "#     top_tokens = [(model.tokenizer.decode(tok), act) for tok, act in top_tokens]\n",
    "\n",
    "#     flattened_acts = einops.rearrange(activations, \"b n -> (b n)\")\n",
    "#     topk_indices = torch.argsort(flattened_acts, dim=0, descending=True)[:k]\n",
    "#     batch_indices = topk_indices // activations.shape[1]\n",
    "#     token_indices = topk_indices % activations.shape[1]\n",
    "#     tokens = [\n",
    "#         tokens[batch_idx, : token_idx + 1].tolist()\n",
    "#         for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "#     ]\n",
    "#     activations = [\n",
    "#         activations[batch_idx, : token_id + 1, None, None]\n",
    "#         for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "#     ]\n",
    "#     decoded_tokens = _list_decode(tokens)\n",
    "#     top_contexts = text_neuron_activations(decoded_tokens, activations)\n",
    "\n",
    "#     top_affected = interp.feature_effect(\n",
    "#         model, submodule, dictionary, feat_idx, tokens, max_length=max_length, k=k\n",
    "#     )\n",
    "#     top_affected = [(model.tokenizer.decode(tok), prob.item()) for tok, prob in zip(*top_affected)]\n",
    "\n",
    "#     return namedtuple(\"featureProfile\", [\"top_contexts\", \"top_tokens\", \"top_affected\"])(\n",
    "#         top_contexts, top_tokens, top_affected\n",
    "#     )\n",
    "\n",
    "# # DEVICE = \"cuda\"\n",
    "# # model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# # model = LanguageModel(model_name, device_map=DEVICE, dispatch=True)\n",
    "\n",
    "# # ae_path = ae_paths[2]\n",
    "# # submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)\n",
    "\n",
    "# # context_length = config['buffer']['ctx_len']\n",
    "\n",
    "# # data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "# # buffer = ActivationBuffer(\n",
    "# #     data,\n",
    "# #     model,\n",
    "# #     submodule,\n",
    "# #     d_submodule=512,\n",
    "# #     ctx_len=context_length,\n",
    "# #     refresh_batch_size=128, # decrease to fit on smaller GPUs\n",
    "# #     n_ctxs=512, # decrease to fit on smaller GPUs\n",
    "# #     device=DEVICE\n",
    "# # )\n",
    "\n",
    "# feat_idx = 0\n",
    "# sae_feat_idx = top_k_indices[feat_idx].item()\n",
    "# print(sae_feat_idx)\n",
    "\n",
    "# n_inputs = 1024\n",
    "# batch_size = 256\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.set_grad_enabled(False)\n",
    "\n",
    "# out = examine_dimension(\n",
    "#     model,\n",
    "#     submodule,\n",
    "#     buffer,\n",
    "#     sae_feat_idx,\n",
    "#     n_inputs,\n",
    "#     context_length,\n",
    "#     batch_size,\n",
    "#     dictionary,\n",
    "#     max_length=context_length,\n",
    "#     k=30,\n",
    "# )\n",
    "\n",
    "# print(f'\\n\\ntop activating tokens for feature {sae_feat_idx}')\n",
    "# for token in out.top_tokens:\n",
    "#     print(token)\n",
    "# print(f'\\n\\ntop affected tokens for feature {sae_feat_idx}')\n",
    "# for token in out.top_affected:\n",
    "#     print(token)\n",
    "\n",
    "# out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_activating_prompts_old(\n",
    "    model,\n",
    "    submodule,\n",
    "    inputs_bL: list[str],\n",
    "    dim_indices: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    dictionary=None,\n",
    "    n_inputs: int = 512,\n",
    "    k: int = 30,\n",
    "):\n",
    "\n",
    "    assert n_inputs % batch_size == 0\n",
    "    n_iters = n_inputs // batch_size\n",
    "\n",
    "    dim_count = dim_indices.shape[0]\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    max_activating_indices_FK = torch.zeros((dim_count, k), device=device, dtype=torch.int)\n",
    "    max_activations_FK = torch.zeros((dim_count, k), device=device, dtype=torch.float32)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "\n",
    "        batch_offset = i * batch_size\n",
    "\n",
    "        inputs_BL = inputs_bL[batch_offset : batch_offset + batch_size]\n",
    "\n",
    "        with torch.no_grad(), model.trace(inputs_BL, **tracer_kwargs):\n",
    "            activations_BLD = submodule.output\n",
    "            if type(activations_BLD.shape) == tuple:\n",
    "                activations_BLD = activations_BLD[0]\n",
    "            activations_BLF = dictionary.encode(activations_BLD)\n",
    "            activations_BLF = activations_BLF[:, :, dim_indices].save()\n",
    "\n",
    "        # Use einops to find the max activation per input\n",
    "        activations_FB = einops.reduce(activations_BLF.value, 'B L F _> F B', 'max')\n",
    "        \n",
    "        # Keep track of input indices\n",
    "        indices_B = torch.arange(batch_offset, batch_offset + batch_size, device=device)\n",
    "        indices_FB = einops.repeat(indices_B, 'B -> F B', F=dim_count)\n",
    "\n",
    "        # Concatenate current batch activations and indices with the previous ones\n",
    "        combined_activations_FK = torch.cat([max_activations_FK, activations_FB], dim=1)\n",
    "        combined_indices_FK = torch.cat([max_activating_indices_FK, indices_FB], dim=1)\n",
    "\n",
    "        # Sort and keep top k activations for each dimension\n",
    "        topk_activations_FK, topk_indices = torch.topk(combined_activations_FK, k, dim=1)\n",
    "        max_activations_FK = topk_activations_FK\n",
    "        max_activating_indices_FK = torch.gather(combined_indices_FK, 1, topk_indices)\n",
    "\n",
    "    return max_activating_indices_FK, max_activations_FK\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
