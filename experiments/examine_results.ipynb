{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import experiments.utils as utils\n",
    "\n",
    "\n",
    "dictionaries_path = \"../dictionary_learning/dictionaries\"\n",
    "\n",
    "# Another way to generate graphs, where you manually populate sweep_name and submodule_trainers\n",
    "sweep_name = \"pythia70m_test_sae\"\n",
    "submodule_trainers = {\"resid_post_layer_3\": {\"trainer_ids\": [0]}}\n",
    "\n",
    "# Current recommended way to generate graphs. You can copy paste ae_sweep_paths directly from bib_intervention.py\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\"resid_post_layer_3\": {\"trainer_ids\": [1, 7, 11, 18]}}\n",
    "}\n",
    "sweep_name = list(ae_sweep_paths.keys())[0]\n",
    "submodule_trainers = ae_sweep_paths[sweep_name]\n",
    "\n",
    "filter_class_ids = []\n",
    "# filter_class_ids = [-4, -2]\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(dictionaries_path, sweep_name, submodule_trainers)\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "include_diff = True\n",
    "\n",
    "print(ae_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.1, 0.05, 0.025, 0.01, 0.001]\n",
    "top_ns = [1, 10, 100, 500]\n",
    "\n",
    "\n",
    "for i, ae_path in enumerate(ae_paths):\n",
    "    node_effects_filename = f\"{ae_path}/node_effects.pkl\"\n",
    "\n",
    "    with open(node_effects_filename, \"rb\") as f:\n",
    "        node_effects = pickle.load(f)\n",
    "\n",
    "    effects = node_effects[-4][ae_path]\n",
    "\n",
    "    print(f\"\\nEffects for {ae_path}\")\n",
    "    for theshold in thresholds:\n",
    "        above_threshold = effects[effects > theshold]\n",
    "        count_above_threshold = above_threshold.shape[0]\n",
    "        avg_above_threshold = above_threshold.mean().item()\n",
    "        print(\n",
    "            f\"Threshold {theshold}: {count_above_threshold} nodes above threshold, {avg_above_threshold:.3f} average\"\n",
    "        )\n",
    "\n",
    "    for top_n in top_ns:\n",
    "        top_k = torch.topk(effects, top_n)\n",
    "        avg_top_k = top_k.values.mean().item()\n",
    "        print(f\"Top {top_n}: {avg_top_k:.3f} average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ae_path in enumerate(ae_paths):\n",
    "    node_effects_filename = f\"{ae_path}/node_effects.pkl\"\n",
    "\n",
    "    with open(node_effects_filename, \"rb\") as f:\n",
    "        node_effects = pickle.load(f)\n",
    "\n",
    "    print(node_effects.keys())\n",
    "    print(node_effects[-2].keys())\n",
    "\n",
    "    effects = node_effects[-4][ae_path]\n",
    "    print(effects.shape)\n",
    "\n",
    "    # Create histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(effects, bins=100)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title(f'Histogram for {ae_path.split(\"/\")[-3]}')\n",
    "    plt.xlabel(\"Effect Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ae_path in ae_paths:\n",
    "    print(ae_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import dictionary_learning.interp as interp\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from collections import namedtuple\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "import experiments.utils as utils\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "\n",
    "\n",
    "DEBUGGING = True\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "def examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    feat_idx: int,\n",
    "    n_inputs: int,\n",
    "    context_length: int,\n",
    "    batch_size: int,\n",
    "    dictionary=None,\n",
    "    max_length: int = 128,\n",
    "    k: int = 30,\n",
    "):\n",
    "\n",
    "\n",
    "    def _list_decode(x):\n",
    "        if isinstance(x, int):\n",
    "            return model.tokenizer.decode(x)\n",
    "        else:\n",
    "            return [_list_decode(y) for y in x]\n",
    "\n",
    "    # if dim_indices is None:\n",
    "        # dim_indices = random.randint(0, activations.shape[-1] - 1)\n",
    "\n",
    "    assert n_inputs % batch_size == 0\n",
    "    n_iters = n_inputs // batch_size\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    activations = torch.zeros((n_inputs, context_length), device=device)\n",
    "    tokens = torch.zeros((n_inputs, context_length), dtype=torch.long, device=device)\n",
    "\n",
    "    for i in tqdm(range(n_iters), desc=\"Collecting activations\"):\n",
    "        inputs_BL = buffer.tokenized_batch(batch_size=batch_size)\n",
    "\n",
    "        with torch.no_grad(), model.trace(inputs_BL, **tracer_kwargs):\n",
    "            tokens_BL = model.input[1][\n",
    "                \"input_ids\"\n",
    "            ].save()  # if you're getting errors, check here; might only work for pythia models\n",
    "            activations_BLD = submodule.output\n",
    "            if type(activations_BLD.shape) == tuple:\n",
    "                activations_BLD = activations_BLD[0]\n",
    "            if dictionary is not None:\n",
    "                activations_BLF = dictionary.encode(activations_BLD)\n",
    "            activations_BL = activations_BLF[:, :, feat_idx].save()\n",
    "\n",
    "        activations[i * batch_size : (i + 1) * batch_size] = activations_BL.value\n",
    "        tokens[i * batch_size : (i + 1) * batch_size] = tokens_BL.value\n",
    "\n",
    "    token_mean_acts = {}\n",
    "    for ctx in tokens:\n",
    "        for tok in ctx:\n",
    "            if tok.item() in token_mean_acts:\n",
    "                continue\n",
    "            idxs = (tokens == tok).nonzero(as_tuple=True)\n",
    "            token_mean_acts[tok.item()] = activations[idxs].mean().item()\n",
    "    top_tokens = sorted(token_mean_acts.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    top_tokens = [(model.tokenizer.decode(tok), act) for tok, act in top_tokens]\n",
    "\n",
    "    flattened_acts = einops.rearrange(activations, \"b n -> (b n)\")\n",
    "    topk_indices = torch.argsort(flattened_acts, dim=0, descending=True)[:k]\n",
    "    batch_indices = topk_indices // activations.shape[1]\n",
    "    token_indices = topk_indices % activations.shape[1]\n",
    "    tokens = [\n",
    "        tokens[batch_idx, : token_idx + 1].tolist()\n",
    "        for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "    ]\n",
    "    activations = [\n",
    "        activations[batch_idx, : token_id + 1, None, None]\n",
    "        for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "    ]\n",
    "    decoded_tokens = _list_decode(tokens)\n",
    "    top_contexts = text_neuron_activations(decoded_tokens, activations)\n",
    "\n",
    "    top_affected = interp.feature_effect(\n",
    "        model, submodule, dictionary, feat_idx, tokens, max_length=max_length, k=k\n",
    "    )\n",
    "    top_affected = [(model.tokenizer.decode(tok), prob.item()) for tok, prob in zip(*top_affected)]\n",
    "\n",
    "    return namedtuple(\"featureProfile\", [\"top_contexts\", \"top_tokens\", \"top_affected\"])(\n",
    "        top_contexts, top_tokens, top_affected\n",
    "    )\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model = LanguageModel(model_name, device_map=DEVICE, dispatch=True)\n",
    "\n",
    "ae_path = ae_paths[2]\n",
    "submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)\n",
    "\n",
    "context_length = config['buffer']['ctx_len']\n",
    "\n",
    "data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    d_submodule=512,\n",
    "    ctx_len=context_length,\n",
    "    refresh_batch_size=128, # decrease to fit on smaller GPUs\n",
    "    n_ctxs=512, # decrease to fit on smaller GPUs\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import einops\n",
    "# import dictionary_learning.interp as interp\n",
    "# from circuitsvis.activations import text_neuron_activations\n",
    "# from collections import namedtuple\n",
    "# from nnsight import LanguageModel\n",
    "\n",
    "# import experiments.utils as utils\n",
    "# from dictionary_learning.utils import hf_dataset_to_generator\n",
    "# from dictionary_learning.buffer import ActivationBuffer\n",
    "\n",
    "\n",
    "# DEBUGGING = True\n",
    "\n",
    "# if DEBUGGING:\n",
    "#     tracer_kwargs = dict(scan=True, validate=True)\n",
    "# else:\n",
    "#     tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "\n",
    "# def examine_dimension(\n",
    "#     model,\n",
    "#     submodule,\n",
    "#     buffer,\n",
    "#     dim_indices: torch.Tensor,\n",
    "#     context_length: int,\n",
    "#     batch_size: int,\n",
    "#     dictionary=None,\n",
    "#     max_length: int = 128,\n",
    "#     n_inputs: int = 512,\n",
    "#     k: int = 30,\n",
    "# ):\n",
    "\n",
    "#     def _list_decode(x):\n",
    "#         if isinstance(x, int):\n",
    "#             return model.tokenizer.decode(x)\n",
    "#         else:\n",
    "#             return [_list_decode(y) for y in x]\n",
    "\n",
    "#     # if dim_indices is None:\n",
    "#     # dim_indices = random.randint(0, activations.shape[-1] - 1)\n",
    "\n",
    "#     assert n_inputs % batch_size == 0\n",
    "#     n_iters = n_inputs // batch_size\n",
    "\n",
    "#     dim_count = dim_indices.shape[0]\n",
    "\n",
    "#     device = model.device\n",
    "\n",
    "#     activations_bLF = torch.zeros((n_inputs, context_length, dim_count), device=device)\n",
    "#     tokens_bL = torch.zeros((n_inputs, context_length), dtype=torch.long, device=device)\n",
    "\n",
    "#     for i in range(n_iters):\n",
    "#         inputs_BL = buffer.tokenized_batch(batch_size=batch_size)\n",
    "\n",
    "#         with torch.no_grad(), model.trace(inputs_BL, **tracer_kwargs):\n",
    "#             tokens_BL = model.input[1][\n",
    "#                 \"input_ids\"\n",
    "#             ].save()  # if you're getting errors, check here; might only work for pythia models\n",
    "#             activations_BLD = submodule.output\n",
    "#             if type(activations_BLD.shape) == tuple:\n",
    "#                 activations_BLD = activations_BLD[0]\n",
    "#             if dictionary is not None:\n",
    "#                 activations_BLF = dictionary.encode(activations_BLD)\n",
    "#             activations_BLF = activations_BLF[:, :, dim_indices].save()\n",
    "\n",
    "#         activations_bLF[i * batch_size : (i + 1) * batch_size] = activations_BLF.value\n",
    "#         tokens_bL[i * batch_size : (i + 1) * batch_size] = tokens_BL.value\n",
    "\n",
    "#     token_mean_acts = {}\n",
    "#     for ctx in tokens_bL:\n",
    "#         for tok in ctx:\n",
    "#             if tok.item() in token_mean_acts:\n",
    "#                 continue\n",
    "#             idxs = (tokens_bL == tok).nonzero(as_tuple=True)\n",
    "#             token_mean_acts[tok.item()] = activations_bLF[idxs].mean().item()\n",
    "#     top_tokens = sorted(token_mean_acts.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "#     top_tokens = [(model.tokenizer.decode(tok), act) for tok, act in top_tokens]\n",
    "\n",
    "#     flattened_acts_NF = einops.rearrange(activations_bLF, \"B L F -> (B L F)\")\n",
    "#     topk_indices = torch.argsort(flattened_acts_NF, dim=0, descending=True)[:k]\n",
    "#     batch_indices = topk_indices // activations_bLF.shape[1]\n",
    "#     token_indices = topk_indices % activations_bLF.shape[1]\n",
    "#     tokens_bL = [\n",
    "#         tokens_bL[batch_idx, : token_idx + 1].tolist()\n",
    "#         for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "#     ]\n",
    "#     activations_bLF = [\n",
    "#         activations_bLF[batch_idx, : token_id + 1, None, None]\n",
    "#         for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "#     ]\n",
    "#     decoded_tokens = _list_decode(tokens_bL)\n",
    "\n",
    "#     return namedtuple(\n",
    "#         \"featureProfile\",\n",
    "#         [\n",
    "#             \"top_tokens\",\n",
    "#             \"encoded_tokens_bL\",\n",
    "#             \"decoded_tokens_bL\",\n",
    "#             \"activations_bLF\",\n",
    "#         ],\n",
    "#     )(top_tokens, tokens_bL, decoded_tokens, activations_bLF)\n",
    "\n",
    "\n",
    "# DEVICE = \"cuda\"\n",
    "# model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# model = LanguageModel(model_name, device_map=DEVICE, dispatch=True)\n",
    "\n",
    "# ae_path = ae_paths[2]\n",
    "# submodule, dictionary, config = utils.load_dictionary(model, ae_path, DEVICE)\n",
    "\n",
    "# context_length = config[\"buffer\"][\"ctx_len\"]\n",
    "\n",
    "# data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "# buffer = ActivationBuffer(\n",
    "#     data,\n",
    "#     model,\n",
    "#     submodule,\n",
    "#     d_submodule=512,\n",
    "#     ctx_len=context_length,\n",
    "#     refresh_batch_size=128,  # decrease to fit on smaller GPUs\n",
    "#     n_ctxs=512,  # decrease to fit on smaller GPUs\n",
    "#     device=DEVICE,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_effects_filename = f\"{ae_path}/node_effects.pkl\"\n",
    "\n",
    "with open(node_effects_filename, \"rb\") as f:\n",
    "    node_effects = pickle.load(f)\n",
    "\n",
    "effects = node_effects[-2][ae_path]\n",
    "\n",
    "print(effects.shape)\n",
    "\n",
    "k = 10\n",
    "top_k_values, top_k_indices = torch.topk(effects, k)\n",
    "\n",
    "print(top_k_values)\n",
    "print(top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_idx = 0\n",
    "sae_feat_idx = top_k_indices[feat_idx].item()\n",
    "print(sae_feat_idx)\n",
    "\n",
    "n_inputs = 1024\n",
    "batch_size = 256\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    sae_feat_idx,\n",
    "    n_inputs,\n",
    "    context_length,\n",
    "    batch_size,\n",
    "    dictionary,\n",
    "    max_length=context_length,\n",
    "    k=30,\n",
    ")\n",
    "\n",
    "print(f'\\n\\ntop activating tokens for feature {sae_feat_idx}')\n",
    "for token in out.top_tokens:\n",
    "    print(token)\n",
    "print(f'\\n\\ntop affected tokens for feature {sae_feat_idx}')\n",
    "for token in out.top_affected:\n",
    "    print(token)\n",
    "\n",
    "out.top_contexts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
