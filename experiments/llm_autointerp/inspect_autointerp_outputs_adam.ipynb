{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from experiments.pipeline_config import PipelineConfig\n",
    "from experiments.llm_autointerp.llm_query import perform_llm_autointerp, construct_llm_features_prompts\n",
    "from typing import Dict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run autointerp with\n",
    "```python\n",
    "cd experiments\n",
    "python llm_autointerp/run_autointerp_can.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create run.sh script for above to run llm_query with custom args from this notebook\n",
    "# For now, manually copy hyperparameters here\n",
    "repo_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "ae_path = \"dictionary_learning/dictionaries/pythia70m_sweep_topk_ctx128_0730/resid_post_layer_3/trainer_10\"\n",
    "ae_path = os.path.abspath(os.path.join(repo_dir, ae_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the raw llm output when querying llm, set DEBUG=True\n",
    "with open(os.path.join(ae_path, \"raw_llm_outputs.json\"), \"r\") as f:\n",
    "    raw_llm_outputs = json.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"extracted_json_llm_outputs.json\"), \"r\") as f:\n",
    "    extracted_json_llm_outputs = json.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects.pkl\"), \"rb\") as f:\n",
    "    node_effects_classprobe = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects_auto_interp_attrib_top20.pkl\"), \"rb\") as f:\n",
    "    node_effects_autointerp = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects_dist_diff.pkl\"), \"rb\") as f:\n",
    "    node_effects_classprobe = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"node_effects_auto_interp_dist_top20.pkl\"), \"rb\") as f:\n",
    "    node_effects_autointerp = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(ae_path, \"max_activating_inputs.pkl\"), \"rb\") as f:\n",
    "    max_activating_inputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show config\n",
    "cfg = PipelineConfig()\n",
    "\n",
    "cfg.prompt_dir = \"llm_autointerp/\"\n",
    "cfg.force_autointerp_recompute = True\n",
    "cfg.chosen_autointerp_class_names = [\n",
    "    \"gender\",\n",
    "    \"professor\",\n",
    "    \"nurse\",\n",
    "    \"accountant\",\n",
    "    \"architect\",\n",
    "    \"attorney\",\n",
    "    \"dentist\",\n",
    "]\n",
    "\n",
    "cfg.num_top_features_per_class = 1\n",
    "\n",
    "for k, v in cfg.__dict__.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_effects_classprobe.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_effects_autointerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_interpretable = 0\n",
    "\n",
    "for class_name in node_effects_autointerp:\n",
    "    # if isinstance(class_name, str):\n",
    "    #     continue\n",
    "    effects = node_effects_autointerp[class_name]\n",
    "    total_interpretable += (effects > 0).sum()\n",
    "\n",
    "total_interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cfg.num_features_per_class for each class\n",
    "from experiments.utils_bib_dataset import profession_dict\n",
    "import torch as t\n",
    "\n",
    "top_feature_idxs_per_class = {}\n",
    "\n",
    "for cls in cfg.chosen_autointerp_class_names:\n",
    "    if cls in ['gender', 'professor', 'nurse', ]:\n",
    "        continue\n",
    "    else:\n",
    "        cls_idx = profession_dict[cls]\n",
    "\n",
    "    top_feature_vals, top_feature_idxs = t.topk(node_effects_classprobe[cls_idx], cfg.num_top_features_per_class)\n",
    "    top_feature_idxs_per_class[cls] = top_feature_idxs\n",
    "    autointerp_vals = node_effects_autointerp[cls_idx][top_feature_idxs]\n",
    "    extracted_jsons = [extracted_json_llm_outputs[str(j.item())] for j in top_feature_idxs]\n",
    "    # raw_outputs = [raw_llm_outputs[str(j.item())][0][-100:] for j in top_feature_idxs]\n",
    "    print(f\"{cls}:\")\n",
    "    print(f'feature_idx, probe_effect, autointerp_effects_val, autointerp_extracted_json')\n",
    "    for i in range(cfg.num_top_features_per_class):\n",
    "        if extracted_jsons[i] is not None:\n",
    "            print(f'{top_feature_idxs[i]}, {top_feature_vals[i]}, {autointerp_vals[i]}, {extracted_jsons[i][cls]}')#, raw_outputs[i])\n",
    "        else:\n",
    "            print(f'{top_feature_idxs[i]}, {top_feature_vals[i]}, {autointerp_vals[i]}, None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_prompts = construct_llm_features_prompts(ae_path, model.tokenizer, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Are the outputs that have been set to 0 really not related at all\n",
    "\n",
    "for cls in cfg.chosen_autointerp_class_names:\n",
    "    if cls in ['gender', 'professor', 'nurse', ]:\n",
    "        continue\n",
    "    for top_feature_idx in top_feature_idxs_per_class[cls]:\n",
    "        idx = top_feature_idx.item()\n",
    "        if node_effects_autointerp[profession_dict[cls]][idx] == 0:\n",
    "            print(f'feature_idx: {idx}, class: {cls}')\n",
    "            print(f\"(high effect on the classprobe of this class). Autointerp scored with 0 for this class. Here is the prompt to autointerp:\\n\")\n",
    "            print(f'this is the rating from autointerp: {extracted_json_llm_outputs[str(idx)]}')\n",
    "            print(features_prompts[idx][:-1])\n",
    "\n",
    "            # print border\n",
    "            print(\"\\n--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feat idx with obvious errors for further inspection\n",
    "\n",
    "err_idxs = [\n",
    "    (6036, 'architect'),\n",
    "    (14889, 'attorney'),\n",
    "]\n",
    "\n",
    "for idx, cls in err_idxs:\n",
    "    print(f'feature_idx: {idx}, class: {cls}')\n",
    "    print(f'prompts: {features_prompts[idx]}')\n",
    "    print(f'raw_llm_output: {raw_llm_outputs[str(idx)][0]}')\n",
    "\n",
    "    print(f'\\n\\n___________________________________\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_prompts[14889])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
