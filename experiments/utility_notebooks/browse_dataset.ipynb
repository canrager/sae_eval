{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from collections import defaultdict\n",
    "from experiments.dataset_info import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_amazon_category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect different datasets within amazon_reviews\n",
    "# dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_All_Beauty\", trust_remote_code=True)\n",
    "# dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)\n",
    "# dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"0core_rating_only_All_Beauty\", trust_remote_code=True)\n",
    "# dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"0core_last_out_All_Beauty\", trust_remote_code=True)\n",
    "# dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"0core_timestamp_w_his_All_Beauty\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_beauty = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True, streaming=True)\n",
    "ds_beauty = iter(dataset_beauty[\"full\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(ds_beauty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_electronics = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Electronics\", trust_remote_code=True, streaming=True)\n",
    "ds_electronics = iter(dataset_electronics[\"full\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(ds_electronics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample, category, char_count_range):\n",
    "    return {\n",
    "        'text': f'Title: {sample[\"title\"]}\\nReview: {sample[\"text\"][:char_count_range[-1]]}',\n",
    "        'rating': amazon_rating_dict[sample['rating']],\n",
    "        'category': category,\n",
    "    }\n",
    "\n",
    "def filter_and_sample(dataset, ratings, category, n_samples, char_count_range):\n",
    "    rating_counter = {rating: 0 for rating in ratings}\n",
    "    samples_dict = defaultdict(list)\n",
    "    for sample in dataset:\n",
    "        l = len(sample['text'])\n",
    "        if l < char_count_range[0]:\n",
    "            continue\n",
    "        r = sample['rating'] \n",
    "        if r in rating_counter:\n",
    "            samples_dict[r].append(process_sample(sample, category, char_count_range))\n",
    "            rating_counter[r] += 1\n",
    "            if rating_counter[r] >= n_samples:\n",
    "                rating_counter.pop(r) # desired number of samples reached\n",
    "        if len(rating_counter) == 0:\n",
    "            break\n",
    "    return samples_dict\n",
    "\n",
    "def split_samples_dict(samples_dict, n_train_samples_per_rating, n_test_samples_per_rating):\n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "    for samples in samples_dict.values():\n",
    "        train_samples.extend(samples[:n_train_samples_per_rating])\n",
    "        test_samples.extend(samples[n_train_samples_per_rating:n_train_samples_per_rating+n_test_samples_per_rating])\n",
    "    return train_samples, test_samples # contains multiple rating_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "# dataset_electronics = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Electronics\", trust_remote_code=True, streaming=True)\n",
    "# dataset_beauty = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True, streaming=True)\n",
    "# dataset_dict = {\n",
    "#     'electronics': dataset_electronics['full'],\n",
    "#     'beauty': dataset_beauty['full'],\n",
    "# }\n",
    "\n",
    "categories_to_load = full_amazon_category_dict.keys()\n",
    "dataset_dict = {\n",
    "    category: load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_review_{category}\", split='full', trust_remote_code=True, streaming=True)\n",
    "    for category in categories_to_load\n",
    "}\n",
    "\n",
    "ratings = (1.0, 5.0) # There's no 0 star rating in the dataset\n",
    "# ratings = (1.0, 2.0, 3.0, 4.0, 5.0) # There's no 0 star rating in the dataset\n",
    "char_count_range = [500, 750]\n",
    "n_train_samples_per_rating = 10000\n",
    "n_test_samples_per_rating = 2500\n",
    "n_total_samples_per_rating = n_train_samples_per_rating + n_test_samples_per_rating\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "for category, dataset in dataset_dict.items():\n",
    "    print(f\"Processing {category} dataset...\")\n",
    "    category_idx = full_amazon_category_dict[category]\n",
    "    samples_dict = filter_and_sample(dataset, ratings, category_idx, n_total_samples_per_rating, char_count_range)\n",
    "    train_samples_category, test_samples_category = split_samples_dict(samples_dict, n_train_samples_per_rating, n_test_samples_per_rating)\n",
    "    train_samples.extend(train_samples_category)\n",
    "    test_samples.extend(test_samples_category)\n",
    "\n",
    "balanced_dataset = DatasetDict({\n",
    "    'train': Dataset.from_list(train_samples),\n",
    "    'test': Dataset.from_list(test_samples),\n",
    "})\n",
    "\n",
    "print(len(balanced_dataset['train']), len(balanced_dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to huggingface\n",
    "\n",
    "ratings_part = \"_\".join(str(rating) for rating in ratings)\n",
    "categories_part = \"_\".join(dataset_dict.keys())\n",
    "\n",
    "fname = f\"dataset_{ratings_part}_{categories_part}_{n_total_samples_per_rating}\"\n",
    "fname = 'dataset_all_categories_ratings_1and5_train10000_test2500'\n",
    "\n",
    "# balanced_dataset.push_to_hub(repo_id = f\"canrager/amazon_reviews_mcauley\", config_name=f\"{fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect how bib is formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.probe_training import load_and_prepare_dataset\n",
    "\n",
    "\n",
    "bib_train_df, bib_test_df = load_and_prepare_dataset('bias_in_bios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bib_train_df), len(bib_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bib_train_df.groupby(['profession', 'gender']).size().unstack(fill_value=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running stuff with this dataset: replicate test_interventions.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
