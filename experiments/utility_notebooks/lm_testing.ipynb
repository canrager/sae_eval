{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "\n",
    "import experiments.utils as utils\n",
    "import experiments.probe_training as probe_training\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\"Short input\", \"Another short input\", \"F\", \"This is a longer input that is more than 10 characters long\"],\n",
    "}\n",
    "\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model_dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LanguageModel(model_name, torch_dtype=model_dtype, device_map=device, attn_implementation=\"eager\")\n",
    "\n",
    "tokenized_data = utils.tokenize_data(data, model.tokenizer, 128, device)\n",
    "\n",
    "submodule = model.gpt_neox.layers[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_data['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaned_activations = probe_training.get_all_meaned_activations(tokenized_data['inputs'], model, 2, submodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meaned_activations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(tokenized_data['inputs']):\n",
    "    acts_BLD = submodule.output.save()\n",
    "    input = model.input.save()\n",
    "\n",
    "acts_BLD = acts_BLD.value\n",
    "if isinstance(acts_BLD, tuple):\n",
    "    acts_BLD = acts_BLD[0]\n",
    "\n",
    "print(acts_BLD.shape)\n",
    "\n",
    "attn_mask_BL = input.value[1][\"attention_mask\"]\n",
    "print(attn_mask_BL.shape)\n",
    "\n",
    "acts_BL_D = acts_BLD[attn_mask_BL != 0]\n",
    "\n",
    "print(acts_BL_D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.pipeline_config import PipelineConfig\n",
    "\n",
    "ae_sweep_paths = {\"pythia70m_test_sae\": {\"resid_post_layer_3\": {\"trainer_ids\": [0]}}}\n",
    "p_config = PipelineConfig()\n",
    "\n",
    "sweep_name, submodule_trainers = list(ae_sweep_paths.items())[0]\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(\n",
    "    p_config.dictionaries_path, sweep_name, submodule_trainers\n",
    ")\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "print(ae_paths)\n",
    "\n",
    "ae_path = ae_paths[0]\n",
    "\n",
    "submodule, dictionary, sae_config = utils.load_dictionary(model, ae_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments.autointerp as autointerp\n",
    "\n",
    "decoder_weight_DF = autointerp.get_decoder_weight(dictionary)\n",
    "print(decoder_weight_DF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "probe_path = os.path.join(p_config.probes_dir, \"pythia-70m-deduped\", \"spurious_probes_bias_in_bios_professor_nurse_ctx_len_128_layer_3.pkl\")\n",
    "\n",
    "with open(probe_path, \"rb\") as f:\n",
    "    probe = pickle.load(f)\n",
    "\n",
    "print(probe.keys())\n",
    "print(probe['male / female'].net.weight.shape)\n",
    "\n",
    "probe_weight_D = probe['male / female'].net.weight.to(dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod_F = probe_weight_D @ decoder_weight_DF\n",
    "print(dot_prod_F.shape)\n",
    "\n",
    "# min, mean, max\n",
    "print(dot_prod_F.min(), dot_prod_F.mean(), dot_prod_F.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effects_path = os.path.join(ae_path, \"node_effects.pkl\")\n",
    "effects_orig_path = os.path.join(ae_path, \"node_effects_orig.pkl\")\n",
    "\n",
    "with open(effects_path, \"rb\") as f:\n",
    "    effects = pickle.load(f)\n",
    "\n",
    "with open(effects_orig_path, \"rb\") as f:\n",
    "    effects_orig = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(effects.keys())\n",
    "print(effects_orig.keys())\n",
    "\n",
    "effects_F = effects['male / female']\n",
    "effects_orig_F = effects_orig['male / female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get the top 20 values and their indices from effects_orig_F\n",
    "top20_orig_values, top20_orig_indices = torch.topk(effects_orig_F, 20)\n",
    "\n",
    "# Get the top 20 values and their indices from effects_F\n",
    "top20_F_values, top20_F_indices = torch.topk(effects_F, 20)\n",
    "\n",
    "print(\"Top 20 indices from effects_orig_F:\")\n",
    "print(top20_orig_indices)\n",
    "\n",
    "print(\"Top 20 indices from effects_F:\")\n",
    "print(top20_F_indices)\n",
    "\n",
    "# Find how many of the top 20 indices from effects_orig_F are in the top 20 of effects_F\n",
    "common_indices = set(top20_orig_indices.tolist()) & set(top20_F_indices.tolist())\n",
    "num_common = len(common_indices)\n",
    "\n",
    "print(f\"Number of common indices in top 20: {num_common}\")\n",
    "\n",
    "# If you want to see the actual common indices:\n",
    "print(\"Common indices:\", common_indices)\n",
    "\n",
    "# If you want to see the values for these common indices in both tensors:\n",
    "for idx in common_indices:\n",
    "    orig_value = effects_orig_F[idx].item()\n",
    "    F_value = effects_F[idx].item()\n",
    "    print(f\"Index {idx}: Original value = {orig_value:.4f}, New value = {F_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
