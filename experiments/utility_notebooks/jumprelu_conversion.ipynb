{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = {\n",
    "    \"trainer\": {\n",
    "        \"trainer_class\": \"TrainerJumpRelu\",\n",
    "        \"dict_class\": \"JumpReluAutoEncoder\",\n",
    "        \"lr\": -1,\n",
    "        \"l1_penalty\": -1,\n",
    "        \"steps\": -1,\n",
    "        \"seed\": -1,\n",
    "        \"activation_dim\": 2304,\n",
    "        \"dict_size\": 16384,\n",
    "        \"device\": \"cuda:0\",\n",
    "        \"layer\": 11,\n",
    "        \"lm_name\": \"google/gemma-2-2b\",\n",
    "        \"wandb_name\": \"-1\",\n",
    "        \"submodule_name\": \"resid_post_layer_11\"\n",
    "    },\n",
    "    \"buffer\": {\n",
    "        \"d_submodule\": 2304,\n",
    "        \"io\": \"out\",\n",
    "        \"n_ctxs\": 2000,\n",
    "        \"ctx_len\": 128,\n",
    "        \"refresh_batch_size\": 32,\n",
    "        \"out_batch_size\": 4096,\n",
    "        \"device\": \"cuda:0\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model is only required if you want to make sure that the SAEs have the correct L0\n",
    "\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "import json\n",
    "\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.dictionary import (\n",
    "    IdentityDict,\n",
    "    GatedAutoEncoder,\n",
    "    AutoEncoderNew,\n",
    ")\n",
    "from dictionary_learning.trainers.top_k import AutoEncoderTopK\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "device = \"cpu\"\n",
    "model_dtype = torch.bfloat16\n",
    "\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    dispatch=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=model_dtype,\n",
    ")\n",
    "\n",
    "layer = 11\n",
    "submodule = model.model.layers[layer]\n",
    "with model.trace(\"Hello World\"):\n",
    "    activations_BLD = submodule.output\n",
    "\n",
    "    if type(submodule.output.shape) == tuple:\n",
    "        activations_BLD = activations_BLD[0]\n",
    "\n",
    "    orig_activations_BLD = activations_BLD.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from dictionary_learning.dictionary import JumpReluAutoEncoder\n",
    "\n",
    "def save_folder(layer: int, l0s: list[int]):\n",
    "\n",
    "    save_dir = \"gemma-2-2b\"\n",
    "\n",
    "    local_dir = f\"gemma-2-2b_sweep_jumprelu_0902/resid_post_layer_{layer}\"\n",
    "\n",
    "    for idx, l0 in enumerate(l0s):\n",
    "\n",
    "        sae_dir = os.path.join(local_dir, f\"trainer_{idx}\")\n",
    "\n",
    "        path_to_params = hf_hub_download(\n",
    "            repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "            filename=f\"layer_{layer}/width_16k/average_l0_{l0}/params.npz\",\n",
    "            force_download=False,\n",
    "            cache_dir=save_dir,\n",
    "        )\n",
    "\n",
    "        os.makedirs(sae_dir, exist_ok=True)\n",
    "\n",
    "        params = np.load(path_to_params)\n",
    "        pt_params = {k: torch.from_numpy(v).cuda() for k, v in params.items()}\n",
    "\n",
    "        embed_dim = params['W_enc'].shape[0]\n",
    "        latent_dim = params['W_enc'].shape[1]\n",
    "\n",
    "        sae = JumpReluAutoEncoder(\n",
    "            activation_dim=embed_dim,\n",
    "            dict_size=latent_dim,\n",
    "            device=\"cpu\"\n",
    "        )\n",
    "\n",
    "        # For original JumpReluAutoEncoder using nn.Parameter instead of nn.Linear\n",
    "        sae.load_state_dict(pt_params)\n",
    "\n",
    "        # If you want to use nn.Linear instead of nn.Parameter\n",
    "        # sae.encoder.weight.data = pt_params['W_enc'].T\n",
    "        # sae.decoder.weight.data = pt_params['W_dec'].T\n",
    "        # sae.b_enc.data = pt_params['b_enc']\n",
    "        # sae.b_dec.data = pt_params['b_dec']\n",
    "        # sae.threshold.data = pt_params['threshold']\n",
    "\n",
    "        sae.to(\"cpu\")\n",
    "\n",
    "        torch.save(sae.state_dict(), os.path.join(sae_dir, \"ae.pt\"))\n",
    "\n",
    "        config[\"trainer\"][\"activation_dim\"] = embed_dim\n",
    "        config[\"trainer\"][\"dict_size\"] = latent_dim\n",
    "\n",
    "        config[\"trainer\"][\"layer\"] = layer\n",
    "        config[\"trainer\"][\"submodule_name\"] = f\"resid_post_layer_{layer}\"\n",
    "\n",
    "        with open(os.path.join(sae_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "\n",
    "l0_dict = {\n",
    "    3: [14, 28, 59, 142, 315],\n",
    "    7: [20, 36, 69, 137, 285],\n",
    "    11: [22, 41, 80, 168, 393],\n",
    "    15: [23, 41, 78, 150, 308],\n",
    "    19: [23, 40, 73, 137, 279]\n",
    "}\n",
    "\n",
    "for layer in l0_dict:\n",
    "    save_folder(layer, l0_dict[layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_path = \"/workspace/sae_eval/dictionary_learning/dictionaries/gemma-2-2b_sweep_jumprelu_0902/resid_post_layer_11/trainer_0/ae.pt\"\n",
    "ae_path = \"/workspace/sae_eval/experiments/gemma-2-2b_sweep_jumprelu_0902/resid_post_layer_11/trainer_0/ae.pt\"\n",
    "\n",
    "sae = JumpReluAutoEncoder(\n",
    "    activation_dim=2304,\n",
    "    dict_size=16384,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "sae.load_state_dict(torch.load(ae_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(orig_activations_BLD.shape)\n",
    "\n",
    "sae = sae.to(dtype=model_dtype)\n",
    "activations_BLD = orig_activations_BLD[:, 1:, :] # Skip the BOS token\n",
    "ae_activations_BLF = sae.encode(activations_BLD)\n",
    "print(ae_activations_BLF.shape)\n",
    "reconstructed_activations_BLD = sae.decode(ae_activations_BLF)\n",
    "print(reconstructed_activations_BLD.shape)\n",
    "\n",
    "l0 = (ae_activations_BLF != 0).float().sum(dim=-1).mean()\n",
    "l2_loss = torch.linalg.norm(activations_BLD - reconstructed_activations_BLD, dim=-1).mean()\n",
    "\n",
    "print(l0, l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sae.W_dec.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
