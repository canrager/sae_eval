{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import experiments.utils as utils\n",
    "\n",
    "DICTIONARIES_PATH = \"../dictionary_learning/dictionaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity_penalty(config: dict, trainer_class: str) -> float:\n",
    "    if trainer_class == \"TrainerTopK\":\n",
    "        return config[\"trainer\"][\"k\"]\n",
    "    elif trainer_class == \"PAnnealTrainer\":\n",
    "        return config[\"trainer\"][\"sparsity_penalty\"]\n",
    "    else:\n",
    "        return config[\"trainer\"][\"l1_penalty\"]\n",
    "\n",
    "\n",
    "def get_l0_frac_recovered(ae_paths: list[str]) -> dict[str, dict[str, float]]:\n",
    "    results = {}\n",
    "    for ae_path in ae_paths:\n",
    "        eval_results_file = f\"{ae_path}/eval_results.json\"\n",
    "        if not os.path.exists(eval_results_file):\n",
    "            print(f\"Warning: {eval_results_file} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        with open(eval_results_file, \"r\") as f:\n",
    "            eval_results = json.load(f)\n",
    "\n",
    "        l0 = eval_results[\"l0\"]\n",
    "        frac_recovered = eval_results[\"frac_recovered\"]\n",
    "\n",
    "        results[ae_path] = {\n",
    "            \"l0\": l0,\n",
    "            \"frac_recovered\": frac_recovered,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def add_ae_config_results(\n",
    "    ae_paths: list[str], results: dict[str, dict[str, float]]\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    for ae_path in ae_paths:\n",
    "        config_file = f\"{ae_path}/config.json\"\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        trainer_class = config[\"trainer\"][\"trainer_class\"]\n",
    "        results[ae_path][\"trainer_class\"] = trainer_class\n",
    "        results[ae_path][\"l1_penalty\"] = get_sparsity_penalty(config, trainer_class)\n",
    "\n",
    "        results[ae_path][\"lr\"] = config[\"trainer\"][\"lr\"]\n",
    "        results[ae_path][\"dict_size\"] = config[\"trainer\"][\"dict_size\"]\n",
    "        results[ae_path][\"steps\"] = config[\"trainer\"][\"steps\"]\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe_names(filename_filter: str, attrib_dir: int) -> tuple[str, str, str]:\n",
    "    eval_probe_class_id = \"male_professor / female_nurse\"\n",
    "    if \"_bias_shift\" in filename_filter:\n",
    "        ablated_probe_class_id = eval_probe_class_id\n",
    "\n",
    "        if filename_filter == \"_bias_shift_dir1\":\n",
    "            eval_data_class_id = \"professor / nurse\"\n",
    "        elif filename_filter == \"_bias_shift_dir2\":\n",
    "            eval_data_class_id = \"male / female\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid filename filter.\")\n",
    "    else:\n",
    "        if attrib_dir == 1:\n",
    "            ablated_probe_class_id = \"male / female\"\n",
    "            eval_data_class_id = \"professor / nurse\"\n",
    "        elif attrib_dir == 2:\n",
    "            ablated_probe_class_id = \"professor / nurse\"\n",
    "            eval_data_class_id = \"male / female\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid attrib_dir.\")\n",
    "        \n",
    "    return ablated_probe_class_id, eval_probe_class_id, eval_data_class_id\n",
    "\n",
    "def get_spurious_correlation_plotting_dict(\n",
    "    ae_paths: list[str],\n",
    "    acc_key: str = \"acc\",\n",
    "    filename_filters: tuple[str] = (\"_attrib\", \"_auto_interp\", \"_bias_shift_dir1\", \"_bias_shift_dir2\"),\n",
    ") -> tuple[dict[str, dict[str, float]], float]:\n",
    "    results = {}\n",
    "    orig_acc = None\n",
    "\n",
    "    for ae_path in ae_paths:\n",
    "        eval_results_file = f\"{ae_path}/eval_results.json\"\n",
    "\n",
    "        if not os.path.exists(eval_results_file):\n",
    "            print(f\"Warning: {eval_results_file} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        with open(eval_results_file, \"r\") as f:\n",
    "            eval_results = json.load(f)\n",
    "\n",
    "        l0 = eval_results[\"l0\"]\n",
    "        frac_recovered = eval_results[\"frac_recovered\"]\n",
    "\n",
    "        results[ae_path] = {\n",
    "            \"l0\": l0,\n",
    "            \"frac_recovered\": frac_recovered,\n",
    "        }\n",
    "\n",
    "        config_file = f\"{ae_path}/config.json\"\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        trainer_class = config[\"trainer\"][\"trainer_class\"]\n",
    "        results[ae_path][\"trainer_class\"] = trainer_class\n",
    "        results[ae_path][\"l1_penalty\"] = get_sparsity_penalty(config, trainer_class)\n",
    "\n",
    "        results[ae_path][\"lr\"] = config[\"trainer\"][\"lr\"]\n",
    "        results[ae_path][\"dict_size\"] = config[\"trainer\"][\"dict_size\"]\n",
    "        if \"steps\" in config[\"trainer\"]:\n",
    "            results[ae_path][\"steps\"] = config[\"trainer\"][\"steps\"]\n",
    "        else:\n",
    "            results[ae_path][\"steps\"] = -1\n",
    "\n",
    "        for filename_filter in filename_filters:\n",
    "\n",
    "            class_accuracies_file = f\"{ae_path}/class_accuracies{filename_filter}.pkl\"\n",
    "\n",
    "            if not os.path.exists(class_accuracies_file):\n",
    "                print(\n",
    "                    f\"Warning: {class_accuracies_file} does not exist. Removing this path from results.\"\n",
    "                )\n",
    "                del results[ae_path]\n",
    "                continue\n",
    "\n",
    "            with open(class_accuracies_file, \"rb\") as f:\n",
    "                class_accuracies = pickle.load(f)\n",
    "\n",
    "            # for class_id in class_accuracies['clean_acc']:\n",
    "            #     print(class_id, class_accuracies['clean_acc'][class_id])\n",
    "\n",
    "            dirs = [1,2]\n",
    "            if \"bias_shift_dir\" in filename_filter:\n",
    "                dirs = [0]\n",
    "\n",
    "            for dir in dirs:\n",
    "\n",
    "                ablated_probe_class_id, eval_probe_class_id, eval_data_class_id = get_probe_names(filename_filter, dir)\n",
    "\n",
    "\n",
    "                for threshold in class_accuracies[ablated_probe_class_id]:\n",
    "\n",
    "                    clean_acc = class_accuracies[\"clean_acc\"][eval_data_class_id][\"acc\"]\n",
    "\n",
    "                    combined_class_name = f\"{eval_probe_class_id} probe on {eval_data_class_id} data\"\n",
    "\n",
    "                    original_acc = class_accuracies[\"clean_acc\"][combined_class_name][\"acc\"]\n",
    "                    if orig_acc is None:\n",
    "                        orig_acc = original_acc\n",
    "                        print(f\"Original acc: {original_acc}\")\n",
    "\n",
    "                    changed_acc = class_accuracies[ablated_probe_class_id][threshold][combined_class_name][\n",
    "                        acc_key\n",
    "                    ]\n",
    "\n",
    "                    changed_acc = (changed_acc - original_acc) / (clean_acc - original_acc)\n",
    "\n",
    "                    if dir == 0:\n",
    "                        metric_key = f\"scr{filename_filter}_threshold_{threshold}\"\n",
    "                    else:\n",
    "                        metric_key = f\"scr{filename_filter}_dir{dir}_threshold_{threshold}\"\n",
    "\n",
    "                    results[ae_path][metric_key] = changed_acc\n",
    "\n",
    "    if orig_acc is None:\n",
    "        raise ValueError(f\"original_acc not found for {ae_paths}\")\n",
    "    return results, orig_acc\n",
    "\n",
    "\n",
    "def get_classes(first_path: str) -> list[int]:\n",
    "    class_accuracies_file = f\"{first_path}/class_accuracies.pkl\"\n",
    "    with open(class_accuracies_file, \"rb\") as f:\n",
    "        class_accuracies = pickle.load(f)\n",
    "    return list(class_accuracies[\"clean_acc\"].keys())\n",
    "\n",
    "def create_tpp_plotting_dict(\n",
    "    ae_paths: list[str],\n",
    "    intended_filter_class_ids: list[int],\n",
    "    unintended_filter_class_ids: list[int],\n",
    "    filename_filters: tuple[str] = (\"_attrib\", \"_auto_interp\"),\n",
    "    acc_key: str = \"acc\",\n",
    "    save_results: bool = True,\n",
    ") -> dict:\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for ae_path in ae_paths:\n",
    "\n",
    "        results[ae_path] = {}\n",
    "\n",
    "        for filename_filter in filename_filters:\n",
    "\n",
    "            intended_diffs = {}\n",
    "            unintended_diffs = {}\n",
    "\n",
    "\n",
    "            class_accuracies_file = f\"{ae_path}/class_accuracies{filename_filter}.pkl\"\n",
    "\n",
    "            if not os.path.exists(class_accuracies_file):\n",
    "                print(\n",
    "                    f\"Warning: {class_accuracies_file} does not exist. Skipping this path.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            with open(class_accuracies_file, \"rb\") as f:\n",
    "                class_accuracies = pickle.load(f)\n",
    "\n",
    "            classes = list(class_accuracies[\"clean_acc\"].keys())\n",
    "\n",
    "\n",
    "            for class_id in classes:\n",
    "                if isinstance(class_id, str) and \" probe on \" in class_id:\n",
    "                    raise ValueError(\"This is deprecated, shouldn't be here.\")\n",
    "\n",
    "                if intended_filter_class_ids and class_id not in intended_filter_class_ids:\n",
    "                    continue\n",
    "\n",
    "                clean = class_accuracies[\"clean_acc\"][class_id][\"acc\"]\n",
    "\n",
    "                for threshold in class_accuracies[class_id]:\n",
    "                    patched = class_accuracies[class_id][threshold][class_id][acc_key]\n",
    "\n",
    "                    diff = clean - patched\n",
    "\n",
    "                    if threshold not in intended_diffs:\n",
    "                        intended_diffs[threshold] = []\n",
    "\n",
    "                    intended_diffs[threshold].append(diff)\n",
    "\n",
    "            for intended_class_id in classes:\n",
    "                if isinstance(intended_class_id, str) and \" probe on \" in intended_class_id:\n",
    "                    raise ValueError(\"This is deprecated, shouldn't be here.\")\n",
    "\n",
    "                # If we have a filter, skip the unintended classes\n",
    "                if intended_filter_class_ids and intended_class_id not in intended_filter_class_ids:\n",
    "                    continue\n",
    "\n",
    "                for unintended_class_id in classes:\n",
    "                    if intended_class_id == unintended_class_id:\n",
    "                        continue\n",
    "\n",
    "                    # If we have a filter, skip the unintended classes\n",
    "                    if (\n",
    "                        unintended_filter_class_ids\n",
    "                        and unintended_class_id not in unintended_filter_class_ids\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    if isinstance(unintended_class_id, str) and \" probe on \" in unintended_class_id:\n",
    "                        raise ValueError(\"This is deprecated, shouldn't be here.\")\n",
    "\n",
    "                    clean = class_accuracies[\"clean_acc\"][unintended_class_id][\"acc\"]\n",
    "\n",
    "                    for threshold in class_accuracies[intended_class_id]:\n",
    "                        patched = class_accuracies[intended_class_id][threshold][unintended_class_id][\n",
    "                            acc_key\n",
    "                        ]\n",
    "                        diff = clean - patched\n",
    "\n",
    "                        if threshold not in unintended_diffs:\n",
    "                            unintended_diffs[threshold] = []\n",
    "\n",
    "                        unintended_diffs[threshold].append(diff)\n",
    "\n",
    "            for threshold in intended_diffs:\n",
    "                assert threshold in unintended_diffs\n",
    "\n",
    "                average_intended_diff = sum(intended_diffs[threshold]) / len(intended_diffs[threshold])\n",
    "                average_unintended_diff = sum(unintended_diffs[threshold]) / len(unintended_diffs[threshold])\n",
    "                average_diff = average_intended_diff - average_unintended_diff\n",
    "\n",
    "                results[ae_path][f\"tpp{filename_filter}_threshold_{threshold}_total_metric\"] = average_diff\n",
    "                results[ae_path][f\"tpp{filename_filter}_threshold_{threshold}_intended_diff_only\"] = average_intended_diff\n",
    "                results[ae_path][f\"tpp{filename_filter}_threshold_{threshold}_unintended_diff_only\"] = average_unintended_diff\n",
    "\n",
    "        if save_results:\n",
    "\n",
    "            single_ae_results_dict = results[ae_path]\n",
    "            single_ae_results_dict['hyperparameters'] = {}\n",
    "            single_ae_results_dict['hyperparameters']['classes'] = classes\n",
    "\n",
    "            print(f\"Saving results for {ae_path}\")\n",
    "            with open(f\"{ae_path}/tpp_results.json\", \"w\") as f:\n",
    "                json.dump(single_ae_results_dict, f)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe_clean_accuracies(ae_paths: list[str], filename_filter: str, acc_key: str) -> dict:\n",
    "    for ae_path in ae_paths:\n",
    "        class_accuracies_file = f\"{ae_path}/class_accuracies{filename_filter}.pkl\"\n",
    "        if not os.path.exists(class_accuracies_file):\n",
    "            print(f\"Warning: {class_accuracies_file} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        with open(class_accuracies_file, \"rb\") as f:\n",
    "            class_accuracies = pickle.load(f)\n",
    "\n",
    "        return class_accuracies[\"clean_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to generate graphs, where you manually populate sweep_name and submodule_trainers\n",
    "sweep_name = \"pythia70m_test_sae\"\n",
    "submodule_trainers = {\"resid_post_layer_3\": {\"trainer_ids\": [0]}}\n",
    "\n",
    "# Current recommended way to generate graphs. You can copy paste ae_sweep_paths directly from bib_intervention.py\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\"resid_post_layer_3\": {\"trainer_ids\": [1, 7, 11, 18]}}\n",
    "}\n",
    "\n",
    "# trainer_ids = [2, 6, 10, 14, 18]\n",
    "trainer_ids = None\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\n",
    "        # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "    },\n",
    "    \"pythia70m_sweep_topk_ctx128_0730\": {\n",
    "        # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_3\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "    # \"pythia70m_sweep_gated_ctx128_0730\": {\n",
    "    #     # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_3\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "    #     \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "}\n",
    "\n",
    "# ae_sweep_paths = {\n",
    "#     \"pythia70m_sweep_topk_ctx128_0730\": {\n",
    "#         # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "#         # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "#         # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "#         \"resid_post_layer_3\": {\"trainer_ids\": None},\n",
    "#         # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "trainer_ids = None\n",
    "# trainer_ids = [1]\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"gemma-2-2b_sweep_standard_ctx128_ef8_0824\": {\n",
    "        # \"resid_post_layer_12\": {\"trainer_ids\": trainer_ids},\n",
    "        \"resid_post_layer_15\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_20\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824\": {\n",
    "        # \"resid_post_layer_12\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "        \"resid_post_layer_15\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_20\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "    \"gemma-2-2b_sweep_jumprelu_0902\": {\n",
    "        \"resid_post_layer_15\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "}\n",
    "\n",
    "# trainer_ids = None\n",
    "# ae_sweep_paths = {\n",
    "#     \"gemma-2-2b_sweep_jumprelu_0902_probe_layer24_results\": {\n",
    "#         \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "#     \"gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer24_results\": {\n",
    "#         \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "#     \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer24_results\": {\n",
    "#         \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "#     \"gemma-2-2b_sweep_standard_ctx128_ef2_0824_probe_layer_24_results\": {\n",
    "#         \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "#     \"gemma-2-2b_sweep_topk_ctx128_ef2_0824_probe_layer_24_results\": {\n",
    "#         \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# ae_sweep_paths = {\n",
    "#     \"gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer20_results\": {\n",
    "#         \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "#     \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer20_results\": {\n",
    "#         \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "#     },\n",
    "# }\n",
    "\n",
    "trainer_ids = None\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer_11_tpp\": {\n",
    "        \"resid_post_layer_11_checkpoints\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer_15_tpp\": {\n",
    "        \"resid_post_layer_15_checkpoints\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer_19_tpp\": {\n",
    "        \"resid_post_layer_19_checkpoints\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "}\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer_19_tpp\": None,\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer_19_tpp\": None,\n",
    "    \"gemma-2-2b_sweep_jumprelu_0902_probe_layer_19_tpp\": None,\n",
    "}\n",
    "\n",
    "# ae_sweep_paths = {\n",
    "#     \"gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer_19_tpp\": None,\n",
    "#     \"gemma-2-2b_sweep_topk_ctx128_ef8_0824_probe_layer_19_tpp\": None,\n",
    "# }\n",
    "\n",
    "DICTIONARIES_PATH = (\n",
    "    \"../dictionary_learning/dictionaries/09_20_gemma_tpp_autointerp_topk_standard_jumprelu\"\n",
    ")\n",
    "\n",
    "plot_spurious = False\n",
    "plot_tpp = True\n",
    "plot_checkpoints = False\n",
    "plot_averaged_results = False\n",
    "\n",
    "l0_threshold = 500\n",
    "\n",
    "model = \"Gemma-2-2B\"\n",
    "\n",
    "l0_threshold = None\n",
    "\n",
    "no_title = True\n",
    "\n",
    "\n",
    "\n",
    "ae_paths = []\n",
    "\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "    ae_group_paths = utils.get_ae_group_paths(DICTIONARIES_PATH, sweep_name, submodule_trainers)\n",
    "    ae_paths.extend(utils.get_ae_paths(ae_group_paths))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_sweep_paths = {}\n",
    "\n",
    "DICTIONARIES_PATH = (\n",
    "    \"../dictionary_learning/dictionaries/09_20_gemma_spurious_autointerp_topk_standard_jumprelu\"\n",
    ")\n",
    "\n",
    "plot_spurious = True\n",
    "plot_tpp = False\n",
    "plot_checkpoints = False\n",
    "plot_averaged_results = True\n",
    "\n",
    "l0_threshold = 500\n",
    "\n",
    "model = \"Gemma-2-2B\"\n",
    "\n",
    "l0_threshold = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "# print(ae_paths)\n",
    "if plot_tpp:\n",
    "    # If not empty, this will filter to only include the specified class ids\n",
    "    intended_filter_class_ids = []\n",
    "    unintended_filter_class_ids = []\n",
    "\n",
    "if plot_averaged_results:\n",
    "    ae_group_paths = [\n",
    "        \"gemma-2-2b_sweep_topk_ctx128_ef8_0824\",\n",
    "        \"gemma-2-2b_sweep_standard_ctx128_ef8_0824\",\n",
    "        \"gemma-2-2b_sweep_jumprelu_0902\",\n",
    "    ]\n",
    "    probe_layers = [19]\n",
    "    column1_vals_list = [\n",
    "        (\"professor\", \"nurse\"),\n",
    "        (\"architect\", \"journalist\"),\n",
    "        # (\"painter\", \"teacher\"),\n",
    "        # (\"photographer\", \"physician\"),\n",
    "    ]\n",
    "    dataset = \"bias_in_bios\"\n",
    "\n",
    "    # dataset = \"amazon_reviews_1and5\"\n",
    "    # column1_vals_list = [\n",
    "    #     (\"Books\", \"CDs_and_Vinyl\"),\n",
    "    #     # (\"Software\", \"Electronics\"),\n",
    "    #     (\"Pet_Supplies\", \"Office_Products\"),\n",
    "    # ]\n",
    "\n",
    "    \n",
    "\n",
    "    ignore_sae_filters = [\"trainer_4\"]\n",
    "    ignore_sae_filters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_tpp:\n",
    "    tpp_results = create_tpp_plotting_dict(\n",
    "        ae_paths,\n",
    "        intended_filter_class_ids,\n",
    "        unintended_filter_class_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to true if you have multiple spurious correlation runs to average over\n",
    "# like: gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer_15_spurious_bias_in_bios_filmmaker_dentist\n",
    "# and: gemma-2-2b_sweep_standard_ctx128_ef8_0824_probe_layer_15_spurious_bias_in_bios_painter_teacher\n",
    "\n",
    "\n",
    "def get_all_probe_clean_accuracies(\n",
    "    ae_group_path: str,\n",
    "    probe_layer: int,\n",
    "    column1_vals_list: list[tuple[str, str]],\n",
    "    filename_filter: str,\n",
    "    acc_key: str = \"acc\",\n",
    "    intervention_type: str = \"spurious\",\n",
    "    dataset: str = \"bias_in_bios\",\n",
    "):\n",
    "    ae_base_path = f\"{ae_group_path}_probe_layer_{probe_layer}_{intervention_type}_{dataset}\"\n",
    "    class_acc_dict = {}\n",
    "\n",
    "    for column1_vals in column1_vals_list:\n",
    "        ae_run_path = f\"{ae_base_path}_{column1_vals[0]}_{column1_vals[1]}\"\n",
    "        sweep_name = ae_run_path\n",
    "        submodule_trainers = None\n",
    "        ae_group_paths = utils.get_ae_group_paths(DICTIONARIES_PATH, sweep_name, submodule_trainers)\n",
    "        ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "        clean_probe_accs = get_probe_clean_accuracies(ae_paths, filename_filter, acc_key)\n",
    "\n",
    "        class_acc_dict[column1_vals] = clean_probe_accs\n",
    "\n",
    "    return class_acc_dict\n",
    "\n",
    "\n",
    "def average_multiple_scr_runs(\n",
    "    ae_group_path: str,\n",
    "    dictionaries_path: str,\n",
    "    probe_layer: int,\n",
    "    column1_vals_list: list[tuple[str, str]],\n",
    "    ignore_sae_filters: list[str],\n",
    "    acc_key: str = \"acc\",\n",
    "    intervention_type: str = \"spurious\",\n",
    "    dataset: str = \"bias_in_bios\",\n",
    ") -> tuple[dict, float, dict]:\n",
    "    ae_base_path = f\"{ae_group_path}_probe_layer_{probe_layer}_{intervention_type}_{dataset}\"\n",
    "    ae_output_path = f\"{ae_group_path}\"\n",
    "\n",
    "    all_results = {}\n",
    "    original_accs = []\n",
    "    class_acc_dict = {}\n",
    "\n",
    "    for column1_vals in column1_vals_list:\n",
    "        ae_run_path = f\"{ae_base_path}_{column1_vals[0]}_{column1_vals[1]}\"\n",
    "        sweep_name = ae_run_path\n",
    "        submodule_trainers = None\n",
    "        ae_group_paths = utils.get_ae_group_paths(DICTIONARIES_PATH, sweep_name, submodule_trainers)\n",
    "        ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "        temp_results, orig_acc = get_spurious_correlation_plotting_dict(\n",
    "            ae_paths,\n",
    "            acc_key,\n",
    "        )\n",
    "\n",
    "        class_acc_dict[column1_vals] = orig_acc\n",
    "\n",
    "        all_results.update(temp_results)\n",
    "        original_accs.append(orig_acc)\n",
    "\n",
    "        if orig_acc is None:\n",
    "            raise ValueError(f\"Original acc is None for {ae_run_path}\")\n",
    "\n",
    "    final_results = {}\n",
    "\n",
    "    average_orig_acc = sum(original_accs) / len(original_accs)\n",
    "\n",
    "    for ae_path in all_results:\n",
    "        skip_path = False\n",
    "        for filter in ignore_sae_filters:\n",
    "            if filter in ae_path:\n",
    "                skip_path = True\n",
    "                break\n",
    "\n",
    "        if skip_path:\n",
    "            continue\n",
    "\n",
    "        orig_ae_path = ae_path\n",
    "        name_fixed = False\n",
    "        for column1_vals in column1_vals_list:\n",
    "            ae_run_path = f\"{ae_base_path}_{column1_vals[0]}_{column1_vals[1]}\"\n",
    "            if ae_run_path in ae_path:\n",
    "                ae_path = ae_path.replace(ae_run_path, ae_output_path)\n",
    "                ae_path = ae_path.split(dictionaries_path)[1]\n",
    "                name_fixed = True\n",
    "        assert name_fixed\n",
    "\n",
    "        if ae_path not in final_results:\n",
    "            final_results[ae_path] = all_results[orig_ae_path]\n",
    "        else:\n",
    "            for key in all_results[orig_ae_path]:\n",
    "                if isinstance(all_results[orig_ae_path][key], float):\n",
    "                    final_results[ae_path][key] += all_results[orig_ae_path][key]\n",
    "\n",
    "    for ae_path in final_results:\n",
    "        for key in final_results[ae_path]:\n",
    "            if isinstance(final_results[ae_path][key], float):\n",
    "                final_results[ae_path][key] /= len(column1_vals_list)\n",
    "\n",
    "    return final_results, average_orig_acc, class_acc_dict\n",
    "\n",
    "\n",
    "if plot_averaged_results:\n",
    "    all_averaged_results = {}\n",
    "    all_orig_accs = []\n",
    "\n",
    "    for probe_layer in probe_layers:\n",
    "        averaged_results = {}\n",
    "        orig_accs = []\n",
    "\n",
    "        for ae_group_path in ae_group_paths:\n",
    "            single_averaged_results, single_average_orig_acc, class_act_dict = (\n",
    "                average_multiple_scr_runs(\n",
    "                    ae_group_path,\n",
    "                    DICTIONARIES_PATH + \"/\",\n",
    "                    probe_layer,\n",
    "                    column1_vals_list,\n",
    "                    ignore_sae_filters,\n",
    "                    acc_key=\"acc\",\n",
    "                    dataset=dataset,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            averaged_results.update(single_averaged_results)\n",
    "            orig_accs.append(single_average_orig_acc)\n",
    "\n",
    "        average_orig_acc = sum(orig_accs) / len(orig_accs)\n",
    "\n",
    "        print(average_orig_acc)\n",
    "\n",
    "    all_averaged_results.update(averaged_results)\n",
    "    all_orig_accs.append(average_orig_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_averaged_results.keys())\n",
    "first_key = next(iter(all_averaged_results.keys()))\n",
    "print(all_averaged_results[first_key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump json results\n",
    "if plot_averaged_results:\n",
    "    with open(f\"all_scr_results.json\", \"w\") as f:\n",
    "        json.dump(all_averaged_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_averaged_results and plot_checkpoints:\n",
    "    y_label = \"Probe Accuracy Increase\"\n",
    "\n",
    "    if no_title:\n",
    "        title = None\n",
    "\n",
    "    plot_steps_vs_average_diff(\n",
    "        all_averaged_results,\n",
    "        steps_key=\"steps\",\n",
    "        avg_diff_key=custom_metric1,\n",
    "        title=title,\n",
    "        y_label=y_label,\n",
    "        output_filename=f\"{image_filename_prefix}_checkpoints.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spurious_probe_acc_table(\n",
    "    class_acc_dict: dict[tuple[str, str], float],\n",
    "    evaled_probe_class_id: str,\n",
    "    model_name: str,\n",
    "    acc_key: str = \"acc\",\n",
    "):\n",
    "    print(f\"# {model_name} Clean Probe Accuracies\")\n",
    "    print()\n",
    "    # Print the header in Markdown format\n",
    "    print(\"| Class 1 / Class 2      | Train Accuracy | Profession Accuracy | Gender Accuracy |\")\n",
    "    print(\n",
    "        \"|------------------------|---------------------|---------------------|-----------------|\"\n",
    "    )\n",
    "\n",
    "    # Iterate over the items in dir1 (assuming keys are shared between dir1 and dir2)\n",
    "    for column1_vals, clean_accs in class_acc_dict.items():\n",
    "        class_name = f\"{column1_vals[0]} / {column1_vals[1]}\"\n",
    "\n",
    "        train_acc = clean_accs[evaled_probe_class_id][acc_key]\n",
    "\n",
    "        dir1_acc_key = f\"{evaled_probe_class_id} probe on professor / nurse data\"\n",
    "        dir2_acc_key = f\"{evaled_probe_class_id} probe on male / female data\"\n",
    "\n",
    "        dir1_acc = clean_accs[dir1_acc_key][acc_key]\n",
    "        dir2_acc = clean_accs[dir2_acc_key][acc_key]\n",
    "\n",
    "        print(\n",
    "            f\"| {class_name:<22} | {train_acc:.4f}              | {dir1_acc:.4f}              | {dir2_acc:.4f}           |\"\n",
    "        )\n",
    "\n",
    "\n",
    "def single_class_probe_acc_table(\n",
    "    class_acc_dict: dict[tuple[str, str], float],\n",
    "    model_name: str,\n",
    "    acc_key: str = \"acc\",\n",
    "):\n",
    "    print(f\"# {model_name} Clean Probe Accuracies\")\n",
    "    print()\n",
    "    # Print the header in Markdown format\n",
    "    print(\"| Class 1 / Class 2      | Profession Accuracy | Gender Accuracy |\")\n",
    "    print(\"|------------------------|---------------------|-----------------|\")\n",
    "\n",
    "    # Iterate over the items in dir1 (assuming keys are shared between dir1 and dir2)\n",
    "    for column1_vals, clean_accs in class_acc_dict.items():\n",
    "        class_name = f\"{column1_vals[0]} / {column1_vals[1]}\"\n",
    "\n",
    "        dir1_acc = clean_accs[\"professor / nurse\"][acc_key]\n",
    "        dir2_acc = clean_accs[\"male / female\"][acc_key]\n",
    "\n",
    "        print(f\"| {class_name:<22} | | {dir1_acc:.4f}              | {dir2_acc:.4f}           |\")\n",
    "\n",
    "\n",
    "evaled_probe = \"male_professor / female_nurse\"\n",
    "\n",
    "if plot_averaged_results:\n",
    "    for probe_layer in probe_layers:\n",
    "        for ae_group_path in ae_group_paths:\n",
    "            class_act_dict = get_all_probe_clean_accuracies(\n",
    "                ae_group_path, probe_layer, column1_vals_list, filename_filter, dataset=dataset\n",
    "            )\n",
    "\n",
    "            break\n",
    "        assert class_act_dict is not None\n",
    "\n",
    "        spurious_probe_acc_table(class_act_dict, evaled_probe, model)\n",
    "        single_class_probe_acc_table(class_act_dict, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
