{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "activation_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "# data preparation hyperparameters\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "# To fit on 24GB VRAM GPU, I set the next 2 default batch_sizes to 64\n",
    "def get_data(train=True, ambiguous=True, batch_size=64, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=64, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "\n",
    "layer = 4 # model layer for attaching linear classification head\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to(DEVICE)\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1] \n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()\n",
    "    \n",
    "def get_acts(text):\n",
    "    with t.no_grad(): \n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.input[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous test accuracy 0.9247211813926697\n",
      "ground truth accuracy: 0.9285714030265808\n",
      "unintended feature accuracy: 0.489631325006485\n"
     ]
    }
   ],
   "source": [
    "oracle, _ = train_probe(get_acts, label_idx=0, batches=get_data(ambiguous=False))\n",
    "print(\"ambiguous test accuracy\", test_probe(oracle, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print(\"ground truth accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=0))\n",
    "print(\"unintended feature accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9679111242294312\n",
      "Accuracy for (0, 1): 0.9576417207717896\n",
      "Accuracy for (1, 0): 0.9032257795333862\n",
      "Accuracy for (1, 1): 0.884061336517334\n"
     ]
    }
   ],
   "source": [
    "# get worst-group accuracy of oracle probe\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(oracle, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9957016706466675\n",
      "Ground truth accuracy: 0.5921658873558044\n",
      "Unintended feature accuracy: 0.9020737409591675\n"
     ]
    }
   ],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9977167248725891\n",
      "Accuracy for (0, 1): 0.17649267613887787\n",
      "Accuracy for (1, 0): 0.22580644488334656\n",
      "Accuracy for (1, 1): 0.9941914677619934\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dictionaries\n",
    "\n",
    "import importlib\n",
    "import dictionary_learning\n",
    "from dictionary_learning import AutoEncoder\n",
    "importlib.reload(dictionary_learning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# dictionary hyperparameters\n",
    "dict_id = 10\n",
    "expansion_factor = 64\n",
    "dictionary_size = expansion_factor * activation_dim\n",
    "\n",
    "submodules = []\n",
    "dictionaries = {}\n",
    "\n",
    "submodules.append(model.gpt_neox.embed_in)\n",
    "dictionaries[model.gpt_neox.embed_in] = AutoEncoder.from_pretrained(\n",
    "    f'../dictionaries/pythia-70m-deduped/embed/{dict_id}_{dictionary_size}/ae.pt',\n",
    "    device=DEVICE\n",
    ")\n",
    "for i in range(layer + 1):\n",
    "    submodules.append(model.gpt_neox.layers[i].attention)\n",
    "    dictionaries[model.gpt_neox.layers[i].attention] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/attn_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i].mlp)\n",
    "    dictionaries[model.gpt_neox.layers[i].mlp] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/mlp_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i])\n",
    "    dictionaries[model.gpt_neox.layers[i]] = AutoEncoder.from_pretrained(\n",
    "        f'../dictionaries/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.input[1]['attention_mask']\n",
    "    acts = model.gpt_neox.layers[layer].output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "    \n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:54<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# find most influential features\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_data(train=True, ambiguous=True, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "946 0.2662804424762726\n",
      "5719 0.15922614932060242\n",
      "7392 0.4218404293060303\n",
      "10784 0.18791018426418304\n",
      "17846 0.37995392084121704\n",
      "22068 0.20120769739151\n",
      "23079 0.17827855050563812\n",
      "25904 0.13578002154827118\n",
      "28533 0.2241894155740738\n",
      "29476 0.2351006269454956\n",
      "31461 0.20119118690490723\n",
      "31467 0.19470244646072388\n",
      "32081 0.39268073439598083\n",
      "32469 1.5978020429611206\n",
      "Component 1:\n",
      "4427 0.10151158273220062\n",
      "23752 0.12343307584524155\n",
      "Component 2:\n",
      "2995 0.12388136237859726\n",
      "3842 0.17480571568012238\n",
      "10258 0.3594623804092407\n",
      "13387 0.17669181525707245\n",
      "13968 0.15486972033977509\n",
      "14861 0.11585155874490738\n",
      "18382 0.31952595710754395\n",
      "19369 0.2192731499671936\n",
      "21736 0.10511206090450287\n",
      "28127 1.267708659172058\n",
      "30037 0.11253583431243896\n",
      "30518 0.23797392845153809\n",
      "Component 3:\n",
      "1022 0.38165226578712463\n",
      "9651 0.7097998857498169\n",
      "10060 2.938981294631958\n",
      "18967 0.8327097296714783\n",
      "22084 0.3067134916782379\n",
      "23898 0.5447293519973755\n",
      "24799 0.12614016234874725\n",
      "26504 0.38154157996177673\n",
      "29626 0.3487662672996521\n",
      "31201 0.21050408482551575\n",
      "Component 4:\n",
      "8147 0.12726058065891266\n",
      "Component 5:\n",
      "24159 0.29754629731178284\n",
      "25018 0.5960058569908142\n",
      "29397 0.10581367462873459\n",
      "Component 6:\n",
      "4592 0.4792095720767975\n",
      "8920 0.7144878506660461\n",
      "9877 0.4431629180908203\n",
      "12128 0.7757964134216309\n",
      "15017 3.8023176193237305\n",
      "17369 0.1061968132853508\n",
      "17673 0.10113462060689926\n",
      "26969 0.1340278536081314\n",
      "30248 1.253791332244873\n",
      "Component 7:\n",
      "13570 0.15163393318653107\n",
      "27472 1.4931261539459229\n",
      "Component 8:\n",
      "Component 9:\n",
      "1995 1.2087295055389404\n",
      "4539 0.1011863574385643\n",
      "8944 0.11456690728664398\n",
      "9128 1.846718192100525\n",
      "11656 0.19466502964496613\n",
      "12440 0.22674992680549622\n",
      "14638 0.16199353337287903\n",
      "29206 0.2646845579147339\n",
      "29295 0.7898961305618286\n",
      "31098 0.13643264770507812\n",
      "Component 10:\n",
      "2959 1.1779327392578125\n",
      "19128 0.3210594952106476\n",
      "22029 0.21496647596359253\n",
      "Component 11:\n",
      "Component 12:\n",
      "19558 1.8631731271743774\n",
      "23545 0.4520905613899231\n",
      "24806 0.14762826263904572\n",
      "27334 0.18942807614803314\n",
      "31182 0.1075160875916481\n",
      "31453 0.2057984620332718\n",
      "Component 13:\n",
      "22821 0.1198783591389656\n",
      "31101 0.692297101020813\n",
      "Component 14:\n",
      "Component 15:\n",
      "9766 0.27969229221343994\n",
      "12420 1.6309893131256104\n",
      "30220 0.3233208656311035\n",
      "total features: 77\n"
     ]
    }
   ],
   "source": [
    "n_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    for idx in (effect > 0.1).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        n_features += 1\n",
    "print(f\"total features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db1ab0e6ba042618635f88812f140e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6504db301d664801a738e2017ddde5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' nursing', 5.238176345825195), (' nurses', 2.8025174140930176), (' nurse', 2.7965087890625), (' Teaching', 0.7016171216964722), (' rehabilitation', 0.6920654773712158), ('unte', 0.6074657440185547), (' caring', 0.429257333278656), ('akers', 0.3814794421195984), (' volunteers', 0.32599467039108276), (' Medical', 0.21756425499916077), (' relational', 0.19916227459907532), (' Dou', 0.1964026391506195), (' teaching', 0.17128399014472961), (' health', 0.10173860192298889), (' hospitals', 0.09990262985229492), (' education', 0.08860695362091064), (' engineering', 0.07574862241744995), (' patient', 0.07372689247131348), (' stance', 0.0615004301071167), (' dors', 0.04598349332809448), (' OR', 0.038925766944885254), (' toddler', 0.030461221933364868), ('opedic', 0.02716928720474243), (' undergraduate', 0.020936429500579834), (' and', 0.009745706804096699), (' but', 0.008150335401296616), ('It', 0.0), (' is', 0.0), (' done', 0.0), (',', 0.0)]\n",
      "[('�', 0.5553182363510132), ('��', 0.5427490472793579), ('�', 0.5322347283363342), ('�', 0.5259277820587158), ('�', 0.5168904662132263), ('�', 0.5127319693565369), (' homes', 0.470832884311676), ('�', 0.4686523675918579), (' home', 0.46425744891166687), ('giving', 0.46108347177505493), ('�', 0.456205278635025), ('�', 0.44180911779403687), ('�', 0.42859703302383423), ('�', 0.4101603329181671), ('�', 0.4076782464981079), ('��', 0.4056966304779053), ('�', 0.4025675654411316), ('�', 0.40143638849258423), ('�', 0.38992515206336975), ('�', 0.38831788301467896), ('rs', 0.3857172727584839), ('�', 0.37440186738967896), ('��', 0.3740234673023224), (' mothers', 0.3710566461086273), ('��', 0.3704793453216553), (' aides', 0.36413925886154175), ('�', 0.3620605766773224), ('�', 0.35971274971961975), ('��', 0.35635173320770264), ('�', 0.35432130098342896)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-10d6eefc-2c2c\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-10d6eefc-2c2c\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\", \" provision\", \" for\", \" nurses\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\"], [\"PCI\", \" Alternative\", \" Using\", \" S\", \"ust\", \"ained\", \" Exercise\", \" (\", \"PA\", \"USE\", \"):\", \" R\", \"ational\", \"e\", \" and\", \" trial\", \" design\", \".\", \"\\n\", \"Card\", \"i\", \"ovascular\", \" disease\", \" (\", \"C\", \"VD\", \")\", \" currently\", \" claims\", \" nearly\", \" one\", \" million\", \" lives\", \" yearly\", \" in\", \" the\", \" US\", \",\", \" accounting\", \" for\", \" nearly\", \" 40\", \"%\", \" of\", \" all\", \" deaths\", \".\", \" Coron\", \"ary\", \" artery\", \" disease\", \" (\", \"CAD\", \")\", \" accounts\", \" for\", \" the\", \" largest\", \" number\", \" of\", \" these\", \" deaths\", \".\", \" While\", \" efforts\", \" aimed\", \" at\", \" treating\", \" CAD\", \" in\", \" recent\", \" decades\", \" have\", \" concentrated\", \" on\", \" surgical\", \" and\", \" catheter\", \"-\", \"based\", \" interventions\", \",\", \" limited\", \" resources\", \" have\", \" been\", \" directed\", \" toward\", \" prevention\", \" and\", \" rehabilitation\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\", \" provision\", \" for\", \" nurses\", \" and\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"ers\", \" at\", \" CAM\", \"C\", \" bring\", \" their\", \" unique\", \" personalities\", \" and\", \" skills\", \" to\", \" our\", \" hospital\", \".\", \" They\", \" range\", \" in\", \" age\", \" from\", \" 15\", \" to\", \" 99\", \".\", \" Our\", \" ranks\", \" are\", \" made\", \" up\", \" of\", \" men\", \" and\", \" women\", \";\", \" students\", \" and\", \" retire\", \"es\", \";\", \" homem\", \"akers\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\"], [\"j\", \"OO\", \"Q\", \" on\", \" The\", \" OR\", \"M\", \" Foundation\", \"?\", \"\\n\", \"\\n\", \"I\", \" am\", \" the\", \" developer\", \" of\", \" j\", \"OO\", \"Q\", \",\", \" a\", \" Java\", \" database\", \" abstraction\", \" framework\", \".\", \" I\", \" was\", \" wondering\", \" whether\", \" j\", \"OO\", \"Q\", \" might\", \" be\", \" an\", \" interesting\", \" tool\", \" for\", \" discussion\", \" on\", \" your\", \" website\", \",\", \" even\", \" if\", \" it\", \" is\", \" not\", \" exactly\", \" an\", \" OR\", \"M\", \" in\", \" the\", \" classic\", \" meaning\", \" (\", \"as\", \" in\", \" mapping\", \" objects\", \" to\", \" the\", \" relational\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"ers\", \" at\", \" CAM\", \"C\", \" bring\", \" their\", \" unique\", \" personalities\", \" and\", \" skills\", \" to\", \" our\", \" hospital\", \".\", \" They\", \" range\", \" in\", \" age\", \" from\", \" 15\", \" to\", \" 99\", \".\", \" Our\", \" ranks\", \" are\", \" made\", \" up\", \" of\", \" men\", \" and\", \" women\", \";\", \" students\", \" and\", \" retire\", \"es\", \";\", \" homem\", \"akers\", \" and\", \" business\", \" people\", \".\", \" Last\", \" year\", \",\", \" 334\", \" volunteers\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"ers\", \" at\", \" CAM\", \"C\", \" bring\", \" their\", \" unique\", \" personalities\", \" and\", \" skills\", \" to\", \" our\", \" hospital\", \".\", \" They\", \" range\", \" in\", \" age\", \" from\", \" 15\", \" to\", \" 99\", \".\", \" Our\", \" ranks\", \" are\", \" made\", \" up\", \" of\", \" men\", \" and\", \" women\", \";\", \" students\", \" and\", \" retire\", \"es\", \";\", \" homem\", \"akers\", \" and\", \" business\", \" people\", \".\", \" Last\", \" year\", \",\", \" 334\", \" volunteers\", \" contributed\", \" over\", \" 36\", \",\", \"000\", \" hours\", \" to\", \" our\", \" hospitals\", \" and\", \" Cancer\", \" Center\", \".\", \"\\n\", \"\\n\", \"We\", \" are\", \" looking\", \" for\", \" volunteers\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\"], [\"Vol\", \"unte\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"2017\", \" X\", \"IX\", \"O\", \" Ladies\", \" Open\", \" H\", \"\\u00f3\", \"d\", \"me\", \"z\", \"\\u0151\", \"v\", \"\\u00e1s\", \"\\u00e1r\", \"he\", \"ly\", \" \\u2013\", \" Dou\"], [\"j\", \"OO\", \"Q\", \" on\", \" The\", \" OR\", \"M\", \" Foundation\", \"?\", \"\\n\", \"\\n\", \"I\", \" am\", \" the\", \" developer\", \" of\", \" j\", \"OO\", \"Q\", \",\", \" a\", \" Java\", \" database\", \" abstraction\", \" framework\", \".\", \" I\", \" was\", \" wondering\", \" whether\", \" j\", \"OO\", \"Q\", \" might\", \" be\", \" an\", \" interesting\", \" tool\", \" for\", \" discussion\", \" on\", \" your\", \" website\", \",\", \" even\", \" if\", \" it\", \" is\", \" not\", \" exactly\", \" an\", \" OR\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"2017\", \" X\", \"IX\", \"O\", \" Ladies\", \" Open\", \" H\", \"\\u00f3\", \"d\", \"me\", \"z\", \"\\u0151\", \"v\", \"\\u00e1s\", \"\\u00e1r\", \"he\", \"ly\", \" \\u2013\", \" Dou\", \"bles\", \"\\n\", \"\\n\", \"L\", \"aura\", \" Pig\", \"oss\", \"i\", \" and\", \" Nad\", \"ia\", \" Pod\", \"or\", \"os\", \"ka\", \" were\", \" the\", \" defending\", \" champions\", \",\", \" but\", \" both\", \" players\", \" chose\", \" not\", \" to\", \" participate\", \".\", \"\\n\", \"\\n\", \"K\", \"ot\", \"omi\", \" Tak\", \"ah\", \"ata\", \" and\", \" Pr\", \"arth\", \"ana\", \" Th\", \"omb\", \"are\", \" won\", \" the\", \" title\", \" after\", \" Ul\", \"rik\", \"ke\", \" E\", \"iker\", \"i\", \" and\", \" Te\", \"re\", \"za\", \" Mr\", \"de\", \"\\u017e\", \"a\", \" retired\", \" in\", \" the\", \" final\", \" at\", \" 1\", \"\\u2013\", \"0\", \".\", \"\\n\", \"\\n\", \"Se\", \"eds\", \"\\n\", \"\\n\", \"Draw\", \"\\n\", \"\\n\", \"References\", \"\\n\", \"Main\", \" Draw\", \"\\n\", \"\\n\", \"X\", \"IX\", \"O\", \" Ladies\", \" Open\", \" H\", \"\\u00f3\", \"d\", \"me\", \"z\", \"\\u0151\", \"v\", \"\\u00e1s\", \"\\u00e1r\", \"he\", \"ly\", \" -\", \" Dou\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\"], [\"During\", \" my\", \" pregnancy\", \",\", \" I\", \" tried\", \" to\", \" gather\", \" as\", \" much\", \" information\", \" on\", \" how\", \" painful\", \" labor\", \" might\", \" actually\", \" be\", \".\", \" I\", \" would\", \" often\", \" hear\", \" \\u201c\", \"mine\", \" was\", \" horrible\", \",\", \" but\", \" everyone\", \"\\u2019\", \"s\", \" pregnancy\", \" is\", \" different\", \"\\u201d\", \" or\", \" \\u201c\", \"it\", \" was\", \" the\", \" worst\", \" pain\", \" I\", \"\\u2019\", \"ve\", \" ever\", \" felt\", \" in\", \" my\", \" life\", \"!\\u201d\", \"\\n\", \"\\n\", \"I\", \" heard\", \" many\", \" horror\", \" stories\", \" which\", \" often\", \" ended\", \" with\", \",\", \" \\u201c\", \"well\", \",\", \" don\", \"\\u2019\", \"t\", \" worry\", \".\", \" You\", \"\\u2019\", \"ll\", \" forget\", \" about\", \" the\", \" pain\", \" as\", \" soon\", \" as\", \" your\", \" child\", \" is\", \" born\", \".\\u201d\", \" Not\", \" the\", \" most\", \" reassuring\", \" for\", \" a\", \" first\", \"-\", \"time\", \" mother\", \",\", \" but\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"ers\", \" at\", \" CAM\", \"C\", \" bring\", \" their\", \" unique\", \" personalities\", \" and\", \" skills\", \" to\", \" our\", \" hospital\", \".\", \" They\", \" range\", \" in\", \" age\", \" from\", \" 15\", \" to\", \" 99\", \".\", \" Our\", \" ranks\", \" are\", \" made\", \" up\", \" of\", \" men\", \" and\", \" women\", \";\", \" students\", \" and\", \" retire\", \"es\", \";\", \" homem\", \"akers\", \" and\", \" business\", \" people\", \".\", \" Last\", \" year\", \",\", \" 334\", \" volunteers\", \" contributed\", \" over\", \" 36\", \",\", \"000\", \" hours\", \" to\", \" our\", \" hospitals\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\"], [\"j\", \"OO\", \"Q\", \" on\", \" The\", \" OR\", \"M\", \" Foundation\", \"?\", \"\\n\", \"\\n\", \"I\", \" am\", \" the\", \" developer\", \" of\", \" j\", \"OO\", \"Q\", \",\", \" a\", \" Java\", \" database\", \" abstraction\", \" framework\", \".\", \" I\", \" was\", \" wondering\", \" whether\", \" j\", \"OO\", \"Q\", \" might\", \" be\", \" an\", \" interesting\", \" tool\", \" for\", \" discussion\", \" on\", \" your\", \" website\", \",\", \" even\", \" if\", \" it\", \" is\", \" not\", \" exactly\", \" an\", \" OR\", \"M\", \" in\", \" the\", \" classic\", \" meaning\", \" (\", \"as\", \" in\", \" mapping\", \" objects\", \" to\", \" the\", \" relational\", \" world\", \" >\", \" OR\", \"M\", \").\", \" Instead\", \",\", \" j\", \"OO\", \"Q\", \" uses\", \" a\", \" reverse\", \" engineering\", \" paradigm\", \" (\", \"as\", \" in\", \" mapping\", \" relational\"], [\"Ti\", \"O\", \"2\", \" nanot\", \"ubes\", \" for\", \" bone\", \" regeneration\", \".\", \"\\n\", \"Nan\", \"ost\", \"ruct\", \"ured\", \" materials\", \" are\", \" believed\", \" to\", \" play\", \" a\", \" fundamental\", \" role\", \" in\", \" orth\", \"opedic\"], [\"j\", \"OO\", \"Q\", \" on\", \" The\", \" OR\", \"M\", \" Foundation\", \"?\", \"\\n\", \"\\n\", \"I\", \" am\", \" the\", \" developer\", \" of\", \" j\", \"OO\", \"Q\", \",\", \" a\", \" Java\", \" database\", \" abstraction\", \" framework\", \".\", \" I\", \" was\", \" wondering\", \" whether\", \" j\", \"OO\", \"Q\", \" might\", \" be\", \" an\", \" interesting\", \" tool\", \" for\", \" discussion\", \" on\", \" your\", \" website\", \",\", \" even\", \" if\", \" it\", \" is\", \" not\", \" exactly\", \" an\", \" OR\", \"M\", \" in\", \" the\", \" classic\", \" meaning\", \" (\", \"as\", \" in\", \" mapping\", \" objects\", \" to\", \" the\", \" relational\", \" world\", \" >\", \" OR\", \"M\", \").\", \" Instead\", \",\", \" j\", \"OO\", \"Q\", \" uses\", \" a\", \" reverse\", \" engineering\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174869060516357]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174869060516357]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38622045516967773]], [[0.16560834646224976]], [[0.0]], [[0.0]], [[0.020936429500579834]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256798028945923]], [[0.0]], [[0.0]], [[2.8025174140930176]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.43512850999832153]], [[0.0]], [[0.43192577362060547]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.429257333278656]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6920654773712158]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.43512850999832153]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.43512850999832153]], [[0.0]], [[0.43192577362060547]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.43512850999832153]], [[0.0]], [[0.43192577362060547]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.429257333278656]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174869060516357]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38622045516967773]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174869060516357]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38622045516967773]], [[0.16560834646224976]], [[0.0]], [[0.0]], [[0.020936429500579834]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256798028945923]], [[0.0]], [[0.0]], [[2.8025174140930176]], [[0.38558435440063477]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.43512850999832153]], [[0.0]], [[0.43192577362060547]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.429257333278656]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3814794421195984]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174869060516357]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38622045516967773]], [[0.16560834646224976]], [[0.0]], [[0.0]], [[0.020936429500579834]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34256798028945923]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19462883472442627]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.31664860248565674]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.43512850999832153]], [[0.0]], [[0.43192577362060547]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.429257333278656]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3814794421195984]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.27480703592300415]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.43512850999832153]], [[0.0]], [[0.43192577362060547]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.429257333278656]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3814794421195984]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.27480703592300415]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.09990262985229492]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2712511420249939]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174869060516357]]], [[[0.0]], [[0.2398119568824768]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.21766358613967896]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19462883472442627]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.21766358613967896]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17514169216156006]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016171216964722]], [[0.24174869060516357]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38622045516967773]], [[0.16560834646224976]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.13040536642074585]]], [[[0.0]], [[0.2398119568824768]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305054664611816]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.43512850999832153]], [[0.0]], [[0.43192577362060547]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.429257333278656]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520798683166504]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3814794421195984]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.27480703592300415]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.09990262985229492]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.504889965057373]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.7965087890625]], [[0.08860695362091064]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19462883472442627]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.31664860248565674]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.07574862241744995]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0816759467124939]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0815078616142273]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19462883472442627]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.31664860248565674]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.07574862241744995]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fc4ccf20f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpret features\n",
    "\n",
    "# change the following two lines to pick which feature to interpret\n",
    "component_idx = 9\n",
    "feat_idx = 31098\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[submodule]\n",
    "\n",
    "# interpret some features\n",
    "data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    d_submodule=512,\n",
    "    refresh_batch_size=128, # decrease to fit on smaller GPUs\n",
    "    n_ctxs=512, # decrease to fit on smaller GPUs\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    "    n_inputs=50 # decrease to fit on smaller GPUs\n",
    ")\n",
    "print(out.top_tokens)\n",
    "print(out.top_affected)\n",
    "out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting feats_to_ablate in a more useful format\n",
    "def n_hot(feats, dim=dictionary_size):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=DEVICE)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for ablating features\n",
    "is_tuple = {}\n",
    "with t.no_grad(), model.trace(\"_\"):\n",
    "    for submodule in submodules:\n",
    "        is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "def get_acts_ablated(\n",
    "    text,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate\n",
    "):\n",
    "\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            feat_idxs = to_ablate[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            res = x - x_hat\n",
    "            f[...,feat_idxs] = 0. # zero ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = dictionary.decode(f) + res\n",
    "            else:\n",
    "                submodule.output = dictionary.decode(f) + res\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after ablating features judged irrelevant by human annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/31 [00:35<17:46, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 0, Submodules [0]: Ambiguous: 0.9484, Ground Truth: 0.8566, Spurious: 0.5893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/31 [01:12<17:33, 36.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 1, Submodules [1]: Ambiguous: 0.9957, Ground Truth: 0.5922, Spurious: 0.9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3/31 [01:49<17:08, 36.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 2, Submodules [2]: Ambiguous: 0.9748, Ground Truth: 0.7615, Spurious: 0.7131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/31 [02:27<16:41, 37.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 3, Submodules [3]: Ambiguous: 0.9708, Ground Truth: 0.7972, Spurious: 0.6740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 5/31 [03:05<16:10, 37.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 4, Submodules [4]: Ambiguous: 0.9957, Ground Truth: 0.5922, Spurious: 0.9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/31 [03:42<15:36, 37.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 5, Submodules [5]: Ambiguous: 0.9954, Ground Truth: 0.6135, Spurious: 0.8796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/31 [04:20<15:02, 37.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 6, Submodules [6]: Ambiguous: 0.9617, Ground Truth: 0.7725, Spurious: 0.6861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 8/31 [04:58<14:28, 37.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 7, Submodules [7]: Ambiguous: 0.9940, Ground Truth: 0.6596, Spurious: 0.8289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 9/31 [05:36<13:53, 37.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 8, Submodules [8]: Ambiguous: 0.9957, Ground Truth: 0.5922, Spurious: 0.9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10/31 [06:14<13:16, 37.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 9, Submodules [9]: Ambiguous: 0.9746, Ground Truth: 0.7598, Spurious: 0.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 11/31 [06:52<12:38, 37.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 10, Submodules [10]: Ambiguous: 0.9942, Ground Truth: 0.7010, Spurious: 0.7886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 12/31 [07:30<11:58, 37.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 11, Submodules [11]: Ambiguous: 0.9957, Ground Truth: 0.5922, Spurious: 0.9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 13/31 [08:07<11:18, 37.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 12, Submodules [12]: Ambiguous: 0.9870, Ground Truth: 0.7500, Spurious: 0.7350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 14/31 [08:45<10:41, 37.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 13, Submodules [13]: Ambiguous: 0.9958, Ground Truth: 0.6169, Spurious: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 15/31 [09:23<10:03, 37.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 14, Submodules [14]: Ambiguous: 0.9957, Ground Truth: 0.5922, Spurious: 0.9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 16/31 [10:00<09:25, 37.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 15, Submodules [15]: Ambiguous: 0.9924, Ground Truth: 0.7402, Spurious: 0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 17/31 [10:52<09:45, 41.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 16, Submodules [0, 3]: Ambiguous: 0.9460, Ground Truth: 0.8577, Spurious: 0.5847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 18/31 [11:44<09:43, 44.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 17, Submodules [3, 6]: Ambiguous: 0.9685, Ground Truth: 0.7955, Spurious: 0.6711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 19/31 [12:35<09:22, 46.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 18, Submodules [6, 9]: Ambiguous: 0.9553, Ground Truth: 0.7863, Spurious: 0.6676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 20/31 [13:27<08:52, 48.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 19, Submodules [9, 12]: Ambiguous: 0.9531, Ground Truth: 0.7713, Spurious: 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 21/31 [14:19<08:14, 49.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 20, Submodules [12, 15]: Ambiguous: 0.9780, Ground Truth: 0.7926, Spurious: 0.6820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 22/31 [15:25<08:09, 54.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 21, Submodules [0, 3, 6]: Ambiguous: 0.9434, Ground Truth: 0.8635, Spurious: 0.5766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 23/31 [16:32<07:44, 58.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 22, Submodules [3, 6, 9]: Ambiguous: 0.9614, Ground Truth: 0.8088, Spurious: 0.6498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 24/31 [17:38<07:04, 60.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 23, Submodules [6, 9, 12]: Ambiguous: 0.9423, Ground Truth: 0.7961, Spurious: 0.6429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 25/31 [18:44<06:13, 62.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 24, Submodules [9, 12, 15]: Ambiguous: 0.9445, Ground Truth: 0.8041, Spurious: 0.6325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 26/31 [20:47<06:41, 80.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 25, Submodules [0, 1, 2, 3, 4, 5, 6]: Ambiguous: 0.9431, Ground Truth: 0.8635, Spurious: 0.5766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 27/31 [23:04<06:29, 97.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 26, Submodules [1, 2, 3, 4, 5, 6, 7, 8]: Ambiguous: 0.9628, Ground Truth: 0.8018, Spurious: 0.6613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 28/31 [27:01<06:58, 139.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 27, Submodules [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]: Ambiguous: 0.9289, Ground Truth: 0.8422, Spurious: 0.5818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 29/31 [30:31<05:20, 160.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 28, Submodules [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]: Ambiguous: 0.9333, Ground Truth: 0.8468, Spurious: 0.5818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 30/31 [33:17<02:42, 162.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 29, Submodules [6, 7, 8, 9, 10, 11, 12, 13, 14, 15]: Ambiguous: 0.9215, Ground Truth: 0.8318, Spurious: 0.5876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [35:20<00:00, 68.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation 30, Submodules [9, 10, 11, 12, 13, 14, 15]: Ambiguous: 0.9308, Ground Truth: 0.8116, Spurious: 0.6123\n",
      "Ablation complete. Results saved to 'ablation_results.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feats_to_ablate = {\n",
    "    submodules[0] : [\n",
    "        946, # 'his'\n",
    "        # 5719, # 'research'\n",
    "        7392, # 'He'\n",
    "        # 10784, # 'Nursing'\n",
    "        17846, # 'He'\n",
    "        22068, # 'His'\n",
    "        # 23079, # 'tastes'\n",
    "        # 25904, # 'nursing'\n",
    "        28533, # 'She'\n",
    "        29476, # 'he'\n",
    "        31461, # 'His'\n",
    "        31467, # 'she'\n",
    "        32081, # 'her'\n",
    "        32469, # 'She'\n",
    "    ],\n",
    "    submodules[1] : [\n",
    "        # 23752, # capitalized words, especially pronouns\n",
    "    ],\n",
    "    submodules[2] : [\n",
    "        2995, # 'he'\n",
    "        3842, # 'She'\n",
    "        10258, # female names\n",
    "        13387, # 'she'\n",
    "        13968, # 'He'\n",
    "        18382, # 'her'\n",
    "        19369, # 'His'\n",
    "        28127, # 'She'\n",
    "        30518, # 'He'\n",
    "    ],\n",
    "    submodules[3] : [\n",
    "        1022, # 'she'\n",
    "        9651, # female names\n",
    "        10060, # 'She'\n",
    "        18967, # 'He'\n",
    "        22084, # 'he'\n",
    "        23898, # 'His'\n",
    "        # 24799, # promotes surnames\n",
    "        26504, # 'her'\n",
    "        29626, # 'his'\n",
    "        # 31201, # 'nursing'\n",
    "    ],\n",
    "    submodules[4] : [\n",
    "        # 8147, # unclear, something with names\n",
    "    ],\n",
    "    submodules[5] : [\n",
    "        24159, # 'She', 'she'\n",
    "        25018, # female names\n",
    "    ],\n",
    "    submodules[6] : [\n",
    "        4592, # 'her'\n",
    "        8920, # 'he'\n",
    "        9877, # female names\n",
    "        12128, # 'his'\n",
    "        15017, # 'she'\n",
    "        # 17369, # contact info\n",
    "        # 26969, # related to nursing\n",
    "        30248, # female names\n",
    "    ],\n",
    "    submodules[7] : [\n",
    "        13570, # promotes male-related words\n",
    "        27472, # female names, promotes female-related words\n",
    "    ],\n",
    "    submodules[8] : [\n",
    "    ],\n",
    "    submodules[9] : [\n",
    "        1995, # promotes female-associated words\n",
    "        9128, # feminine pronouns\n",
    "        11656, # promotes male-associated words\n",
    "        12440, # promotes female-associated words\n",
    "        # 14638, # related to contact information?\n",
    "        29206, # gendered pronouns\n",
    "        29295, # female names\n",
    "        # 31098, # nursing-related words\n",
    "    ],\n",
    "    submodules[10] : [\n",
    "        2959, # promotes female-associated words\n",
    "        19128, # promotes male-associated words\n",
    "        22029, # promotes female-associated words\n",
    "    ],\n",
    "    submodules[11] : [\n",
    "    ],\n",
    "    submodules[12] : [\n",
    "        19558, # promotes female-associated words\n",
    "        23545, # 'she'\n",
    "        24806, # 'her'\n",
    "        27334, # promotes male-associated words\n",
    "        31453, # female names\n",
    "    ],\n",
    "    submodules[13] : [\n",
    "        31101, # promotes female-associated words\n",
    "    ],\n",
    "    submodules[14] : [\n",
    "    ],\n",
    "    submodules[15] : [\n",
    "        9766, # promotes female-associated words\n",
    "        12420, # promotes female pronouns\n",
    "        30220, # promotes male pronouns\n",
    "    ]\n",
    "}\n",
    "\n",
    "# print(f\"Number of features to ablate: {sum(len(v) for v in feats_to_ablate.values())}\")\n",
    "\n",
    "# feats_to_ablate = {\n",
    "#     submodule : n_hot(feats) for submodule, feats in feats_to_ablate.items()\n",
    "# }\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "# Flatten the feats_to_ablate dictionary\n",
    "# all_features = [(submodule, feats) for submodule, feats in feats_to_ablate.items() for feat in feats]\n",
    "\n",
    "# Generate ablation ranges\n",
    "ranges_to_ablate = []\n",
    "\n",
    "# Individual indices 0-15\n",
    "for i in range(16):\n",
    "    ranges_to_ablate.append([i])\n",
    "\n",
    "# Specific ranges\n",
    "specific_ranges = [[0,3], [3,6], [6,9], [9,12], [12,15], [0,3,6], [3,6,9], [6,9,12], [9,12,15]]\n",
    "ranges_to_ablate.extend(specific_ranges)\n",
    "\n",
    "# Continuous ranges'\n",
    "ranges_to_ablate.append(list(range(0, 7))) \n",
    "ranges_to_ablate.append(list(range(1, 9))) \n",
    "ranges_to_ablate.append(list(range(1, 16)))\n",
    "ranges_to_ablate.append(list(range(3, 16)))\n",
    "ranges_to_ablate.append(list(range(6, 16)))\n",
    "ranges_to_ablate.append(list(range(9, 16)))\n",
    "\n",
    "for range_index, range_to_ablate in enumerate(tqdm(ranges_to_ablate)):\n",
    "    # All elements in ranges_to_ablate are now lists, so no type checking is needed\n",
    "    submodules_to_ablate = [submodules[i] for i in range_to_ablate]\n",
    "    feats = [feat for i in range_to_ablate for feat in feats_to_ablate[submodules[i]]]\n",
    "    \n",
    "    # Create the ablation dictionary\n",
    "    ablation_dict = {submodule: n_hot(feats_to_ablate[submodule]) for submodule in submodules_to_ablate}\n",
    "    \n",
    "    # Define the get_acts_abl function for this ablation\n",
    "    get_acts_abl = lambda text: get_acts_ablated(text, model, submodules_to_ablate, dictionaries, ablation_dict)\n",
    "    \n",
    "    # Perform the tests\n",
    "    ambiguous_acc = test_probe(probe, get_acts_abl, label_idx=0)\n",
    "    \n",
    "    batches = get_data(train=False, ambiguous=False)\n",
    "    ground_truth_acc = test_probe(probe, get_acts_abl, batches=batches, label_idx=0)\n",
    "    spurious_acc = test_probe(probe, get_acts_abl, batches=batches, label_idx=1)\n",
    "    \n",
    "    # Save results\n",
    "    result = {\n",
    "        \"ablation_index\": range_index,\n",
    "        \"submodules\": range_to_ablate,\n",
    "        \"ambiguous_accuracy\": float(ambiguous_acc),\n",
    "        \"ground_truth_accuracy\": float(ground_truth_acc),\n",
    "        \"spurious_accuracy\": float(spurious_acc)\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    print(f\"Ablation {range_index}, Submodules {range_to_ablate}: Ambiguous: {ambiguous_acc:.4f}, Ground Truth: {ground_truth_acc:.4f}, Spurious: {spurious_acc:.4f}\")\n",
    "    \n",
    "    # Save to JSON after each iteration\n",
    "    with open('ablation_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Ablation complete. Results saved to 'ablation_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept bottleneck probing baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [    \n",
    "    ' nurse',\n",
    "    ' healthcare',\n",
    "    ' hospital',\n",
    "    ' patient',\n",
    "    ' medical',\n",
    "    ' clinic',\n",
    "    ' triage',\n",
    "    ' medication',\n",
    "    ' emergency',\n",
    "    ' surgery',\n",
    "    ' professor',\n",
    "    ' academia',\n",
    "    ' research',\n",
    "    ' university',\n",
    "    ' tenure',\n",
    "    ' faculty',\n",
    "    ' dissertation',\n",
    "    ' sabbatical',\n",
    "    ' publication',\n",
    "    ' grant',\n",
    "]\n",
    "# get concept vectors\n",
    "with t.no_grad(), model.trace(concepts):\n",
    "    concept_vectors = model.gpt_neox.layers[layer].output[0][:, -1, :].save()\n",
    "concept_vectors = concept_vectors.value - concept_vectors.value.mean(0, keepdim=True)\n",
    "\n",
    "def get_bottleneck(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        acts = model.gpt_neox.layers[layer].output[0]\n",
    "        acts = acts * attn_mask[:, :, None]\n",
    "        acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        # compute cosine similarity with concept vectors\n",
    "        sims = (acts @ concept_vectors.T) / (acts.norm(dim=-1)[:, None] @ concept_vectors.norm(dim=-1)[None])\n",
    "        sims = sims.save()\n",
    "    return sims.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp_probe, _ = train_probe(get_bottleneck, label_idx=0, dim=len(concepts))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subgroup accuracies\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline neuron performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get neurons which are most influential for giving gender label\n",
    "neuron_dicts = {\n",
    "    submodule : IdentityDict(activation_dim).to(DEVICE) for submodule in submodules\n",
    "}\n",
    "\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        neuron_dicts,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_to_ablate = {}\n",
    "total_neurons = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    neurons_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect.act > 0.2135).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        neurons_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_neurons += 1\n",
    "print(f\"total neurons: {total_neurons}\")\n",
    "\n",
    "neurons_to_ablate = {\n",
    "    submodule : n_hot([neuron_idx], dim=512) for submodule, neuron_idx in neurons_to_ablate.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_abl(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x[...,neurons_to_ablate[submodule]] = x.mean(dim=(0,1))[...,neurons_to_ablate[submodule]] # mean ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = x\n",
    "            else:\n",
    "                submodule.output = x\n",
    "\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline feature performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features which are most useful for predicting gender label\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats_to_ablate = {}\n",
    "total_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    top_feats_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect > 0.1107).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        top_feats_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_features += 1\n",
    "print(f\"total features: {total_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in top_feats_to_ablate.items()\n",
    "}\n",
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, top_feats_to_ablate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining probe on activations after ablating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "new_probe, _ = train_probe(get_acts_abl, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(new_probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
