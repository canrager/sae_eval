{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sfc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/sfc/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "\n",
    "\n",
    "from dictionary_learning.trainers.top_k import AutoEncoderTopK\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "activation_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "# data preparation hyperparameters\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "# To fit on 24GB VRAM GPU, I set the next 2 default batch_sizes to 64\n",
    "def get_data(train=True, ambiguous=True, batch_size=64, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=64, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "\n",
    "layer = 4 # model layer for attaching linear classification head\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to(DEVICE)\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1] \n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()\n",
    "    \n",
    "def get_acts(text):\n",
    "    with t.no_grad(): \n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.input[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous test accuracy 0.9247211813926697\n",
      "ground truth accuracy: 0.9285714030265808\n",
      "unintended feature accuracy: 0.489631325006485\n"
     ]
    }
   ],
   "source": [
    "oracle, _ = train_probe(get_acts, label_idx=0, batches=get_data(ambiguous=False))\n",
    "print(\"ambiguous test accuracy\", test_probe(oracle, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print(\"ground truth accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=0))\n",
    "print(\"unintended feature accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9679111242294312\n",
      "Accuracy for (0, 1): 0.9576417207717896\n",
      "Accuracy for (1, 0): 0.9032257795333862\n",
      "Accuracy for (1, 1): 0.884061336517334\n"
     ]
    }
   ],
   "source": [
    "# get worst-group accuracy of oracle probe\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(oracle, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9957016706466675\n",
      "Ground truth accuracy: 0.5921658873558044\n",
      "Unintended feature accuracy: 0.9020737409591675\n"
     ]
    }
   ],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9977167248725891\n",
      "Accuracy for (0, 1): 0.17649267613887787\n",
      "Accuracy for (1, 0): 0.22580644488334656\n",
      "Accuracy for (1, 1): 0.9941914677619934\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dictionaries\n",
    "\n",
    "# dictionary hyperparameters\n",
    "dict_id = 10\n",
    "expansion_factor = 64\n",
    "dictionary_size = expansion_factor * activation_dim\n",
    "\n",
    "submodules = []\n",
    "dictionaries = {}\n",
    "\n",
    "# submodules.append(model.gpt_neox.embed_in)\n",
    "# dictionaries[model.gpt_neox.embed_in] = AutoEncoder.from_pretrained(\n",
    "#     f'../dictionaries/pythia-70m-deduped/embed/{dict_id}_{dictionary_size}/ae.pt',\n",
    "#     device=DEVICE\n",
    "# )\n",
    "for i in range(3,5):\n",
    "    # submodules.append(model.gpt_neox.layers[i].attention)\n",
    "    # dictionaries[model.gpt_neox.layers[i].attention] = AutoEncoder.from_pretrained(\n",
    "    #     f'../dictionaries/pythia-70m-deduped/attn_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "    #     device=DEVICE\n",
    "    # )\n",
    "\n",
    "    # submodules.append(model.gpt_neox.layers[i].mlp)\n",
    "    # dictionaries[model.gpt_neox.layers[i].mlp] = AutoEncoder.from_pretrained(\n",
    "    #     f'../dictionaries/pythia-70m-deduped/mlp_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "    #     device=DEVICE\n",
    "    # )\n",
    "\n",
    "    # submodules.append(model.gpt_neox.layers[i])\n",
    "    # dictionaries[model.gpt_neox.layers[i]] = AutoEncoder.from_pretrained(\n",
    "    #     f'../dictionaries/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "    #     device=DEVICE\n",
    "    # )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i])\n",
    "    dictionaries[model.gpt_neox.layers[i]] = AutoEncoderTopK.from_pretrained(\n",
    "        f'../dictionaries/pythia70m_sweep_transpose_topk_ctx128_0730/resid_post_layer_{i}/trainer_10/ae.pt',\n",
    "        k=80,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.input[1]['attention_mask']\n",
    "    acts = model.gpt_neox.layers[layer].output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "    \n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# find most influential features\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_data(train=True, ambiguous=True, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "3616 0.18850840628147125\n",
      "3973 3.875211715698242\n",
      "6269 0.14719291031360626\n",
      "6401 0.21210166811943054\n",
      "9054 0.4044865071773529\n",
      "10444 0.14245520532131195\n",
      "11699 0.11233112961053848\n",
      "12378 0.13008801639080048\n",
      "15298 0.16910229623317719\n",
      "Component 1:\n",
      "822 0.8533684611320496\n",
      "1523 0.2153107076883316\n",
      "3973 3.708561658859253\n",
      "14260 0.23314306139945984\n",
      "total features: 13\n"
     ]
    }
   ],
   "source": [
    "n_features = 0\n",
    "\n",
    "feats_to_ablate = {\n",
    "    submodules[0] : [3616, 3973, 6269, 10444, 11699],\n",
    "    submodules[1] : [822],\n",
    "    # submodules[1] : [],\n",
    "}\n",
    "\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    for idx in (effect > 0.1).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        n_features += 1\n",
    "print(f\"total features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "Component 0:\n",
      "3616 0.18850840628147125\n",
      "-0.0394241064786911 \n",
      "\n",
      "3973 3.875211715698242\n",
      "0.6938527822494507 \n",
      "\n",
      "6269 0.14719291031360626\n",
      "0.003104033414274454 \n",
      "\n",
      "6401 0.21210166811943054\n",
      "-0.027395794168114662 \n",
      "\n",
      "9054 0.4044865071773529\n",
      "-0.6784382462501526 \n",
      "\n",
      "10444 0.14245520532131195\n",
      "0.06015932187438011 \n",
      "\n",
      "11699 0.11233112961053848\n",
      "0.13328564167022705 \n",
      "\n",
      "12378 0.13008801639080048\n",
      "-0.22796553373336792 \n",
      "\n",
      "15298 0.16910229623317719\n",
      "0.06867565214633942 \n",
      "\n",
      "Component 1:\n",
      "822 0.8533684611320496\n",
      "-0.4050905704498291 \n",
      "\n",
      "1523 0.2153107076883316\n",
      "0.12383705377578735 \n",
      "\n",
      "3973 3.708561658859253\n",
      "0.7013324499130249 \n",
      "\n",
      "14260 0.23314306139945984\n",
      "-0.6936933994293213 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "probe_vec = probe.net.weight.squeeze(0)\n",
    "print(probe_vec.shape)\n",
    "\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    for idx in (effect > 0.1).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        autoencoder = dictionaries[submodules[component_idx]]\n",
    "        decoder_vec = autoencoder.decoder.weight[:, idx].squeeze()\n",
    "\n",
    "        # print(decoder_vec.shape)\n",
    "\n",
    "        print(t.nn.functional.cosine_similarity(probe_vec, decoder_vec, dim=0).item(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret features\n",
    "\n",
    "# change the following two lines to pick which feature to interpret\n",
    "component_idx = 0\n",
    "feat_idx = 15298\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[submodule]\n",
    "\n",
    "# interpret some features\n",
    "data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    d_submodule=512,\n",
    "    refresh_batch_size=128, # decrease to fit on smaller GPUs\n",
    "    n_ctxs=512, # decrease to fit on smaller GPUs\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sfc/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' recurrent', 5.450101375579834), (' biliary', 5.447664260864258), (' progressive', 5.1983537673950195), (' cyst', 5.192357063293457), (' failure', 4.873469352722168), (' asymptomatic', 4.870468616485596), ('rost', 4.815242767333984), ('agulation', 4.792477130889893), (' psychiatric', 4.664707183837891), (' liver', 4.65831184387207), (' coronary', 4.646127223968506), (' case', 4.53131628036499), (' renal', 4.5067644119262695), ('heses', 4.49003791809082), (' pediatric', 4.488770961761475), (' tiny', 4.459965705871582), (' thrombosis', 4.388312339782715), (' anomalies', 4.371264457702637), (' Comprehensive', 4.350966453552246), ('rosis', 4.346313953399658), (' cysts', 4.315825462341309), (' lesions', 4.307556629180908), ('roidism', 4.261579513549805), (' limb', 4.241881370544434), (' artery', 4.240646839141846), (' mental', 4.219875335693359), (' refractory', 4.17466402053833), (' treated', 4.162898063659668), (' percutaneous', 4.09993314743042), (' frontal', 4.062190055847168)]\n",
      "[('��', 4.106856346130371), ('�', 3.396378755569458), ('�', 3.3759727478027344), ('�', 3.3618288040161133), ('�', 3.2218589782714844), ('�', 3.1579184532165527), ('�', 3.156909227371216), ('�', 3.1047608852386475), ('�', 3.085209369659424), ('�', 3.0734009742736816), ('�', 3.0436933040618896), ('�', 3.0329551696777344), ('�', 3.0148847103118896), ('�', 2.9242920875549316), ('�', 2.920214891433716), ('�', 2.828397750854492), ('�', 2.7577922344207764), ('�', 2.738008737564087), ('��', 2.7112388610839844), ('��', 2.707487106323242), ('��', 2.6897544860839844), ('�', 2.6239542961120605), ('�', 2.602217674255371), ('�', 2.5670084953308105), ('�', 2.566467523574829), ('�', 2.5215659141540527), ('��', 2.519169330596924), ('�', 2.4717326164245605), ('�', 2.461104393005371), ('�', 2.4332804679870605)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-d384c66f-eaad\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-d384c66f-eaad\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\", \"heses\", \":\", \" in\", \" vitro\", \" measurements\", \" and\", \" in\", \" vivo\", \" results\", \" in\", \" 30\", \" patients\", \".\", \"\\n\", \"Our\", \" goal\", \" was\", \" to\", \" evaluate\", \" the\", \" ability\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" show\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"Serial\", \" imaging\", \" and\", \" SW\", \"AN\", \" sequence\", \" of\", \" developmental\", \" venous\", \" anomaly\", \" thrombosis\", \" with\", \" hemat\", \"oma\", \":\", \" Diagnosis\", \" and\", \" follow\", \"-\", \"up\", \".\", \"\\n\", \"Development\", \"al\", \" venous\", \" anomalies\", \" (\", \"DV\", \"As\", \")\", \" are\", \" usually\", \" asymptomatic\", \".\", \" We\", \" report\", \" a\", \" case\", \" of\", \" D\", \"VA\", \" thrombosis\", \" with\", \" recurrent\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\", \" cyst\", \" development\", \" and\", \" resulting\", \" in\", \" end\", \" stage\", \" renal\", \" failure\", \" (\", \"ES\", \"RD\", \")\", \" in\", \" 50\", \"%\", \" of\", \" patients\", \" by\", \" 60\", \"y\", \".\", \" However\", \",\", \" there\", \" is\", \" considerable\", \" phenotypic\", \" variability\", \",\", \" extending\", \" from\", \" in\", \" ut\", \"ero\", \" onset\", \" to\", \" patients\", \" with\", \" adequate\", \" renal\", \" function\", \" into\", \" old\", \" age\", \".\", \" Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" liver\", \" disease\", \" (\", \"AD\", \"PL\", \"D\", \"),\", \" as\", \" traditionally\", \" defined\", \",\", \" results\", \" in\", \" P\", \"LD\", \" with\", \" minimal\", \" renal\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\", \" cyst\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\", \"mid\", \"wife\", \" (\", \"CN\", \"M\", \")\", \" in\", \" the\", \" primary\", \" care\", \" assessment\", \" of\", \",\", \" and\", \" appropriate\", \" referral\", \" for\", \" women\", \" with\", \" mental\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\", \" cyst\", \" development\", \" and\", \" resulting\", \" in\", \" end\", \" stage\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\", \" cyst\", \" development\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\", \" cyst\", \" development\", \" and\", \" resulting\", \" in\", \" end\", \" stage\", \" renal\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\", \" cyst\", \" development\", \" and\", \" resulting\", \" in\", \" end\", \" stage\", \" renal\", \" failure\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"Serial\", \" imaging\", \" and\", \" SW\", \"AN\", \" sequence\", \" of\", \" developmental\", \" venous\", \" anomaly\", \" thrombosis\", \" with\", \" hemat\", \"oma\", \":\", \" Diagnosis\", \" and\", \" follow\", \"-\", \"up\", \".\", \"\\n\", \"Development\", \"al\", \" venous\", \" anomalies\", \" (\", \"DV\", \"As\", \")\", \" are\", \" usually\", \" asymptomatic\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\", \"heses\"], [\"Introduction\", \" {#\", \"sec\", \"1\", \"-\", \"1\", \"}\", \"\\n\", \"============\", \"\\n\", \"\\n\", \"In\", \"flix\", \"imab\", \" (\", \"IF\", \"X\", \"),\", \" a\", \" chim\", \"eric\", \" anti\", \"-\", \"TNF\", \"\\u03b1\", \" antibody\", \",\", \" is\", \" effective\", \" in\", \" inducing\", \" and\", \" maintaining\", \" remission\", \" in\", \" a\", \" considerable\", \" proportion\", \" of\", \" IBD\", \" patients\", \" refractory\", \" to\", \" any\", \" other\", \" treatments\", \" \\\\[[@\", \"ref\", \"1\", \"],[@\", \"ref\", \"2\", \"]\\\\].\", \" However\", \",\", \" 8\", \"-\", \"12\", \"%\", \" of\", \" adult\", \" and\", \"/\", \"or\", \" pediatric\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"Serial\", \" imaging\", \" and\", \" SW\", \"AN\", \" sequence\", \" of\", \" developmental\", \" venous\", \" anomaly\", \" thrombosis\", \" with\", \" hemat\", \"oma\", \":\", \" Diagnosis\", \" and\", \" follow\", \"-\", \"up\", \".\", \"\\n\", \"Development\", \"al\", \" venous\", \" anomalies\", \" (\", \"DV\", \"As\", \")\", \" are\", \" usually\", \" asymptomatic\", \".\", \" We\", \" report\", \" a\", \" case\", \" of\", \" D\", \"VA\", \" thrombosis\", \" with\", \" recurrent\", \" tiny\", \" frontal\", \" hemat\", \"oma\", \" in\", \" a\", \" 24\", \"-\", \"year\", \"-\", \"old\", \" man\", \".\", \" The\", \" contribution\", \" of\", \" T\", \"2\", \"-\", \"GRE\", \" and\", \" SW\", \"AN\", \" sequences\", \" are\", \" discussed\", \".\", \" Follow\", \"-\", \"up\", \" att\", \"ested\", \" complete\", \" rec\", \"anal\", \"ization\", \" after\", \" antico\", \"agulation\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"Serial\", \" imaging\", \" and\", \" SW\", \"AN\", \" sequence\", \" of\", \" developmental\", \" venous\", \" anomaly\", \" thrombosis\", \" with\", \" hemat\", \"oma\", \":\", \" Diagnosis\", \" and\", \" follow\", \"-\", \"up\", \".\", \"\\n\", \"Development\", \"al\", \" venous\", \" anomalies\", \" (\", \"DV\", \"As\", \")\", \" are\", \" usually\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\", \"heses\", \":\", \" in\", \" vitro\", \" measurements\", \" and\", \" in\", \" vivo\", \" results\", \" in\", \" 30\", \" patients\", \".\", \"\\n\", \"Our\", \" goal\", \" was\", \" to\", \" evaluate\", \" the\", \" ability\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" show\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\", \"mid\", \"wife\", \" (\", \"CN\", \"M\", \")\", \" in\", \" the\", \" primary\", \" care\", \" assessment\", \" of\", \",\", \" and\", \" appropriate\", \" referral\", \" for\", \" women\", \" with\", \" mental\", \" health\", \" problems\", \",\", \" especially\", \" in\", \" cases\", \" of\", \" psychiatric\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\", \"heses\", \":\", \" in\", \" vitro\", \" measurements\", \" and\", \" in\", \" vivo\", \" results\", \" in\", \" 30\", \" patients\", \".\", \"\\n\", \"Our\", \" goal\", \" was\", \" to\", \" evaluate\", \" the\", \" ability\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" show\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\", \" cyst\", \" development\", \" and\", \" resulting\", \" in\", \" end\", \" stage\", \" renal\", \" failure\", \" (\", \"ES\", \"RD\", \")\", \" in\", \" 50\", \"%\", \" of\", \" patients\", \" by\", \" 60\", \"y\", \".\", \" However\", \",\", \" there\", \" is\", \" considerable\", \" phenotypic\", \" variability\", \",\", \" extending\", \" from\", \" in\", \" ut\", \"ero\", \" onset\", \" to\", \" patients\", \" with\", \" adequate\", \" renal\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\", \"heses\", \":\", \" in\", \" vitro\", \" measurements\", \" and\", \" in\", \" vivo\", \" results\", \" in\", \" 30\", \" patients\", \".\", \"\\n\", \"Our\", \" goal\", \" was\", \" to\", \" evaluate\", \" the\", \" ability\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" show\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\", \"mid\", \"wife\", \" (\", \"CN\", \"M\", \")\", \" in\", \" the\", \" primary\", \" care\", \" assessment\", \" of\", \",\", \" and\", \" appropriate\", \" referral\", \" for\", \" women\", \" with\", \" mental\", \" health\", \" problems\", \",\", \" especially\", \" in\", \" cases\", \" of\", \" psychiatric\", \" emergencies\", \".\", \" Ess\", \"ential\", \" aspects\", \" of\", \" assessment\", \",\", \" diagnosis\", \",\", \" and\", \" treatment\", \" of\", \" the\", \" more\", \" common\", \" psychiatric\"], [\"Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" kidney\", \" disease\", \" (\", \"AD\", \"PK\", \"D\", \")\", \" is\", \" a\", \" common\", \" mono\", \"alle\", \"lic\", \" disorder\", \" associated\", \" with\", \" progressive\", \" cyst\", \" development\", \" and\", \" resulting\", \" in\", \" end\", \" stage\", \" renal\", \" failure\", \" (\", \"ES\", \"RD\", \")\", \" in\", \" 50\", \"%\", \" of\", \" patients\", \" by\", \" 60\", \"y\", \".\", \" However\", \",\", \" there\", \" is\", \" considerable\", \" phenotypic\", \" variability\", \",\", \" extending\", \" from\", \" in\", \" ut\", \"ero\", \" onset\", \" to\", \" patients\", \" with\", \" adequate\", \" renal\", \" function\", \" into\", \" old\", \" age\", \".\", \" Aut\", \"osomal\", \" dominant\", \" poly\", \"cy\", \"stic\", \" liver\"], [\"PCI\", \" Alternative\", \" Using\", \" S\", \"ust\", \"ained\", \" Exercise\", \" (\", \"PA\", \"USE\", \"):\", \" R\", \"ational\", \"e\", \" and\", \" trial\", \" design\", \".\", \"\\n\", \"Card\", \"i\", \"ovascular\", \" disease\", \" (\", \"C\", \"VD\", \")\", \" currently\", \" claims\", \" nearly\", \" one\", \" million\", \" lives\", \" yearly\", \" in\", \" the\", \" US\", \",\", \" accounting\", \" for\", \" nearly\", \" 40\", \"%\", \" of\", \" all\", \" deaths\", \".\", \" Coron\", \"ary\", \" artery\", \" disease\", \" (\", \"CAD\", \")\", \" accounts\", \" for\", \" the\", \" largest\", \" number\", \" of\", \" these\", \" deaths\", \".\", \" While\", \" efforts\", \" aimed\", \" at\", \" treating\", \" CAD\", \" in\", \" recent\", \" decades\", \" have\", \" concentrated\", \" on\", \" surgical\", \" and\", \" catheter\", \"-\", \"based\", \" interventions\", \",\", \" limited\", \" resources\", \" have\", \" been\", \" directed\", \" toward\", \" prevention\", \" and\", \" rehabilitation\", \".\", \" CAD\", \" is\", \" commonly\", \" treated\", \" using\", \" percutaneous\", \" coronary\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\", \"mid\", \"wife\", \" (\", \"CN\", \"M\", \")\", \" in\", \" the\", \" primary\", \" care\", \" assessment\", \" of\", \",\", \" and\", \" appropriate\", \" referral\", \" for\", \" women\", \" with\", \" mental\", \" health\", \" problems\", \",\", \" especially\", \" in\", \" cases\", \" of\", \" psychiatric\", \" emergencies\", \".\", \" Ess\", \"ential\", \" aspects\", \" of\", \" assessment\", \",\", \" diagnosis\", \",\", \" and\", \" treatment\", \" of\", \" the\", \" more\", \" common\", \" psychiatric\", \" problems\", \" are\", \" included\", \",\", \" and\", \" the\", \" treatment\", \" modalities\", \" that\", \" are\", \" considered\", \" when\", \" referral\", \" results\", \" in\", \" psychiatric\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\"]], \"activations\": [[[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]], [[5.408239364624023]], [[4.2204694747924805]], [[4.709869384765625]], [[4.9559855461120605]], [[4.860649585723877]], [[3.201185464859009]], [[2.9966752529144287]], [[1.6020538806915283]], [[2.546316385269165]], [[2.5983002185821533]], [[2.384272575378418]], [[2.2998616695404053]], [[2.2429864406585693]], [[2.828308343887329]], [[2.409018039703369]], [[4.193361759185791]], [[0.1809910386800766]], [[1.7697676420211792]], [[2.330406427383423]], [[0.7692694664001465]], [[1.1467987298965454]], [[1.1010538339614868]], [[2.2103965282440186]], [[2.4458863735198975]], [[0.8991338014602661]], [[1.5168973207473755]], [[3.3015785217285156]], [[4.274301528930664]], [[3.9434845447540283]], [[2.973876714706421]], [[2.2966930866241455]], [[2.0188632011413574]], [[3.373004198074341]], [[2.3583168983459473]], [[2.4943478107452393]], [[3.338210344314575]], [[2.5141451358795166]], [[3.1032986640930176]], [[4.740543365478516]], [[4.76818323135376]], [[5.48708963394165]]], [[[1.654991626739502]], [[1.6211023330688477]], [[1.5836236476898193]], [[1.5369893312454224]], [[1.560797095298767]], [[1.6642202138900757]], [[1.8066341876983643]], [[1.878203272819519]], [[1.8581879138946533]], [[1.8071911334991455]], [[1.7363415956497192]], [[1.7671635150909424]], [[1.8255680799484253]], [[1.8353934288024902]], [[1.8537964820861816]], [[1.8047138452529907]], [[1.685596227645874]], [[1.7454814910888672]], [[1.8786273002624512]], [[1.9724645614624023]], [[2.007915496826172]], [[1.961135745048523]], [[1.9052141904830933]], [[1.9903900623321533]], [[2.1181483268737793]], [[2.2766990661621094]], [[2.2448325157165527]], [[2.2086517810821533]], [[2.1583216190338135]], [[1.9535294771194458]], [[1.9124194383621216]], [[2.014965772628784]], [[1.9816055297851562]], [[1.9911842346191406]], [[1.9230884313583374]], [[1.807127833366394]], [[2.02919602394104]], [[2.221322536468506]], [[2.1891250610351562]], [[2.01784348487854]], [[1.8600742816925049]], [[1.8702386617660522]], [[2.0078024864196777]], [[2.0484790802001953]], [[0.0]], [[2.3457229137420654]], [[1.5449680089950562]], [[0.4069466292858124]], [[0.0]], [[0.0]], [[0.37942707538604736]], [[0.8524672389030457]], [[3.38166880607605]], [[3.2225842475891113]], [[4.295046806335449]], [[2.788184881210327]], [[3.373448133468628]], [[3.8329222202301025]], [[2.3303728103637695]], [[3.7966485023498535]], [[4.019662857055664]], [[4.053778648376465]], [[3.0910348892211914]], [[4.313021183013916]], [[0.0]], [[2.351816415786743]], [[2.8160080909729004]], [[3.4953956604003906]], [[4.430559158325195]], [[4.371264457702637]], [[2.003478527069092]], [[2.411344289779663]], [[1.9499045610427856]], [[3.4369559288024902]], [[3.645530939102173]], [[4.773699760437012]], [[4.870468616485596]], [[3.447312116622925]], [[3.6235878467559814]], [[4.10631799697876]], [[3.9515604972839355]], [[4.53131628036499]], [[4.119044303894043]], [[3.057497978210449]], [[3.8716678619384766]], [[4.481578350067139]], [[4.016504287719727]], [[5.450101375579834]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]], [[5.408239364624023]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]], [[5.192357063293457]], [[4.929948329925537]], [[4.046893119812012]], [[3.712813138961792]], [[4.154253005981445]], [[3.95293927192688]], [[5.0176615715026855]], [[4.921051502227783]], [[4.873469352722168]], [[2.280308246612549]], [[2.840231418609619]], [[3.214754343032837]], [[3.1507656574249268]], [[2.7994608879089355]], [[2.585636854171753]], [[2.3302619457244873]], [[2.6370058059692383]], [[4.494330406188965]], [[2.1816699504852295]], [[1.940346598625183]], [[1.7802177667617798]], [[0.28747132420539856]], [[2.001431465148926]], [[2.150895595550537]], [[2.307056188583374]], [[1.6079717874526978]], [[1.5924746990203857]], [[2.066718578338623]], [[1.6579359769821167]], [[1.770216703414917]], [[2.023242950439453]], [[2.081094264984131]], [[2.4069700241088867]], [[2.3798828125]], [[2.893505096435547]], [[3.2751200199127197]], [[2.752840280532837]], [[4.069777965545654]], [[3.895228147506714]], [[3.385930299758911]], [[4.703797340393066]], [[4.138007164001465]], [[2.8206491470336914]], [[2.901380777359009]], [[2.674833059310913]], [[2.379676103591919]], [[3.272508144378662]], [[3.0121474266052246]], [[2.19553542137146]], [[1.6827619075775146]], [[2.6190578937530518]], [[3.366537570953369]], [[4.65831184387207]], [[4.112795352935791]], [[1.63459050655365]], [[2.2630648612976074]], [[2.374645948410034]], [[2.659404754638672]], [[3.020780563354492]], [[2.2595434188842773]], [[1.9556940793991089]], [[3.186615228652954]], [[2.3355047702789307]], [[2.962529420852661]], [[3.3246262073516846]], [[2.334857225418091]], [[3.721635580062866]], [[2.904045343399048]], [[3.531677484512329]], [[5.293469429016113]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]], [[5.192357063293457]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]]], [[[1.5362088680267334]], [[3.6297149658203125]], [[2.9340341091156006]], [[3.2164878845214844]], [[0.0]], [[4.350966453552246]], [[3.078064441680908]], [[2.798365592956543]], [[3.7586326599121094]], [[3.64320969581604]], [[3.5392730236053467]], [[3.7932655811309814]], [[3.957193613052368]], [[3.3556699752807617]], [[2.7558751106262207]], [[2.1109936237335205]], [[2.676754951477051]], [[1.0634644031524658]], [[0.58790522813797]], [[1.530377745628357]], [[1.0191272497177124]], [[1.7230613231658936]], [[1.837022066116333]], [[1.6607627868652344]], [[1.3807621002197266]], [[1.7745903730392456]], [[1.7978761196136475]], [[2.271989583969116]], [[1.657340168952942]], [[1.4937243461608887]], [[2.4595000743865967]], [[1.2967416048049927]], [[1.4884778261184692]], [[1.1297435760498047]], [[1.8399372100830078]], [[1.576192021369934]], [[2.093252420425415]], [[3.7846264839172363]], [[3.8300275802612305]], [[2.3146870136260986]], [[2.265477418899536]], [[1.9693002700805664]], [[1.2389007806777954]], [[2.220510482788086]], [[3.448120594024658]], [[3.537182331085205]], [[3.210604190826416]], [[3.7533488273620605]], [[5.076484203338623]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]], [[5.192357063293457]], [[4.929948329925537]], [[4.046893119812012]], [[3.712813138961792]], [[4.154253005981445]], [[3.95293927192688]], [[5.0176615715026855]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]], [[5.408239364624023]], [[4.2204694747924805]], [[4.709869384765625]], [[4.9559855461120605]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]], [[5.192357063293457]], [[4.929948329925537]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]], [[5.192357063293457]], [[4.929948329925537]], [[4.046893119812012]], [[3.712813138961792]], [[4.154253005981445]], [[3.95293927192688]], [[5.0176615715026855]], [[4.921051502227783]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]], [[5.192357063293457]], [[4.929948329925537]], [[4.046893119812012]], [[3.712813138961792]], [[4.154253005981445]], [[3.95293927192688]], [[5.0176615715026855]], [[4.921051502227783]], [[4.873469352722168]]], [[[1.654991626739502]], [[1.6211023330688477]], [[1.5836236476898193]], [[1.5369893312454224]], [[1.560797095298767]], [[1.6642202138900757]], [[1.8066341876983643]], [[1.878203272819519]], [[1.8581879138946533]], [[1.8071911334991455]], [[1.7363415956497192]], [[1.7671635150909424]], [[1.8255680799484253]], [[1.8353934288024902]], [[1.8537964820861816]], [[1.8047138452529907]], [[1.685596227645874]], [[1.7454814910888672]], [[1.8786273002624512]], [[1.9724645614624023]], [[2.007915496826172]], [[1.961135745048523]], [[1.9052141904830933]], [[1.9903900623321533]], [[2.1181483268737793]], [[2.2766990661621094]], [[2.2448325157165527]], [[2.2086517810821533]], [[2.1583216190338135]], [[1.9535294771194458]], [[1.9124194383621216]], [[2.014965772628784]], [[1.9816055297851562]], [[1.9911842346191406]], [[1.9230884313583374]], [[1.807127833366394]], [[2.02919602394104]], [[2.221322536468506]], [[2.1891250610351562]], [[2.01784348487854]], [[1.8600742816925049]], [[1.8702386617660522]], [[2.0078024864196777]], [[2.0484790802001953]], [[0.0]], [[2.3457229137420654]], [[1.5449680089950562]], [[0.4069466292858124]], [[0.0]], [[0.0]], [[0.37942707538604736]], [[0.8524672389030457]], [[3.38166880607605]], [[3.2225842475891113]], [[4.295046806335449]], [[2.788184881210327]], [[3.373448133468628]], [[3.8329222202301025]], [[2.3303728103637695]], [[3.7966485023498535]], [[4.019662857055664]], [[4.053778648376465]], [[3.0910348892211914]], [[4.313021183013916]], [[0.0]], [[2.351816415786743]], [[2.8160080909729004]], [[3.4953956604003906]], [[4.430559158325195]], [[4.371264457702637]], [[2.003478527069092]], [[2.411344289779663]], [[1.9499045610427856]], [[3.4369559288024902]], [[3.645530939102173]], [[4.773699760437012]], [[4.870468616485596]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]], [[5.408239364624023]], [[4.2204694747924805]], [[4.709869384765625]], [[4.9559855461120605]], [[4.860649585723877]]], [[[1.0291895866394043]], [[0.6563169956207275]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5903139710426331]], [[1.2007222175598145]], [[1.227205753326416]], [[0.647400975227356]], [[2.9649970531463623]], [[1.1431918144226074]], [[1.6462867259979248]], [[1.49726402759552]], [[1.584434986114502]], [[1.6698707342147827]], [[1.9534112215042114]], [[0.8177840709686279]], [[2.1336920261383057]], [[1.7950570583343506]], [[2.676931619644165]], [[2.083230495452881]], [[2.123924493789673]], [[1.3537875413894653]], [[1.2323637008666992]], [[1.8687005043029785]], [[2.487287759780884]], [[2.389697790145874]], [[1.167357087135315]], [[2.6605520248413086]], [[3.565145254135132]], [[3.6058573722839355]], [[2.366992950439453]], [[2.1474194526672363]], [[1.3567259311676025]], [[2.0849215984344482]], [[3.8006176948547363]], [[3.5186307430267334]], [[4.17466402053833]], [[2.702638626098633]], [[2.704390287399292]], [[2.8215861320495605]], [[2.911206007003784]], [[0.9773883819580078]], [[0.0]], [[0.0]], [[0.0]], [[0.39192914962768555]], [[0.4225236177444458]], [[1.7786803245544434]], [[1.405376672744751]], [[1.2384922504425049]], [[0.845748245716095]], [[0.831209659576416]], [[0.8393502831459045]], [[1.4450054168701172]], [[1.6918377876281738]], [[2.7920868396759033]], [[2.1933209896087646]], [[1.4357733726501465]], [[2.413353681564331]], [[4.835414409637451]]], [[[1.654991626739502]], [[1.6211023330688477]], [[1.5836236476898193]], [[1.5369893312454224]], [[1.560797095298767]], [[1.6642202138900757]], [[1.8066341876983643]], [[1.878203272819519]], [[1.8581879138946533]], [[1.8071911334991455]], [[1.7363415956497192]], [[1.7671635150909424]], [[1.8255680799484253]], [[1.8353934288024902]], [[1.8537964820861816]], [[1.8047138452529907]], [[1.685596227645874]], [[1.7454814910888672]], [[1.8786273002624512]], [[1.9724645614624023]], [[2.007915496826172]], [[1.961135745048523]], [[1.9052141904830933]], [[1.9903900623321533]], [[2.1181483268737793]], [[2.2766990661621094]], [[2.2448325157165527]], [[2.2086517810821533]], [[2.1583216190338135]], [[1.9535294771194458]], [[1.9124194383621216]], [[2.014965772628784]], [[1.9816055297851562]], [[1.9911842346191406]], [[1.9230884313583374]], [[1.807127833366394]], [[2.02919602394104]], [[2.221322536468506]], [[2.1891250610351562]], [[2.01784348487854]], [[1.8600742816925049]], [[1.8702386617660522]], [[2.0078024864196777]], [[2.0484790802001953]], [[0.0]], [[2.3457229137420654]], [[1.5449680089950562]], [[0.4069466292858124]], [[0.0]], [[0.0]], [[0.37942707538604736]], [[0.8524672389030457]], [[3.38166880607605]], [[3.2225842475891113]], [[4.295046806335449]], [[2.788184881210327]], [[3.373448133468628]], [[3.8329222202301025]], [[2.3303728103637695]], [[3.7966485023498535]], [[4.019662857055664]], [[4.053778648376465]], [[3.0910348892211914]], [[4.313021183013916]], [[0.0]], [[2.351816415786743]], [[2.8160080909729004]], [[3.4953956604003906]], [[4.430559158325195]], [[4.371264457702637]], [[2.003478527069092]], [[2.411344289779663]], [[1.9499045610427856]], [[3.4369559288024902]], [[3.645530939102173]], [[4.773699760437012]], [[4.870468616485596]], [[3.447312116622925]], [[3.6235878467559814]], [[4.10631799697876]], [[3.9515604972839355]], [[4.53131628036499]], [[4.119044303894043]], [[3.057497978210449]], [[3.8716678619384766]], [[4.481578350067139]], [[4.016504287719727]], [[5.450101375579834]], [[4.459965705871582]], [[4.062190055847168]], [[4.241410255432129]], [[3.865691900253296]], [[2.758078098297119]], [[2.930183172225952]], [[2.2628302574157715]], [[2.3459858894348145]], [[3.367086410522461]], [[2.7169651985168457]], [[3.416656017303467]], [[3.9532828330993652]], [[3.378309488296509]], [[3.9642186164855957]], [[1.920994520187378]], [[2.3940720558166504]], [[2.672240972518921]], [[3.260341167449951]], [[2.7152304649353027]], [[0.8120164275169373]], [[1.8851991891860962]], [[2.162062406539917]], [[1.127063512802124]], [[1.4277567863464355]], [[1.8773807287216187]], [[1.8033119440078735]], [[2.551504135131836]], [[3.5838661193847656]], [[2.5354814529418945]], [[3.4093921184539795]], [[1.2261772155761719]], [[3.286789655685425]], [[4.2103753089904785]], [[3.63104510307312]], [[3.0705337524414062]], [[3.839939832687378]], [[3.98771071434021]], [[3.79609751701355]], [[4.792477130889893]]], [[[1.654991626739502]], [[1.6211023330688477]], [[1.5836236476898193]], [[1.5369893312454224]], [[1.560797095298767]], [[1.6642202138900757]], [[1.8066341876983643]], [[1.878203272819519]], [[1.8581879138946533]], [[1.8071911334991455]], [[1.7363415956497192]], [[1.7671635150909424]], [[1.8255680799484253]], [[1.8353934288024902]], [[1.8537964820861816]], [[1.8047138452529907]], [[1.685596227645874]], [[1.7454814910888672]], [[1.8786273002624512]], [[1.9724645614624023]], [[2.007915496826172]], [[1.961135745048523]], [[1.9052141904830933]], [[1.9903900623321533]], [[2.1181483268737793]], [[2.2766990661621094]], [[2.2448325157165527]], [[2.2086517810821533]], [[2.1583216190338135]], [[1.9535294771194458]], [[1.9124194383621216]], [[2.014965772628784]], [[1.9816055297851562]], [[1.9911842346191406]], [[1.9230884313583374]], [[1.807127833366394]], [[2.02919602394104]], [[2.221322536468506]], [[2.1891250610351562]], [[2.01784348487854]], [[1.8600742816925049]], [[1.8702386617660522]], [[2.0078024864196777]], [[2.0484790802001953]], [[0.0]], [[2.3457229137420654]], [[1.5449680089950562]], [[0.4069466292858124]], [[0.0]], [[0.0]], [[0.37942707538604736]], [[0.8524672389030457]], [[3.38166880607605]], [[3.2225842475891113]], [[4.295046806335449]], [[2.788184881210327]], [[3.373448133468628]], [[3.8329222202301025]], [[2.3303728103637695]], [[3.7966485023498535]], [[4.019662857055664]], [[4.053778648376465]], [[3.0910348892211914]], [[4.313021183013916]], [[0.0]], [[2.351816415786743]], [[2.8160080909729004]], [[3.4953956604003906]], [[4.430559158325195]], [[4.371264457702637]], [[2.003478527069092]], [[2.411344289779663]], [[1.9499045610427856]], [[3.4369559288024902]], [[3.645530939102173]], [[4.773699760437012]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]], [[5.408239364624023]], [[4.2204694747924805]], [[4.709869384765625]], [[4.9559855461120605]], [[4.860649585723877]], [[3.201185464859009]], [[2.9966752529144287]], [[1.6020538806915283]], [[2.546316385269165]], [[2.5983002185821533]], [[2.384272575378418]], [[2.2998616695404053]], [[2.2429864406585693]], [[2.828308343887329]], [[2.409018039703369]], [[4.193361759185791]], [[0.1809910386800766]], [[1.7697676420211792]], [[2.330406427383423]], [[0.7692694664001465]], [[1.1467987298965454]], [[1.1010538339614868]], [[2.2103965282440186]], [[2.4458863735198975]], [[0.8991338014602661]], [[1.5168973207473755]], [[3.3015785217285156]], [[4.274301528930664]], [[3.9434845447540283]], [[2.973876714706421]], [[2.2966930866241455]], [[2.0188632011413574]], [[3.373004198074341]], [[2.3583168983459473]], [[2.4943478107452393]], [[3.338210344314575]], [[2.5141451358795166]], [[3.1032986640930176]], [[4.740543365478516]], [[4.76818323135376]]], [[[1.5362088680267334]], [[3.6297149658203125]], [[2.9340341091156006]], [[3.2164878845214844]], [[0.0]], [[4.350966453552246]], [[3.078064441680908]], [[2.798365592956543]], [[3.7586326599121094]], [[3.64320969581604]], [[3.5392730236053467]], [[3.7932655811309814]], [[3.957193613052368]], [[3.3556699752807617]], [[2.7558751106262207]], [[2.1109936237335205]], [[2.676754951477051]], [[1.0634644031524658]], [[0.58790522813797]], [[1.530377745628357]], [[1.0191272497177124]], [[1.7230613231658936]], [[1.837022066116333]], [[1.6607627868652344]], [[1.3807621002197266]], [[1.7745903730392456]], [[1.7978761196136475]], [[2.271989583969116]], [[1.657340168952942]], [[1.4937243461608887]], [[2.4595000743865967]], [[1.2967416048049927]], [[1.4884778261184692]], [[1.1297435760498047]], [[1.8399372100830078]], [[1.576192021369934]], [[2.093252420425415]], [[3.7846264839172363]], [[3.8300275802612305]], [[2.3146870136260986]], [[2.265477418899536]], [[1.9693002700805664]], [[1.2389007806777954]], [[2.220510482788086]], [[3.448120594024658]], [[3.537182331085205]], [[3.210604190826416]], [[3.7533488273620605]], [[5.076484203338623]], [[4.5252366065979]], [[3.303556203842163]], [[2.4847376346588135]], [[2.7192680835723877]], [[1.9765145778656006]], [[3.032259225845337]], [[2.9938509464263916]], [[4.766262531280518]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]], [[5.408239364624023]], [[4.2204694747924805]], [[4.709869384765625]], [[4.9559855461120605]], [[4.860649585723877]], [[3.201185464859009]], [[2.9966752529144287]], [[1.6020538806915283]], [[2.546316385269165]], [[2.5983002185821533]], [[2.384272575378418]], [[2.2998616695404053]], [[2.2429864406585693]], [[2.828308343887329]], [[2.409018039703369]], [[4.193361759185791]], [[0.1809910386800766]], [[1.7697676420211792]], [[2.330406427383423]], [[0.7692694664001465]], [[1.1467987298965454]], [[1.1010538339614868]], [[2.2103965282440186]], [[2.4458863735198975]], [[0.8991338014602661]], [[1.5168973207473755]], [[3.3015785217285156]], [[4.274301528930664]], [[3.9434845447540283]], [[2.973876714706421]], [[2.2966930866241455]], [[2.0188632011413574]], [[3.373004198074341]], [[2.3583168983459473]], [[2.4943478107452393]], [[3.338210344314575]], [[2.5141451358795166]], [[3.1032986640930176]], [[4.740543365478516]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]], [[5.408239364624023]], [[4.2204694747924805]], [[4.709869384765625]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]], [[5.192357063293457]], [[4.929948329925537]], [[4.046893119812012]], [[3.712813138961792]], [[4.154253005981445]], [[3.95293927192688]], [[5.0176615715026855]], [[4.921051502227783]], [[4.873469352722168]], [[2.280308246612549]], [[2.840231418609619]], [[3.214754343032837]], [[3.1507656574249268]], [[2.7994608879089355]], [[2.585636854171753]], [[2.3302619457244873]], [[2.6370058059692383]], [[4.494330406188965]], [[2.1816699504852295]], [[1.940346598625183]], [[1.7802177667617798]], [[0.28747132420539856]], [[2.001431465148926]], [[2.150895595550537]], [[2.307056188583374]], [[1.6079717874526978]], [[1.5924746990203857]], [[2.066718578338623]], [[1.6579359769821167]], [[1.770216703414917]], [[2.023242950439453]], [[2.081094264984131]], [[2.4069700241088867]], [[2.3798828125]], [[2.893505096435547]], [[3.2751200199127197]], [[2.752840280532837]], [[4.069777965545654]], [[3.895228147506714]], [[3.385930299758911]], [[4.703797340393066]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]], [[2.8980488777160645]], [[2.466041326522827]], [[4.0297017097473145]], [[2.857694387435913]], [[2.7894487380981445]], [[5.782786846160889]], [[5.190469741821289]], [[5.408239364624023]], [[4.2204694747924805]], [[4.709869384765625]], [[4.9559855461120605]], [[4.860649585723877]], [[3.201185464859009]], [[2.9966752529144287]], [[1.6020538806915283]], [[2.546316385269165]], [[2.5983002185821533]], [[2.384272575378418]], [[2.2998616695404053]], [[2.2429864406585693]], [[2.828308343887329]], [[2.409018039703369]], [[4.193361759185791]], [[0.1809910386800766]], [[1.7697676420211792]], [[2.330406427383423]], [[0.7692694664001465]], [[1.1467987298965454]], [[1.1010538339614868]], [[2.2103965282440186]], [[2.4458863735198975]], [[0.8991338014602661]], [[1.5168973207473755]], [[3.3015785217285156]], [[4.274301528930664]], [[3.9434845447540283]], [[2.973876714706421]], [[2.2966930866241455]], [[2.0188632011413574]], [[3.373004198074341]], [[2.3583168983459473]], [[2.4943478107452393]], [[3.338210344314575]], [[2.5141451358795166]], [[3.1032986640930176]], [[4.740543365478516]], [[4.76818323135376]], [[5.48708963394165]], [[4.450486183166504]], [[4.330029010772705]], [[4.67449951171875]]], [[[1.5362088680267334]], [[3.6297149658203125]], [[2.9340341091156006]], [[3.2164878845214844]], [[0.0]], [[4.350966453552246]], [[3.078064441680908]], [[2.798365592956543]], [[3.7586326599121094]], [[3.64320969581604]], [[3.5392730236053467]], [[3.7932655811309814]], [[3.957193613052368]], [[3.3556699752807617]], [[2.7558751106262207]], [[2.1109936237335205]], [[2.676754951477051]], [[1.0634644031524658]], [[0.58790522813797]], [[1.530377745628357]], [[1.0191272497177124]], [[1.7230613231658936]], [[1.837022066116333]], [[1.6607627868652344]], [[1.3807621002197266]], [[1.7745903730392456]], [[1.7978761196136475]], [[2.271989583969116]], [[1.657340168952942]], [[1.4937243461608887]], [[2.4595000743865967]], [[1.2967416048049927]], [[1.4884778261184692]], [[1.1297435760498047]], [[1.8399372100830078]], [[1.576192021369934]], [[2.093252420425415]], [[3.7846264839172363]], [[3.8300275802612305]], [[2.3146870136260986]], [[2.265477418899536]], [[1.9693002700805664]], [[1.2389007806777954]], [[2.220510482788086]], [[3.448120594024658]], [[3.537182331085205]], [[3.210604190826416]], [[3.7533488273620605]], [[5.076484203338623]], [[4.5252366065979]], [[3.303556203842163]], [[2.4847376346588135]], [[2.7192680835723877]], [[1.9765145778656006]], [[3.032259225845337]], [[2.9938509464263916]], [[4.766262531280518]], [[3.6517930030822754]], [[1.7156579494476318]], [[1.1034973859786987]], [[1.8391627073287964]], [[1.5267976522445679]], [[1.6120632886886597]], [[2.4394760131835938]], [[1.682921290397644]], [[2.952302932739258]], [[2.7249436378479004]], [[2.8658950328826904]], [[2.9852917194366455]], [[2.906881809234619]], [[2.7864561080932617]], [[1.6765276193618774]], [[3.7321701049804688]], [[4.674220561981201]]], [[[0.0]], [[1.3069499731063843]], [[0.6703327894210815]], [[0.5227311253547668]], [[1.0338159799575806]], [[2.137460947036743]], [[3.1416168212890625]], [[4.349970817565918]], [[1.6232765913009644]], [[1.766451358795166]], [[1.5727572441101074]], [[1.12847900390625]], [[2.629944086074829]], [[2.44765567779541]], [[2.1533265113830566]], [[3.9156298637390137]], [[1.7006964683532715]], [[1.9920673370361328]], [[1.4886547327041626]], [[3.474630117416382]], [[4.049614906311035]], [[3.3983240127563477]], [[5.1983537673950195]], [[5.192357063293457]], [[4.929948329925537]], [[4.046893119812012]], [[3.712813138961792]], [[4.154253005981445]], [[3.95293927192688]], [[5.0176615715026855]], [[4.921051502227783]], [[4.873469352722168]], [[2.280308246612549]], [[2.840231418609619]], [[3.214754343032837]], [[3.1507656574249268]], [[2.7994608879089355]], [[2.585636854171753]], [[2.3302619457244873]], [[2.6370058059692383]], [[4.494330406188965]], [[2.1816699504852295]], [[1.940346598625183]], [[1.7802177667617798]], [[0.28747132420539856]], [[2.001431465148926]], [[2.150895595550537]], [[2.307056188583374]], [[1.6079717874526978]], [[1.5924746990203857]], [[2.066718578338623]], [[1.6579359769821167]], [[1.770216703414917]], [[2.023242950439453]], [[2.081094264984131]], [[2.4069700241088867]], [[2.3798828125]], [[2.893505096435547]], [[3.2751200199127197]], [[2.752840280532837]], [[4.069777965545654]], [[3.895228147506714]], [[3.385930299758911]], [[4.703797340393066]], [[4.138007164001465]], [[2.8206491470336914]], [[2.901380777359009]], [[2.674833059310913]], [[2.379676103591919]], [[3.272508144378662]], [[3.0121474266052246]], [[2.19553542137146]], [[1.6827619075775146]], [[2.6190578937530518]], [[3.366537570953369]], [[4.65831184387207]]], [[[1.1718367338180542]], [[0.9336207509040833]], [[0.7874543070793152]], [[0.5114582777023315]], [[0.40540221333503723]], [[0.7572274208068848]], [[2.223734140396118]], [[0.6309672594070435]], [[0.5402271747589111]], [[0.5929144620895386]], [[0.5601405501365662]], [[0.7684839963912964]], [[1.015339732170105]], [[0.3064730763435364]], [[0.35658231377601624]], [[0.4434208869934082]], [[0.25487691164016724]], [[0.0]], [[0.2715074419975281]], [[1.9709718227386475]], [[2.3233721256256104]], [[3.477863311767578]], [[3.840384006500244]], [[1.2196241617202759]], [[1.9916388988494873]], [[3.262476921081543]], [[2.7587759494781494]], [[2.6544225215911865]], [[1.781943678855896]], [[1.0852190256118774]], [[0.9675235748291016]], [[1.4269137382507324]], [[2.194990873336792]], [[1.620473861694336]], [[0.5876055359840393]], [[0.7909682989120483]], [[1.16664719581604]], [[0.499151349067688]], [[0.368525892496109]], [[0.642810046672821]], [[0.4931400418281555]], [[0.5429264903068542]], [[0.335404634475708]], [[0.8425754904747009]], [[1.4446284770965576]], [[2.586287260055542]], [[1.1567833423614502]], [[3.8931925296783447]], [[3.572380781173706]], [[4.240646839141846]], [[3.7933778762817383]], [[1.2410120964050293]], [[2.847745656967163]], [[3.025022506713867]], [[1.0839505195617676]], [[1.792413592338562]], [[1.3670271635055542]], [[1.4495084285736084]], [[0.9188582301139832]], [[2.0159707069396973]], [[1.4960408210754395]], [[2.8258700370788574]], [[1.7515873908996582]], [[0.855542004108429]], [[0.49174410104751587]], [[0.0]], [[0.0]], [[3.4206383228302]], [[3.342398166656494]], [[1.9126050472259521]], [[0.5345447063446045]], [[0.29964563250541687]], [[0.0]], [[0.44119536876678467]], [[1.0090519189834595]], [[3.9734485149383545]], [[2.4450929164886475]], [[3.8382537364959717]], [[1.6095342636108398]], [[2.278336524963379]], [[2.8681788444519043]], [[0.9824045896530151]], [[1.0704841613769531]], [[0.6034372448921204]], [[0.0]], [[0.0]], [[0.3666088879108429]], [[0.9316893815994263]], [[2.3319520950317383]], [[2.192415237426758]], [[2.9245474338531494]], [[1.2258923053741455]], [[2.5708892345428467]], [[1.6298160552978516]], [[2.691596031188965]], [[4.162898063659668]], [[2.2452383041381836]], [[4.09993314743042]], [[4.646127223968506]]], [[[1.5362088680267334]], [[3.6297149658203125]], [[2.9340341091156006]], [[3.2164878845214844]], [[0.0]], [[4.350966453552246]], [[3.078064441680908]], [[2.798365592956543]], [[3.7586326599121094]], [[3.64320969581604]], [[3.5392730236053467]], [[3.7932655811309814]], [[3.957193613052368]], [[3.3556699752807617]], [[2.7558751106262207]], [[2.1109936237335205]], [[2.676754951477051]], [[1.0634644031524658]], [[0.58790522813797]], [[1.530377745628357]], [[1.0191272497177124]], [[1.7230613231658936]], [[1.837022066116333]], [[1.6607627868652344]], [[1.3807621002197266]], [[1.7745903730392456]], [[1.7978761196136475]], [[2.271989583969116]], [[1.657340168952942]], [[1.4937243461608887]], [[2.4595000743865967]], [[1.2967416048049927]], [[1.4884778261184692]], [[1.1297435760498047]], [[1.8399372100830078]], [[1.576192021369934]], [[2.093252420425415]], [[3.7846264839172363]], [[3.8300275802612305]], [[2.3146870136260986]], [[2.265477418899536]], [[1.9693002700805664]], [[1.2389007806777954]], [[2.220510482788086]], [[3.448120594024658]], [[3.537182331085205]], [[3.210604190826416]], [[3.7533488273620605]], [[5.076484203338623]], [[4.5252366065979]], [[3.303556203842163]], [[2.4847376346588135]], [[2.7192680835723877]], [[1.9765145778656006]], [[3.032259225845337]], [[2.9938509464263916]], [[4.766262531280518]], [[3.6517930030822754]], [[1.7156579494476318]], [[1.1034973859786987]], [[1.8391627073287964]], [[1.5267976522445679]], [[1.6120632886886597]], [[2.4394760131835938]], [[1.682921290397644]], [[2.952302932739258]], [[2.7249436378479004]], [[2.8658950328826904]], [[2.9852917194366455]], [[2.906881809234619]], [[2.7864561080932617]], [[1.6765276193618774]], [[3.7321701049804688]], [[4.674220561981201]], [[3.1394848823547363]], [[1.3624781370162964]], [[1.0389213562011719]], [[1.0128388404846191]], [[1.3709113597869873]], [[1.4698578119277954]], [[2.5103700160980225]], [[2.3106026649475098]], [[1.0912747383117676]], [[1.2457257509231567]], [[1.5343457460403442]], [[1.5453020334243774]], [[3.5075533390045166]], [[2.149819850921631]], [[2.3027541637420654]], [[4.553637504577637]]], [[[0.0]], [[0.4303557872772217]], [[0.5526170134544373]], [[2.6927266120910645]], [[3.514474868774414]], [[3.1904044151306152]], [[3.6676993370056152]], [[2.3447072505950928]], [[1.8703163862228394]], [[4.549525737762451]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f10c031b9d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    "    n_inputs=128 # decrease to fit on smaller GPUs\n",
    ")\n",
    "print(out.top_tokens)\n",
    "print(out.top_affected)\n",
    "out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats_to_ablate = {\n",
    "#     submodules[0] : [\n",
    "#         946, # 'his'\n",
    "#         # 5719, # 'research'\n",
    "#         7392, # 'He'\n",
    "#         # 10784, # 'Nursing'\n",
    "#         17846, # 'He'\n",
    "#         22068, # 'His'\n",
    "#         # 23079, # 'tastes'\n",
    "#         # 25904, # 'nursing'\n",
    "#         28533, # 'She'\n",
    "#         29476, # 'he'\n",
    "#         31461, # 'His'\n",
    "#         31467, # 'she'\n",
    "#         32081, # 'her'\n",
    "#         32469, # 'She'\n",
    "#     ],\n",
    "#     submodules[1] : [\n",
    "#         # 23752, # capitalized words, especially pronouns\n",
    "#     ],\n",
    "#     submodules[2] : [\n",
    "#         2995, # 'he'\n",
    "#         3842, # 'She'\n",
    "#         10258, # female names\n",
    "#         13387, # 'she'\n",
    "#         13968, # 'He'\n",
    "#         18382, # 'her'\n",
    "#         19369, # 'His'\n",
    "#         28127, # 'She'\n",
    "#         30518, # 'He'\n",
    "#     ],\n",
    "#     submodules[3] : [\n",
    "#         1022, # 'she'\n",
    "#         9651, # female names\n",
    "#         10060, # 'She'\n",
    "#         18967, # 'He'\n",
    "#         22084, # 'he'\n",
    "#         23898, # 'His'\n",
    "#         # 24799, # promotes surnames\n",
    "#         26504, # 'her'\n",
    "#         29626, # 'his'\n",
    "#         # 31201, # 'nursing'\n",
    "#     ],\n",
    "#     submodules[4] : [\n",
    "#         # 8147, # unclear, something with names\n",
    "#     ],\n",
    "#     submodules[5] : [\n",
    "#         24159, # 'She', 'she'\n",
    "#         25018, # female names\n",
    "#     ],\n",
    "#     submodules[6] : [\n",
    "#         4592, # 'her'\n",
    "#         8920, # 'he'\n",
    "#         9877, # female names\n",
    "#         12128, # 'his'\n",
    "#         15017, # 'she'\n",
    "#         # 17369, # contact info\n",
    "#         # 26969, # related to nursing\n",
    "#         30248, # female names\n",
    "#     ],\n",
    "#     submodules[7] : [\n",
    "#         13570, # promotes male-related words\n",
    "#         27472, # female names, promotes female-related words\n",
    "#     ],\n",
    "#     submodules[8] : [\n",
    "#     ],\n",
    "#     submodules[9] : [\n",
    "#         1995, # promotes female-associated words\n",
    "#         9128, # feminine pronouns\n",
    "#         11656, # promotes male-associated words\n",
    "#         12440, # promotes female-associated words\n",
    "#         # 14638, # related to contact information?\n",
    "#         29206, # gendered pronouns\n",
    "#         29295, # female names\n",
    "#         # 31098, # nursing-related words\n",
    "#     ],\n",
    "#     submodules[10] : [\n",
    "#         2959, # promotes female-associated words\n",
    "#         19128, # promotes male-associated words\n",
    "#         22029, # promotes female-associated words\n",
    "#     ],\n",
    "#     submodules[11] : [\n",
    "#     ],\n",
    "#     submodules[12] : [\n",
    "#         19558, # promotes female-associated words\n",
    "#         23545, # 'she'\n",
    "#         24806, # 'her'\n",
    "#         27334, # promotes male-associated words\n",
    "#         31453, # female names\n",
    "#     ],\n",
    "#     submodules[13] : [\n",
    "#         31101, # promotes female-associated words\n",
    "#     ],\n",
    "#     submodules[14] : [\n",
    "#     ],\n",
    "#     submodules[15] : [\n",
    "#         9766, # promotes female-associated words\n",
    "#         12420, # promotes female pronouns\n",
    "#         30220, # promotes male pronouns\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# print(f\"Number of features to ablate: {sum(len(v) for v in feats_to_ablate.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting feats_to_ablate in a more useful format\n",
    "def n_hot(feats, dim=16384):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=DEVICE)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n",
    "\n",
    "feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in feats_to_ablate.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for ablating features\n",
    "is_tuple = {}\n",
    "with t.no_grad(), model.trace(\"_\"):\n",
    "    for submodule in submodules:\n",
    "        is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "def get_acts_ablated(\n",
    "    text,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate\n",
    "):\n",
    "\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            feat_idxs = to_ablate[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            res = x - x_hat\n",
    "            f[...,feat_idxs] = 0. # zero ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = dictionary.decode(f) + res\n",
    "            else:\n",
    "                submodule.output = dictionary.decode(f) + res\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after ablating features judged irrelevant by human annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9703763723373413\n",
      "Ground truth accuracy: 0.8986175060272217\n",
      "Spurious accuracy: 0.5714285969734192\n"
     ]
    }
   ],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9869175553321838\n",
      "Accuracy for (0, 1): 0.8766052722930908\n",
      "Accuracy for (1, 0): 0.7764977216720581\n",
      "Accuracy for (1, 1): 0.9535315632820129\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept bottleneck probing baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [    \n",
    "    ' nurse',\n",
    "    ' healthcare',\n",
    "    ' hospital',\n",
    "    ' patient',\n",
    "    ' medical',\n",
    "    ' clinic',\n",
    "    ' triage',\n",
    "    ' medication',\n",
    "    ' emergency',\n",
    "    ' surgery',\n",
    "    ' professor',\n",
    "    ' academia',\n",
    "    ' research',\n",
    "    ' university',\n",
    "    ' tenure',\n",
    "    ' faculty',\n",
    "    ' dissertation',\n",
    "    ' sabbatical',\n",
    "    ' publication',\n",
    "    ' grant',\n",
    "]\n",
    "# get concept vectors\n",
    "with t.no_grad(), model.trace(concepts):\n",
    "    concept_vectors = model.gpt_neox.layers[layer].output[0][:, -1, :].save()\n",
    "concept_vectors = concept_vectors.value - concept_vectors.value.mean(0, keepdim=True)\n",
    "\n",
    "def get_bottleneck(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        acts = model.gpt_neox.layers[layer].output[0]\n",
    "        acts = acts * attn_mask[:, :, None]\n",
    "        acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        # compute cosine similarity with concept vectors\n",
    "        sims = (acts @ concept_vectors.T) / (acts.norm(dim=-1)[:, None] @ concept_vectors.norm(dim=-1)[None])\n",
    "        sims = sims.save()\n",
    "    return sims.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp_probe, _ = train_probe(get_bottleneck, label_idx=0, dim=len(concepts))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subgroup accuracies\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline neuron performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get neurons which are most influential for giving gender label\n",
    "neuron_dicts = {\n",
    "    submodule : IdentityDict(activation_dim).to(DEVICE) for submodule in submodules\n",
    "}\n",
    "\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        neuron_dicts,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_to_ablate = {}\n",
    "total_neurons = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    neurons_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect.act > 0.2135).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        neurons_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_neurons += 1\n",
    "print(f\"total neurons: {total_neurons}\")\n",
    "\n",
    "neurons_to_ablate = {\n",
    "    submodule : n_hot([neuron_idx], dim=512) for submodule, neuron_idx in neurons_to_ablate.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_abl(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x[...,neurons_to_ablate[submodule]] = x.mean(dim=(0,1))[...,neurons_to_ablate[submodule]] # mean ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = x\n",
    "            else:\n",
    "                submodule.output = x\n",
    "\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline feature performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features which are most useful for predicting gender label\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats_to_ablate = {}\n",
    "total_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    top_feats_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect > 0.1107).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        top_feats_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_features += 1\n",
    "print(f\"total features: {total_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in top_feats_to_ablate.items()\n",
    "}\n",
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, top_feats_to_ablate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining probe on activations after ablating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "new_probe, _ = train_probe(get_acts_abl, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(new_probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
