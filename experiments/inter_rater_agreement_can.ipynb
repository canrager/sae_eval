{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path: str) -> Dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def load_pkl(file_path: str) -> Dict:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "file1 = 'llm_autointerp/manual_labels_can_final.json'\n",
    "file2 = 'llm_autointerp/llm_results_can_final_sonnet.pkl'\n",
    "node_effects_filename = 'llm_autointerp/node_effects_autointerp_can_final.json'\n",
    "\n",
    "manual_file = load_json(file1)\n",
    "llm_file = load_pkl(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Example Prompts\n",
      " ['\\n\\n\\nExample 1: a variance to permit construction of the << dental>>(4) << office>>(1) by the Marion County Board of Zoning Appeals.\\nSubsequently on September 11, 1959, the Director of the Metropolitan Planning Department filed an affidavit to appeal said decision to the Metropolitan Board of Zoning Appeals, as authorized by statute.\\n\"53-969. Petitions for variance. \\x97 The city and county board of zoning appeals and the metropolitan board of zoning appeals are hereby authorized to grant height, bulk, area and use variances in the manner hereinafter set forth. Both city or county board of zoning appeals and the metropolitan board of zoning appeals may grant petitions for variance in their entirety or in\\n\\n\\n\\n\\nExample 2: eliminate << dental>>(4), << vision>>(1) and << pharmacy>>(0) benefits for low-income adults.\\n\\nThe Trump administration had argued Obama\\'s Medicaid expansion essentially created a new program under Medicaid. Allowing states to cover low-income adults with no children living at home changed the nature of the program, the administration asserted, and opened the way for provisions such as work requirements.\\n\\nThe national implications of Friday\\'s ruling could take a while to sort out. Officials in Arkansas have already implemented similar work requirement rules there.\\n\\nThe drive to expand Medicaid in GOP-led states had gotten a boost from the prospect of work requirements, which appeal to conservatives\\n\\n\\n\\n\\nExample 3: a picture of the patient\\\\\\'s facial profiles onto the soft tissue on the 3D-CT images. The asymmetry was improved in the front and lower areas, including the facial profile, following the surgery simulation.\\n\\nThe reduction of the << mandibular>>(2) width was measured, upon confirming the precise occlusal relationship after osteotomy, by optical scanning of the << dental>>(4) cast to link the data to the 3D-CT images.\\n\\nCant correction and yaw correction were implemented in the maxilla with a LeFort I osteotomy, and the right << molar>>(2) << tooth>>(4) was << impacted>>(0) in the upper area, resulting in complete down fracture, while Piezo\\n\\n\\n\\n\\nExample 4: An in vitro microleakage study of three restorative techniques for Class II restorations in posterior << teeth>>(4).\\nMicroleakage associated with a silver reinforced restorative glass ionomer cement used alone and also as a laminate restoration with a composite resin and << dent>>(2)ine adhesive in extracted premolar and << molar>>(2) << teeth>>(4) was evaluated. The influence of artificial saliva, thermal and load cycling was also determined. The composite and dentine adhesive alone were used for comparative purposes. The results showed that the composite resin/ <<dent>>(2)ine adhesive restorations showed substantial microleakage at both the cervical and occlusal margins of the Class II\\n\\n\\n\\n\\nExample 5: that the, \"Self-induced vomiting seen in both anorexia nervosa and bulimia nervosa can lead to swelling of << salivary>>(2) glands, electrolyte and mineral disturbances, and << dental>>(4) << enamel>>(3) erosion.\" In addition, \"rarer complications\" include \"tearing [of] the esophagus, rupturing of the stomach, and life-threatening irregularities of the heart rhythm.\"\\n\\nPossible causes\\nAccording to Cassell and Gleaves, \"biological, psychological, [and] social factors\" all play a role in the development of an eating << disorder>>(0). In their Introduction, they note that, \"In addition to core eating\\n\\n\\n\\n\\nExample 6: MCRCF was among the best prisons he had seen, but he expressed a concern that if as many as 800 men came to be housed there, \"it will become one of the more dangerous places to work and live in.\" Nevertheless, as previously noted, MCRCF, as well as BCRCF and LCRCF have thus far been among the least violent of TDOC\\'s adult institutions.\\n\\n5. MCRCF Health Care\\nMCRCF, a facility which opened in 1980, contains a small clinic with a two-table examination room, a << dental>>(4) room, a room with a bed for short-term use\\n\\n\\n\\n\\nExample 7: Punisher had become such an incredibly popular character, the Code had already become << tooth>>(4)less in many ways. The Punisher was one of the most violent comics on the stands, and the very basic premise of the character as anything other than - repeat after me - a villain (which is essentially what the character had been, a few gritty appearances in Marvel\\'s short-lived black & white magazines notwithstanding), sort of went against the grain of the entire Marvel Universe. Or at least the conception of Marvel as a publishing company whose entire mainline was theoretically fit for kids aged 8 to 80.\\n\\nSo despite the character\\'s unquest\\n\\n\\n\\n\\nExample 8: a first aperture spaced from a second aperture by a first distance; a second link member connected to the first link member and having a first toe spaced from a second toe by a second distance, the toes being adapted for engagement with << tooth>>(3) spaces associated with a sprocket, the << tooth>>(3) spaces being spaced apart by a third distance; and the second distance being substantially equal to the third distance, and the second distance being about one half of the first distance.\\nIn accordance with yet another aspect of the present invention, a chain system is disclosed. The chain system includes a first sprocket having a first plurality of << tooth>>(3) spaces spaced apart by a\\n\\n\\n\\n\\nExample 9: or incentives, were among the most approximately planted next survivors in both europe and north america. Enhanced stature and election, with local non-criminal crisis and significant harmful policy.\\n\\nThe mid-nineteenth difficulties are exposed for 48 songs under likely balloons with five works of the public records official county of san diego deh. In film to the scheme of preferential how to start a background check company on someone without them knowing, significantly administered camera << teeth>>(3) are well used when examining the culture. One public records database bexar county district clerk criminal claimed that he and his goods had back been informed of large built-in\\n\\n\\n\\n\\nExample 10: 40 F.3d 698, 712-13 (5th Cir.1994). The Court must consider all evidence in the light most favorable to the nonmoving party. See Id. at 713.\\nFacts\\nOn October 4, 2002, Olivas, while housed at the Mineral Wells unit, damaged two << teeth>>(3) when a locker lid fell, striking him in the << mouth>>(1). Olivas testified that he reported this to an officer, who told him to fill out a sick-call request, which Olivas completed the next day and submitted on October 6th. Olivas also maintains he told another officer, Chris Little,\\n\\n']\n",
      "\n",
      "\n",
      "##### Manual chain of thought\n",
      "The top promoted logits are related to tooth care.\n",
      "The activating inputs are coherently related to dentists.\n",
      "I will rate the dentist class as a 4.\n",
      "\n",
      "\n",
      "##### LLM chain of thought\n",
      "Step 1. The neuron appears to activate primarily on words related to breasts and breast cancer, with some activations on related body parts.\n",
      "\n",
      "Step 2. Scoring the concepts:\n",
      "\n",
      "Gender: This neuron shows a strong association with a female body part (breasts). While breasts are not exclusively female, they are strongly associated with female anatomy. Score: 3\n",
      "\n",
      "Professor: There is no apparent connection to professors. Score: 0\n",
      "\n",
      "Nurse: While breast cancer might be tangentially related to nursing, there's no direct connection evident. Score: 0\n",
      "\n",
      "Accountant: No connection to accountants is apparent. Score: 0\n",
      "\n",
      "Architect: No connection to architects is evident. Score: 0\n",
      "\n",
      "Attorney: No connection to attorneys is apparent. Score: 0\n",
      "\n",
      "Dentist: No connection to dentists is evident. Score: 0\n",
      "\n",
      "Filmmaker: While some examples mention visual descriptions, there's no clear connection to filmmaking. Score: 0\n",
      "\n",
      "```json\n",
      "{\"gender\": 3, \"professor\": 0, \"nurse\": 0, \"accountant\": 0, \"architect\": 0, \"attorney\": 0, \"dentist\": 0, \"filmmaker\": 0}\n",
      "```\n",
      "\n",
      "\n",
      "manual labels {'gender': 0, 'professor': 0, 'nurse': 0, 'accountant': 0, 'architect': 0, 'attorney': 0, 'dentist': 4, 'filmmaker': 0}\n",
      "LLM labels    {'gender': 3, 'professor': 0, 'nurse': 0, 'accountant': 0, 'architect': 0, 'attorney': 0, 'dentist': 0, 'filmmaker': 0}\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 1\n",
    "\n",
    "print(f'##### Example Prompts\\n {manual_file[str(sample_idx)]['example_prompts']}\\n\\n')\n",
    "print(f'##### Manual chain of thought\\n{manual_file[str(sample_idx)]['chain_of_thought']}\\n\\n')\n",
    "print(f'##### LLM chain of thought\\n{llm_file[sample_idx][0]}\\n\\n')\n",
    "print(f'manual labels {manual_file[str(sample_idx)]['per_class_scores']}')\n",
    "print(f'LLM labels    {llm_file[sample_idx][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 3, 3, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def extract_scores_manual(data: Dict, is_valid: List[bool]) -> Dict[str, List[int]]:\n",
    "    manual_labels = {}\n",
    "    for i, item in enumerate(data.values()):\n",
    "        if is_valid[i]:\n",
    "            for category, score in item['per_class_scores'].items():\n",
    "                if category not in manual_labels:\n",
    "                    manual_labels[category] = []\n",
    "                manual_labels[category].append(score)\n",
    "    return manual_labels\n",
    "\n",
    "def extract_scores_llm(data: List[Tuple[str, Dict[str, int], bool, str]]) -> Dict[str, List[int]]:\n",
    "    is_valid = []\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        scores = item[1]  # The scores dictionary is the second element of each tuple\n",
    "        if scores is None:\n",
    "            is_valid.append(False)\n",
    "        else:\n",
    "            is_valid.append(True)\n",
    "            for category, score in scores.items():\n",
    "                if category not in result:\n",
    "                    result[category] = []\n",
    "                result[category].append(score)\n",
    "    return result, is_valid\n",
    "\n",
    "\n",
    "llm_labels, is_valid = extract_scores_llm(llm_file)\n",
    "manual_labels = extract_scores_manual(manual_file, is_valid)\n",
    "\n",
    "print(llm_labels['gender'][:10])\n",
    "print(manual_labels['gender'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of manual labels: 53\n",
      "Length of LLM labels: 53\n",
      "Number of invalid valid scores: 1\n",
      "Cohen's Kappa scores for each category:\n",
      "gender: 0.0412\n",
      "professor: -0.0678\n",
      "nurse: -0.0236\n",
      "accountant: -0.0297\n",
      "architect: -0.0196\n",
      "attorney: -0.0612\n",
      "dentist: -0.0400\n",
      "filmmaker: -0.0452\n"
     ]
    }
   ],
   "source": [
    "def extract_scores_manual(data: Dict, is_valid: List[bool]) -> Dict[str, List[int]]:\n",
    "    manual_labels = {}\n",
    "    for i, item in enumerate(data.values()):\n",
    "        if is_valid[i]:\n",
    "            for category, score in item['per_class_scores'].items():\n",
    "                if category not in manual_labels:\n",
    "                    manual_labels[category] = []\n",
    "                manual_labels[category].append(score)\n",
    "    return manual_labels\n",
    "\n",
    "def extract_scores_llm(data: List[Tuple[str, Dict[str, int], bool, str]]) -> Dict[str, List[int]]:\n",
    "    is_valid = []\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        scores = item[1]  # The scores dictionary is the second element of each tuple\n",
    "        if scores is None:\n",
    "            is_valid.append(False)\n",
    "        else:\n",
    "            is_valid.append(True)\n",
    "            for category, score in scores.items():\n",
    "                if category not in result:\n",
    "                    result[category] = []\n",
    "                result[category].append(score)\n",
    "    return result, is_valid\n",
    "\n",
    "def cohens_kappa(scores1: Dict[str, List[int]], scores2: Dict[str, List[int]]) -> Dict[str, float]:\n",
    "    def kappa(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        n = len(a)\n",
    "        categories = np.unique(np.concatenate([a, b]))\n",
    "        n_categories = len(categories)\n",
    "        \n",
    "        # Observed agreement\n",
    "        observed = np.sum(a == b) / n\n",
    "        \n",
    "        # Expected agreement\n",
    "        expected = sum((np.sum(a == i) / n) * (np.sum(b == i) / n) for i in categories)\n",
    "        \n",
    "        # Compute kappa\n",
    "        kappa = (observed - expected) / (1 - expected + EPS)\n",
    "        return kappa\n",
    "\n",
    "    results = {}\n",
    "    for category in scores1.keys():\n",
    "        a = np.array(scores1[category])\n",
    "        b = np.array(scores2[category])\n",
    "        results[category] = kappa(a, b)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_kappa_for_files(file1: str, file2: str) -> Dict[str, float]:\n",
    "    manual_labels = load_json(file1)\n",
    "    llm_labels = load_pkl(file2)\n",
    "\n",
    "    print(f'Length of manual labels: {len(manual_labels)}')\n",
    "    print(f'Length of LLM labels: {len(llm_labels)}')\n",
    "\n",
    "    # Find overlapping keys\n",
    "    # overlap = set(data1.keys()) & set(data2.keys())\n",
    "    # print(f'Number of shared keys: {len(overlap)}')\n",
    "    # data1_overlap, data2_overlap = {}, {}\n",
    "    # for key in overlap:\n",
    "    #     data1_overlap[key] = data1[key]\n",
    "    #     data2_overlap[key] = data2[key]\n",
    "\n",
    "\n",
    "\n",
    "    scores_llm, is_valid_llm_output = extract_scores_llm(llm_labels)\n",
    "    print(f'Number of invalid valid scores: {len(is_valid_llm_output) - sum(is_valid_llm_output)}')\n",
    "    scores_manual = extract_scores_manual(manual_labels, is_valid_llm_output)\n",
    "    \n",
    "    return cohens_kappa(scores_llm, scores_manual)\n",
    "\n",
    "\n",
    "kappa_scores = compute_kappa_for_files(file1, file2)\n",
    "\n",
    "print(\"Cohen's Kappa scores for each category:\")\n",
    "for category, score in kappa_scores.items():\n",
    "    print(f\"{category}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as t\n",
    "from experiments.utils_bib_dataset import profession_dict\n",
    "\n",
    "node_effects = {}\n",
    "for labels in llm_labels:\n",
    "    node_effects[profession_dict[labels]] = t.tensor(llm_labels[labels])\n",
    "\n",
    "# save as llm_autointerp/node_effects_autointerp.pkl\n",
    "with open(os.path.join('llm_autointerp', node_effects_filename), 'wb') as f:\n",
    "    pickle.dump(node_effects, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
