{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pickle\n",
    "import json\n",
    "from typing import Optional\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "import time\n",
    "\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "\n",
    "import experiments.probe_training as probe_training\n",
    "import experiments.utils as utils\n",
    "import experiments.eval_saes as eval_saes\n",
    "import experiments.autointerp as autointerp\n",
    "import experiments.llm_autointerp.llm_query as llm_query\n",
    "\n",
    "from experiments.pipeline_config import PipelineConfig\n",
    "from experiments.probe_training import (\n",
    "    load_and_prepare_dataset,\n",
    "    get_train_test_data,\n",
    "    test_probe,\n",
    "    prepare_probe_data,\n",
    "    get_all_activations,\n",
    "    Probe,\n",
    ")\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "\n",
    "class FeatureSelection(Enum):\n",
    "    unique = 1\n",
    "    above_threshold = 2\n",
    "    top_n = 3\n",
    "\n",
    "\n",
    "# Metric function effectively maximizing the logit difference between the classes: selected, and nonclass\n",
    "\n",
    "\n",
    "def metric_fn(\n",
    "    model: LanguageModel, labels: t.Tensor, probe: Probe, probe_act_submodule: utils.submodule_alias\n",
    "):\n",
    "    attn_mask = model.input[1][\"attention_mask\"]\n",
    "    acts = probe_act_submodule.output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "\n",
    "    return t.where(labels == utils.POSITIVE_CLASS_LABEL, probe(acts), -probe(acts))\n",
    "\n",
    "\n",
    "# Attribution Patching\n",
    "\n",
    "\n",
    "def get_class_nonclass_samples(data: dict, class_idx: int, device: str) -> tuple[list, t.Tensor]:\n",
    "    \"\"\"This is for getting equal number of text samples from the chosen class and all other classes.\n",
    "    We use this for attribution patching.\"\"\"\n",
    "    class_samples = data[class_idx]\n",
    "\n",
    "    if isinstance(class_samples, dict) and isinstance(class_samples.get(\"input_ids\"), t.Tensor):\n",
    "        # Combine all non-class tensors\n",
    "\n",
    "        nonclass_input_ids = []\n",
    "        nonclass_attention_mask = []\n",
    "        for profession in data:\n",
    "            if profession != class_idx and isinstance(profession, int):\n",
    "                nonclass_input_ids.append(data[profession][\"input_ids\"])\n",
    "                nonclass_attention_mask.append(data[profession][\"attention_mask\"])\n",
    "        nonclass_input_ids = t.cat(nonclass_input_ids, dim=0)\n",
    "        nonclass_attention_mask = t.cat(nonclass_attention_mask, dim=0)\n",
    "\n",
    "        # Randomly select indices\n",
    "        num_class_samples = class_samples[\"input_ids\"].size(0)\n",
    "        indices = t.randperm(nonclass_input_ids.size(0))[:num_class_samples]\n",
    "\n",
    "        # Select random samples\n",
    "        nonclass_input_ids = nonclass_input_ids[indices]\n",
    "        nonclass_attention_mask = nonclass_attention_mask[indices]\n",
    "\n",
    "        combined_input_ids = t.cat([class_samples[\"input_ids\"], nonclass_input_ids], dim=0)\n",
    "        combined_attention_mask = t.cat(\n",
    "            [class_samples[\"attention_mask\"], nonclass_attention_mask], dim=0\n",
    "        )\n",
    "\n",
    "        combined_input_ids[::2] = class_samples[\"input_ids\"]\n",
    "        combined_input_ids[1::2] = nonclass_input_ids\n",
    "        combined_attention_mask[::2] = class_samples[\"attention_mask\"]\n",
    "        combined_attention_mask[1::2] = nonclass_attention_mask\n",
    "\n",
    "        combined_samples = {\n",
    "            \"input_ids\": combined_input_ids,\n",
    "            \"attention_mask\": combined_attention_mask,\n",
    "        }\n",
    "\n",
    "        num_class_samples = class_samples[\"input_ids\"].size(0)\n",
    "        num_nonclass_samples = nonclass_input_ids.size(0)\n",
    "        num_combined_samples = num_class_samples + num_nonclass_samples\n",
    "    elif isinstance(class_samples, list) and isinstance(class_samples[0], str):\n",
    "        nonclass_samples = []\n",
    "        for profession in data:\n",
    "            if profession != class_idx:\n",
    "                nonclass_samples.extend(data[profession])\n",
    "\n",
    "        nonclass_samples = random.sample(nonclass_samples, len(class_samples))\n",
    "        combined_samples = class_samples + nonclass_samples\n",
    "        num_class_samples = len(class_samples)\n",
    "        num_nonclass_samples = len(nonclass_samples)\n",
    "        num_combined_samples = num_class_samples + num_nonclass_samples\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type\")\n",
    "\n",
    "    combined_labels = t.empty(num_combined_samples, dtype=t.int, device=device)\n",
    "    combined_labels[::2] = utils.POSITIVE_CLASS_LABEL\n",
    "    combined_labels[1::2] = utils.NEGATIVE_CLASS_LABEL\n",
    "\n",
    "    return combined_samples, combined_labels\n",
    "\n",
    "\n",
    "def get_class_samples(data: dict, class_idx: int, device: str) -> tuple[list, t.Tensor]:\n",
    "    \"\"\"This is for getting equal number of text samples from the chosen class and all other classes.\n",
    "    We use this for attribution patching.\"\"\"\n",
    "    class_samples = data[class_idx]\n",
    "\n",
    "    if isinstance(class_samples, list) and isinstance(class_samples[0], str):\n",
    "        num_class_samples = len(class_samples)\n",
    "    elif isinstance(class_samples, dict) and isinstance(class_samples[\"input_ids\"], t.Tensor):\n",
    "        num_class_samples = class_samples[\"input_ids\"].size(0)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type\")\n",
    "\n",
    "    class_labels = t.full(\n",
    "        (num_class_samples,), utils.POSITIVE_CLASS_LABEL, dtype=t.int, device=device\n",
    "    )\n",
    "\n",
    "    return class_samples, class_labels\n",
    "\n",
    "\n",
    "# TODO: Think about removing support for list of string inputs\n",
    "def get_paired_class_samples(data: dict, class_idx, device: str) -> tuple[list, t.Tensor]:\n",
    "    \"\"\"This is for getting equal number of text samples from the chosen class and all other classes.\n",
    "    We use this for attribution patching.\"\"\"\n",
    "\n",
    "    # TODO: Clean this up\n",
    "    # Switch from interleaving to shuffling\n",
    "\n",
    "    if class_idx not in utils.PAIRED_CLASS_KEYS:\n",
    "        raise ValueError(f\"Class {class_idx} not in PAIRED_CLASS_KEYS\")\n",
    "\n",
    "    class_samples = data[class_idx]\n",
    "    paired_class_idx = utils.PAIRED_CLASS_KEYS[class_idx]\n",
    "    paired_class_samples = data[paired_class_idx]\n",
    "\n",
    "    if isinstance(class_samples, list) and isinstance(class_samples[0], str):\n",
    "        combined_samples = class_samples + paired_class_samples\n",
    "        num_class_samples = len(class_samples)\n",
    "        num_nonclass_samples = len(paired_class_samples)\n",
    "        num_combined_samples = num_class_samples + num_nonclass_samples\n",
    "\n",
    "        # Interleave the samples\n",
    "        combined_samples = [None] * num_combined_samples\n",
    "        combined_samples[::2] = class_samples\n",
    "        combined_samples[1::2] = paired_class_samples\n",
    "    elif isinstance(class_samples, dict) and isinstance(class_samples[\"input_ids\"], t.Tensor):\n",
    "        combined_input_ids = t.cat(\n",
    "            [class_samples[\"input_ids\"], paired_class_samples[\"input_ids\"]], dim=0\n",
    "        )\n",
    "        combined_attention_mask = t.cat(\n",
    "            [class_samples[\"attention_mask\"], paired_class_samples[\"attention_mask\"]], dim=0\n",
    "        )\n",
    "\n",
    "        combined_input_ids[::2] = class_samples[\"input_ids\"]\n",
    "        combined_input_ids[1::2] = paired_class_samples[\"input_ids\"]\n",
    "        combined_attention_mask[::2] = class_samples[\"attention_mask\"]\n",
    "        combined_attention_mask[1::2] = paired_class_samples[\"attention_mask\"]\n",
    "\n",
    "        combined_samples = {\n",
    "            \"input_ids\": combined_input_ids,\n",
    "            \"attention_mask\": combined_attention_mask,\n",
    "        }\n",
    "        num_class_samples = class_samples[\"input_ids\"].size(0)\n",
    "        num_nonclass_samples = paired_class_samples[\"input_ids\"].size(0)\n",
    "        num_combined_samples = num_class_samples + num_nonclass_samples\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type\")\n",
    "\n",
    "    assert num_class_samples == num_nonclass_samples\n",
    "\n",
    "    # combined_labels = [utils.POSITIVE_CLASS_LABEL] * num_class_samples + [\n",
    "    #     utils.NEGATIVE_CLASS_LABEL\n",
    "    # ] * num_nonclass_samples\n",
    "\n",
    "    # Create interleaved labels\n",
    "    combined_labels = t.empty(num_combined_samples, dtype=t.int, device=device)\n",
    "    combined_labels[::2] = utils.POSITIVE_CLASS_LABEL\n",
    "    combined_labels[1::2] = utils.NEGATIVE_CLASS_LABEL\n",
    "\n",
    "    return combined_samples, combined_labels\n",
    "\n",
    "\n",
    "def get_effects_per_class(\n",
    "    model: LanguageModel,\n",
    "    submodules: list[utils.submodule_alias],\n",
    "    dictionaries: dict[utils.submodule_alias, AutoEncoder],\n",
    "    probes: dict[int | str, Probe],\n",
    "    probe_act_submodule: utils.submodule_alias,\n",
    "    class_idx: int | str,\n",
    "    train_bios: dict,\n",
    "    seed: int,\n",
    "    batch_size: int = 10,\n",
    "    patching_method: str = \"attrib\",\n",
    "    steps: int = 10,  # only used for ig\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    Probe_act_submodule is the submodule where the probe is attached, usually resid_post.\n",
    "    Att the end of the function nodes is a dict of submodules to tensors. This is if we want to intervene on multiple autoencoders.\n",
    "    We aren't currently using this feature, so we currently only return the tensor.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    probe = probes[class_idx]\n",
    "\n",
    "    if isinstance(class_idx, int):\n",
    "        inputs_train, labels_train = get_class_samples(train_bios, class_idx, device)\n",
    "        # texts_train, labels_train = get_class_nonclass_samples(train_bios, class_idx, device)\n",
    "    else:\n",
    "        inputs_train, labels_train = get_paired_class_samples(train_bios, class_idx, device)\n",
    "\n",
    "    inputs_train = utils.batch_inputs(inputs_train, batch_size)\n",
    "    labels_train = utils.batch_inputs(labels_train, batch_size)\n",
    "\n",
    "    running_total = 0\n",
    "    running_nodes = None\n",
    "\n",
    "    n_batches = len(inputs_train)\n",
    "\n",
    "    for batch_idx, (clean, labels) in enumerate(zip(inputs_train, labels_train)):\n",
    "\n",
    "        print(f\"labels.shape: {labels.shape}\")\n",
    "\n",
    "        if batch_idx == n_batches:\n",
    "            break\n",
    "\n",
    "        effects, _, _, _ = patching_effect(\n",
    "            clean,\n",
    "            None,\n",
    "            model,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            metric_fn,\n",
    "            metric_kwargs=dict(labels=labels, probe=probe, probe_act_submodule=probe_act_submodule),\n",
    "            method=patching_method,\n",
    "            steps=steps,\n",
    "        )\n",
    "        with t.no_grad():\n",
    "            if running_nodes is None:\n",
    "                running_nodes = {\n",
    "                    k: len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()\n",
    "                }\n",
    "            else:\n",
    "                for k, v in effects.items():\n",
    "                    running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "            running_total += len(clean)\n",
    "        del effects, _\n",
    "        gc.collect()\n",
    "\n",
    "    nodes = {k: v / running_total for k, v in running_nodes.items()}\n",
    "    # Convert SparseAct to Tensor\n",
    "    nodes = {k: v.act for k, v in nodes.items()}\n",
    "\n",
    "    assert len(nodes) == 1, \"Only one submodule should be intervened on\"\n",
    "    node_value = next(iter(nodes.values()))\n",
    "\n",
    "    return node_value\n",
    "\n",
    "\n",
    "def get_all_node_effects_for_one_sae(\n",
    "    model: LanguageModel,\n",
    "    submodules: list[utils.submodule_alias],\n",
    "    dictionaries: dict[utils.submodule_alias, AutoEncoder],\n",
    "    ae_path: str,\n",
    "    force_recompute: bool,\n",
    "    probes: dict[int | str, Probe],\n",
    "    probe_act_submodule: utils.submodule_alias,\n",
    "    chosen_class_indices: list[int | str],\n",
    "    train_bios: dict,\n",
    "    seed: int,\n",
    "    batch_size: int = 10,\n",
    "    patching_method: str = \"attrib\",\n",
    "    steps: int = 10,  # only used for ig\n",
    ") -> t.Tensor:\n",
    "    node_effects_path = os.path.join(ae_path, \"node_effects.pkl\")\n",
    "\n",
    "    if os.path.exists(node_effects_path) and not force_recompute:\n",
    "        print(f\"Loading node effects from {node_effects_path}\")\n",
    "\n",
    "        with open(node_effects_path, \"rb\") as f:\n",
    "            node_effects = pickle.load(f)\n",
    "        return node_effects\n",
    "    if not os.path.exists(node_effects_path):\n",
    "        print(f\"Node effects not found, computing for {ae_path}\")\n",
    "    elif force_recompute:\n",
    "        print(f\"Recomputing node effects for {ae_path}\")\n",
    "\n",
    "    node_effects = {}\n",
    "\n",
    "    for ablated_class_idx in tqdm(chosen_class_indices, \"Getting node effects\"):\n",
    "        node_effects[ablated_class_idx] = get_effects_per_class(\n",
    "            model,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            probes,\n",
    "            probe_act_submodule,\n",
    "            ablated_class_idx,\n",
    "            train_bios,\n",
    "            random_seed,\n",
    "            batch_size=batch_size,\n",
    "            patching_method=patching_method,\n",
    "            steps=steps,\n",
    "        )\n",
    "    node_effects = utils.to_device(node_effects, \"cpu\")\n",
    "\n",
    "    save_log_files(ae_path, node_effects, \"node_effects\", \".pkl\")\n",
    "\n",
    "    return node_effects\n",
    "\n",
    "\n",
    "# Get the output activations for the submodule where some saes are ablated\n",
    "# Currently deprecated\n",
    "def get_acts_ablated(text, model, submodules, dictionaries, to_ablate):\n",
    "    is_tuple = {}\n",
    "    with t.no_grad(), model.trace(\"_\"):\n",
    "        for submodule in submodules:\n",
    "            is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            feat_idxs = to_ablate[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            res = x - x_hat\n",
    "            f[..., feat_idxs] = 0.0  # zero ablation\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = dictionary.decode(f) + res\n",
    "            else:\n",
    "                submodule.output = dictionary.decode(f) + res\n",
    "        attn_mask = model.input[1][\"attention_mask\"]\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "\n",
    "    t.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return act.value\n",
    "\n",
    "\n",
    "# Get the output activations for the submodule where some saes are ablated\n",
    "@t.no_grad()\n",
    "def get_all_acts_ablated(\n",
    "    text_inputs: list[str],\n",
    "    model: LanguageModel,\n",
    "    submodules: list[utils.submodule_alias],\n",
    "    dictionaries: dict[utils.submodule_alias, AutoEncoder],\n",
    "    to_ablate: t.Tensor,\n",
    "    batch_size: int,\n",
    "    probe_submodule: utils.submodule_alias,\n",
    "):\n",
    "    text_batches = utils.batch_inputs(text_inputs, batch_size)\n",
    "\n",
    "    is_tuple = {}\n",
    "    with t.no_grad(), model.trace(\"_\"):\n",
    "        for submodule in submodules:\n",
    "            is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "    all_acts_list_BD = []\n",
    "    for text_batch_BL in text_batches:\n",
    "        with t.no_grad(), model.trace(text_batch_BL, **tracer_kwargs):\n",
    "            for submodule in submodules:\n",
    "                dictionary = dictionaries[submodule]\n",
    "                # feat_idxs = to_ablate[submodule] # Uncomment this line to restore ablating multiple SAEs\n",
    "                feat_idxs = to_ablate\n",
    "                x = submodule.output\n",
    "                if is_tuple[submodule]:\n",
    "                    x = x[0]\n",
    "                x_hat, f = dictionary(x, output_features=True)\n",
    "                res = x - x_hat\n",
    "                f[..., feat_idxs] = 0.0  # zero ablation\n",
    "                if is_tuple[submodule]:\n",
    "                    submodule.output[0][:] = dictionary.decode(f) + res\n",
    "                else:\n",
    "                    submodule.output = dictionary.decode(f) + res\n",
    "            attn_mask = model.input[1][\"attention_mask\"]\n",
    "            act = probe_submodule.output[0]\n",
    "            act = act * attn_mask[:, :, None]\n",
    "            act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            act = act.save()\n",
    "        all_acts_list_BD.append(act.value)\n",
    "\n",
    "    all_acts_bD = t.cat(all_acts_list_BD, dim=0)\n",
    "\n",
    "    return all_acts_bD\n",
    "\n",
    "\n",
    "# putting feats_to_ablate in a more useful format\n",
    "def n_hot(feats, dim, device=\"cpu\"):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=device)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n",
    "\n",
    "\n",
    "def select_significant_features(\n",
    "    node_effects: dict[int, dict[utils.submodule_alias, t.Tensor]],\n",
    "    dict_size: int,\n",
    "    T_effect: float = 0.001,\n",
    "    verbose: bool = True,\n",
    "    convert_to_n_hot: bool = True,\n",
    "    device: str = \"cpu\",\n",
    ") -> dict[int, dict[utils.submodule_alias, t.Tensor]]:\n",
    "    \"\"\"There's a bug somewhere in here if the T_effect is too high, it will return an empty dict.\"\"\"\n",
    "    feats_above_T = {}\n",
    "\n",
    "    for abl_class_idx in node_effects.keys():\n",
    "        total_features_per_abl_class = 0\n",
    "        feats_above_T[abl_class_idx] = defaultdict(list)\n",
    "        for submodule in node_effects[abl_class_idx].keys():\n",
    "            # TODO: Warning about .nonzero() and bools\n",
    "            for feat_idx in (node_effects[abl_class_idx][submodule] > T_effect).nonzero():\n",
    "                feats_above_T[abl_class_idx][submodule].append(feat_idx.item())\n",
    "                total_features_per_abl_class += 1\n",
    "        if convert_to_n_hot:\n",
    "            feats_above_T[abl_class_idx] = {\n",
    "                submodule: n_hot(feats, dict_size, device)\n",
    "                for submodule, feats in feats_above_T[abl_class_idx].items()\n",
    "            }\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"T_effect {T_effect}, class {abl_class_idx}, all submodules, #significant features: {total_features_per_abl_class}\"\n",
    "            )\n",
    "\n",
    "    return feats_above_T\n",
    "\n",
    "\n",
    "def select_significant_features2(\n",
    "    node_effects: dict[int | str, t.Tensor],\n",
    "    T_effect: float = 0.001,\n",
    "    verbose: bool = True,\n",
    ") -> dict[int | str, t.Tensor]:\n",
    "    \"\"\"This function is more idiomatic pytorch and doesn't have the bug of returning an empty dict.\"\"\"\n",
    "    # TODO: Switch over to this function, or maybe use the other one for the unique class features.\n",
    "    feats_above_T = {}\n",
    "\n",
    "    for abl_class_idx in node_effects.keys():\n",
    "        total_features_per_abl_class = 0\n",
    "        feats_above_T[abl_class_idx] = {}\n",
    "        feats_above_T[abl_class_idx] = node_effects[abl_class_idx] > T_effect\n",
    "        total_features_per_abl_class += feats_above_T[abl_class_idx].sum().item()\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"T_effect {T_effect}, class {abl_class_idx}, all submodules, #significant features: {total_features_per_abl_class}\"\n",
    "        )\n",
    "\n",
    "    return feats_above_T\n",
    "\n",
    "\n",
    "def select_top_n_features(\n",
    "    node_effects: dict[int | str, t.Tensor],\n",
    "    n: int,\n",
    ") -> dict[int | str, t.Tensor]:\n",
    "    top_n_features = {}\n",
    "\n",
    "    for abl_class_idx, effects in node_effects.items():\n",
    "        top_n_features[abl_class_idx] = {}\n",
    "\n",
    "        assert (\n",
    "            n <= effects.numel()\n",
    "        ), f\"n ({n}) must not be larger than the number of features ({effects.numel()}) for ablation class {abl_class_idx}\"\n",
    "\n",
    "        # Find non-zero effects\n",
    "        non_zero_mask = effects != 0\n",
    "        non_zero_effects = effects[non_zero_mask]\n",
    "        num_non_zero = non_zero_effects.numel()\n",
    "\n",
    "        if num_non_zero < n:\n",
    "            print(\n",
    "                f\"WARNING: only {num_non_zero} non-zero effects found for ablation class {abl_class_idx}, which is less than the requested {n}.\"\n",
    "            )\n",
    "\n",
    "        # Select top n or all non-zero effects, whichever is smaller\n",
    "        k = min(n, num_non_zero)\n",
    "\n",
    "        if k == 0:\n",
    "            print(\n",
    "                f\"WARNING: No non-zero effects found for ablation class {abl_class_idx}. Returning an empty mask.\"\n",
    "            )\n",
    "            top_n_features[abl_class_idx] = t.zeros_like(effects, dtype=t.bool)\n",
    "        else:\n",
    "            # Get the indices of the top N effects\n",
    "            _, top_indices = t.topk(effects, k)\n",
    "\n",
    "            # Create a boolean mask tensor\n",
    "            mask = t.zeros_like(effects, dtype=t.bool)\n",
    "            mask[top_indices] = True\n",
    "\n",
    "            top_n_features[abl_class_idx] = mask\n",
    "\n",
    "    return top_n_features\n",
    "\n",
    "\n",
    "def select_unique_class_features(\n",
    "    node_effects: dict[int, dict[utils.submodule_alias, t.Tensor]],\n",
    "    dict_size: int,\n",
    "    T_effect: float = 0.001,\n",
    "    T_max_sideeffect: float = 0.000001,\n",
    "    verbose: bool = True,\n",
    "    device: str = \"cpu\",\n",
    ") -> dict[int, dict[utils.submodule_alias, t.Tensor]]:\n",
    "    non_neglectable_feats = select_significant_features(\n",
    "        node_effects, dict_size, T_max_sideeffect, convert_to_n_hot=False, verbose=True\n",
    "    )\n",
    "    significant_feats = select_significant_features(\n",
    "        node_effects, dict_size, T_effect, convert_to_n_hot=False, verbose=True\n",
    "    )\n",
    "\n",
    "    feats_above_T = {}\n",
    "    for abl_class_idx in node_effects.keys():\n",
    "        total_features_per_abl_class = 0\n",
    "        feats_above_T[abl_class_idx] = defaultdict(list)\n",
    "        for submodule in node_effects[abl_class_idx].keys():\n",
    "            # Get a blacklist of features that have side effects above T_max_sideeffect in other submodules\n",
    "            sideeffect_features = []\n",
    "            for other_class_idx in node_effects.keys():\n",
    "                if other_class_idx != abl_class_idx:\n",
    "                    sideeffect_features.extend(non_neglectable_feats[other_class_idx][submodule])\n",
    "            sideeffect_features = set(sideeffect_features)\n",
    "            if verbose:\n",
    "                print(f\"sideeffect features: {len(sideeffect_features)}\")\n",
    "\n",
    "            # Add features above T_effect that are not in the blacklist\n",
    "            for feat_idx in significant_feats[abl_class_idx][submodule]:\n",
    "                if feat_idx not in sideeffect_features:\n",
    "                    feats_above_T[abl_class_idx][submodule].append(feat_idx)\n",
    "                    total_features_per_abl_class += 1\n",
    "        feats_above_T[abl_class_idx] = {\n",
    "            submodule: n_hot(feats, dict_size, device)\n",
    "            for submodule, feats in feats_above_T[abl_class_idx].items()\n",
    "        }\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"T_effect {T_effect}, class {abl_class_idx}, all submodules, #unique features: {total_features_per_abl_class}\"\n",
    "            )\n",
    "\n",
    "    return feats_above_T\n",
    "\n",
    "\n",
    "def select_features(\n",
    "    selection_method: FeatureSelection,\n",
    "    node_effects: dict[int | str, t.Tensor],\n",
    "    dict_size: int,\n",
    "    T_effects: list[float],\n",
    "    T_max_sideeffect: float,\n",
    "    verbose: bool = False,\n",
    ") -> dict[int | float, dict[int | str, t.Tensor]]:\n",
    "    selected_features = {}\n",
    "    if selection_method == FeatureSelection.unique:\n",
    "        for T_effect in T_effects:\n",
    "            selected_features[T_effect] = select_unique_class_features(\n",
    "                node_effects,\n",
    "                dict_size,\n",
    "                T_effect=T_effect,\n",
    "                T_max_sideeffect=T_max_sideeffect,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "    elif selection_method == FeatureSelection.above_threshold:\n",
    "        for T_effect in T_effects:\n",
    "            selected_features[T_effect] = select_significant_features2(\n",
    "                node_effects, T_effect=T_effect, verbose=verbose\n",
    "            )\n",
    "    elif selection_method == FeatureSelection.top_n:\n",
    "        for n in T_effects:\n",
    "            selected_features[n] = select_top_n_features(node_effects, n)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid selection method\")\n",
    "\n",
    "    for T_effect in T_effects:\n",
    "        for ablated_class_idx in selected_features[T_effect]:\n",
    "            mask = selected_features[T_effect][ablated_class_idx]\n",
    "            effects = node_effects[ablated_class_idx]\n",
    "            assert mask.size() == effects.size(), \"Mask and effects must have the same size\"\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "def save_log_files(ae_path: str, data: dict, base_filename: str, extension: str):\n",
    "    # Always save/overwrite the main file\n",
    "    main_file = os.path.join(ae_path, f\"{base_filename}{extension}\")\n",
    "    with open(main_file, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Saved main file: {base_filename}{extension}\")\n",
    "\n",
    "    # Find the next available number for the backup file\n",
    "    counter = 1\n",
    "    while True:\n",
    "        backup_filename = f\"{base_filename}{counter}{extension}\"\n",
    "        full_path = os.path.join(ae_path, backup_filename)\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            with open(full_path, \"wb\") as f:\n",
    "                pickle.dump(data, f)\n",
    "            print(f\"Saved backup as: {backup_filename}\")\n",
    "            break\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "selection_method = FeatureSelection.above_threshold\n",
    "selection_method = FeatureSelection.top_n\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "chosen_class_indices = [\n",
    "    # \"male / female\",\n",
    "    # \"professor / nurse\",\n",
    "    # \"male_professor / female_nurse\",\n",
    "    # \"biased_male / biased_female\",\n",
    "    0,\n",
    "    1,\n",
    "    # 2,\n",
    "    # 6,\n",
    "]\n",
    "\n",
    "top_n_features = [2, 5, 10, 20, 50, 100, 500, 2000]\n",
    "# top_n_features = [5, 10, 20, 50, 500]\n",
    "top_n_features = [2, 5, 10, 20]\n",
    "T_effects_all_classes = [0.1, 0.05, 0.025, 0.01, 0.001]\n",
    "T_effects_all_classes = [0.1, 0.01]\n",
    "T_effects_unique_class = [1e-4, 1e-8]\n",
    "\n",
    "if selection_method == FeatureSelection.top_n:\n",
    "    T_effects = top_n_features\n",
    "elif selection_method == FeatureSelection.above_threshold:\n",
    "    T_effects = T_effects_all_classes\n",
    "elif selection_method == FeatureSelection.unique:\n",
    "    T_effects = T_effects_unique_class\n",
    "else:\n",
    "    raise ValueError(\"Invalid selection method\")\n",
    "\n",
    "T_max_sideeffect = 5e-3\n",
    "\n",
    "# Use for debugging / any time you need to run from root dir\n",
    "# dictionaries_path = \"dictionary_learning/dictionaries\"\n",
    "# probes_dir = \"experiments/trained_bib_probes\"\n",
    "\n",
    "# Example of sweeping over all SAEs in a sweep\n",
    "ae_sweep_paths = {\"pythia70m_test_sae\": None}\n",
    "\n",
    "# Example of sweeping over all SAEs in a submodule\n",
    "ae_sweep_paths = {\"pythia70m_test_sae\": {\"resid_post_layer_3\": {\"trainer_ids\": None}}}\n",
    "\n",
    "# Example of sweeping over a single SAE\n",
    "ae_sweep_paths = {\"pythia70m_test_sae\": {\"resid_post_layer_3\": {\"trainer_ids\": [0]}}}\n",
    "\n",
    "ae_sweep_paths = {\"pythia70m_sweep_standard_ctx128_0712\": None}\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\"resid_post_layer_3\": {\"trainer_ids\": None}}\n",
    "}\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": [1, 7, 11, 18]}\n",
    "        # \"resid_post_layer_3\": {\"trainer_ids\": [18]}\n",
    "    }\n",
    "}\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_topk_ctx128_0730\": {\n",
    "        # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_3\": {\"trainer_ids\": None},\n",
    "        # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": [6]},\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer_ids = [2, 6, 10, 14, 18]\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    \"pythia70m_sweep_standard_ctx128_0712\": {\n",
    "        #     # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "        #     # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "        #     # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "        \"resid_post_layer_3\": {\"trainer_ids\": [6]},\n",
    "        #     \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "    },\n",
    "    # \"pythia70m_sweep_gated_ctx128_0730\": {\n",
    "    #     # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "    #     \"resid_post_layer_3\": {\"trainer_ids\": [2, 6, 10, 18]},\n",
    "    #     # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "    # },\n",
    "    # \"pythia70m_sweep_panneal_ctx128_0730\": {\n",
    "    #     # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "    #     \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "    #     # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "    # },\n",
    "    # \"pythia70m_sweep_topk_ctx128_0730\": {\n",
    "    #     # \"resid_post_layer_0\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_1\": {\"trainer_ids\": None},\n",
    "    #     # \"resid_post_layer_2\": {\"trainer_ids\": None},\n",
    "    #     \"resid_post_layer_3\": {\"trainer_ids\": [2, 6, 10, 18]},\n",
    "    #     # \"resid_post_layer_4\": {\"trainer_ids\": None},\n",
    "    # },\n",
    "}\n",
    "\n",
    "trainer_ids = None\n",
    "trainer_ids = [0]\n",
    "\n",
    "ae_sweep_paths = {\n",
    "    # \"gemma-2-2b_sweep_topk_ctx128_ef8_0824\": {\n",
    "    #     # \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "    #     # \"resid_post_layer_7\": {\"trainer_ids\": trainer_ids},\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    #     # \"resid_post_layer_15\": {\"trainer_ids\": trainer_ids},\n",
    "    #     # \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "    # \"gemma-2-2b_sweep_standard_ctx128_ef2_0824\": {\n",
    "    #     # \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "    #     # \"resid_post_layer_7\": {\"trainer_ids\": trainer_ids},\n",
    "    #     \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "    #     # \"resid_post_layer_15\": {\"trainer_ids\": trainer_ids},\n",
    "    #     # \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "    # },\n",
    "    \"gemma-2-2b_sweep_jumprelu_0902\": {\n",
    "        # \"resid_post_layer_3\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_7\": {\"trainer_ids\": trainer_ids},\n",
    "        \"resid_post_layer_11\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_15\": {\"trainer_ids\": trainer_ids},\n",
    "        # \"resid_post_layer_19\": {\"trainer_ids\": trainer_ids},\n",
    "    },\n",
    "}\n",
    "\n",
    "p_config = PipelineConfig()\n",
    "\n",
    "if p_config.use_autointerp:\n",
    "    with open(\"../anthropic_api_key.txt\", \"r\") as f:\n",
    "        api_key = f.read().strip()\n",
    "\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
    "\n",
    "# This will look for any empty folders in any ae_path and raise an error if it finds any\n",
    "for sweep_name, submodule_trainers in ae_sweep_paths.items():\n",
    "    ae_group_paths = utils.get_ae_group_paths(\n",
    "        p_config.dictionaries_path, sweep_name, submodule_trainers\n",
    "    )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "t.autograd.set_detect_anomaly(True)\n",
    "\n",
    "sweep_name, submodule_trainers = list(ae_sweep_paths.items())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "device = \"cuda\"\n",
    "verbose = True\n",
    "\n",
    "model_eval_config = utils.ModelEvalConfig.from_sweep_name(sweep_name)\n",
    "model_name = model_eval_config.full_model_name\n",
    "\n",
    "llm_batch_size, patching_batch_size, eval_results_batch_size = utils.get_batch_sizes(\n",
    "    model_eval_config,\n",
    "    p_config.reduced_GPU_memory,\n",
    "    p_config.train_set_size,\n",
    "    p_config.test_set_size,\n",
    "    p_config.probe_train_set_size,\n",
    "    p_config.probe_test_set_size,\n",
    ")\n",
    "\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    dispatch=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=p_config.model_dtype,\n",
    ")\n",
    "\n",
    "probe_layer = model_eval_config.probe_layer\n",
    "probe_act_submodule = utils.get_submodule(model, \"resid_post\", probe_layer)\n",
    "\n",
    "ae_group_paths = utils.get_ae_group_paths(\n",
    "    p_config.dictionaries_path, sweep_name, submodule_trainers\n",
    ")\n",
    "ae_paths = utils.get_ae_paths(ae_group_paths)\n",
    "\n",
    "# TODO: experiment with different context lengths\n",
    "context_length = utils.get_ctx_length(ae_paths)\n",
    "\n",
    "# This will only run eval_saes on autoencoders that don't yet have a eval_results.json file\n",
    "eval_saes.eval_saes(\n",
    "    model,\n",
    "    ae_paths,\n",
    "    p_config.eval_saes_n_inputs,\n",
    "    eval_results_batch_size,\n",
    "    device,\n",
    "    overwrite_prev_results=True,\n",
    ")\n",
    "\n",
    "if p_config.use_autointerp:\n",
    "    autointerp.get_autointerp_inputs_for_all_saes(\n",
    "        model,\n",
    "        p_config.max_activations_collection_n_inputs,\n",
    "        llm_batch_size,\n",
    "        context_length,\n",
    "        p_config.top_k_inputs_act_collect,\n",
    "        ae_paths,\n",
    "        force_rerun=True,\n",
    "    )\n",
    "\n",
    "dataset, _ = load_and_prepare_dataset()\n",
    "train_bios, test_bios = get_train_test_data(\n",
    "    dataset,\n",
    "    p_config.train_set_size,\n",
    "    p_config.test_set_size,\n",
    "    p_config.include_gender,\n",
    ")\n",
    "\n",
    "train_bios = utils.tokenize_data(train_bios, model.tokenizer, context_length, device)\n",
    "test_bios = utils.tokenize_data(test_bios, model.tokenizer, context_length, device)\n",
    "\n",
    "only_model_name = model_name.split(\"/\")[-1]\n",
    "probe_path = f\"{p_config.probes_dir}/{only_model_name}/probes_ctx_len_{context_length}.pkl\"\n",
    "\n",
    "# TODO: Add logic to ensure probes share keys with train_bios and test_bios\n",
    "# We train the probes and save them as a file.\n",
    "if not os.path.exists(probe_path) or p_config.force_probe_recompute:\n",
    "    if p_config.force_probe_recompute:\n",
    "        print(\"Force recomputing probes\")\n",
    "    else:\n",
    "        print(\"Probes not found, training probes\")\n",
    "    probe_training.train_probes(\n",
    "        p_config.probe_train_set_size,\n",
    "        p_config.probe_test_set_size,\n",
    "        model,\n",
    "        context_length=context_length,\n",
    "        probe_batch_size=p_config.probe_batch_size,\n",
    "        llm_batch_size=llm_batch_size,\n",
    "        device=device,\n",
    "        probe_dir=p_config.probes_dir,\n",
    "        llm_model_name=model_name,\n",
    "        epochs=p_config.probe_epochs,\n",
    "        model_dtype=p_config.model_dtype,\n",
    "        include_gender=p_config.include_gender,\n",
    "    )\n",
    "\n",
    "with open(probe_path, \"rb\") as f:\n",
    "    probes = pickle.load(f)\n",
    "\n",
    "### Get activations for original model, all classes\n",
    "print(\"Getting activations for original model\")\n",
    "test_acts = {}\n",
    "for class_idx in tqdm(chosen_class_indices, desc=\"Getting activations per evaluated class\"):\n",
    "    test_acts[class_idx] = get_all_activations(\n",
    "        test_bios[class_idx], model, llm_batch_size, probe_act_submodule\n",
    "    )\n",
    "\n",
    "    if class_idx in utils.PAIRED_CLASS_KEYS:\n",
    "        paired_class_idx = utils.PAIRED_CLASS_KEYS[class_idx]\n",
    "        test_acts[paired_class_idx] = get_all_activations(\n",
    "            test_bios[paired_class_idx], model, llm_batch_size, probe_act_submodule\n",
    "        )\n",
    "\n",
    "test_accuracies = probe_training.get_probe_test_accuracy(\n",
    "    probes, chosen_class_indices, test_acts, p_config.probe_batch_size\n",
    ")\n",
    "del test_acts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "### Get activations for ablated models\n",
    "# ablating the top features for each class\n",
    "print(\"Getting activations for ablated models\")\n",
    "\n",
    "t.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "for ae_path in ae_paths:\n",
    "    print(f\"Running ablation for {ae_path}\")\n",
    "    submodules = []\n",
    "    dictionaries = {}\n",
    "    submodule, dictionary, sae_config = utils.load_dictionary(model, ae_path, device)\n",
    "    dictionary = dictionary.to(dtype=p_config.model_dtype)\n",
    "    submodules.append(submodule)\n",
    "    dictionaries[submodule] = dictionary\n",
    "    dict_size = sae_config[\"trainer\"][\"dict_size\"]\n",
    "\n",
    "    class_accuracies = {\"clean_acc\": test_accuracies}\n",
    "\n",
    "    # For every class, we get the indirect effects of every SAE feature wrt. the class probe\n",
    "    node_effects = get_all_node_effects_for_one_sae(\n",
    "        model=model,\n",
    "        submodules=submodules,\n",
    "        dictionaries=dictionaries,\n",
    "        ae_path=ae_path,\n",
    "        force_recompute=True,\n",
    "        probes=probes,\n",
    "        probe_act_submodule=probe_act_submodule,\n",
    "        chosen_class_indices=chosen_class_indices,\n",
    "        train_bios=train_bios,\n",
    "        seed=random_seed,\n",
    "        batch_size=patching_batch_size,\n",
    "        patching_method=p_config.attribution_patching_method,\n",
    "        steps=p_config.ig_steps,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # if p_config.use_autointerp:\n",
    "    #     # This will save node_effects_auto_interp.pkl, node_effects_bias_shift_dir1.pkl, and node_effects_bias_shift_dir2.pkl alongside each SAE\n",
    "    #     node_effects_auto_interp, node_effects_bias_shift_dir1, node_effects_bias_shift_dir2 = (\n",
    "    #         llm_query.perform_llm_autointerp(\n",
    "    #             tokenizer=model.tokenizer,\n",
    "    #             p_config=p_config,\n",
    "    #             ae_path=ae_path,\n",
    "    #             debug_mode=True,\n",
    "    #         )\n",
    "    #     )\n",
    "    #     all_node_effects = [\n",
    "    #         (node_effects_auto_interp, \"_auto_interp\"),\n",
    "    #         (node_effects_bias_shift_dir1, \"_bias_shift_dir1\"),\n",
    "    #         (node_effects_bias_shift_dir2, \"_bias_shift_dir2\"),\n",
    "    #         (node_effects, \"_attrib\"),\n",
    "    #     ]\n",
    "    # else:\n",
    "    #     all_node_effects = [(node_effects, \"_attrib\")]\n",
    "\n",
    "    # t.cuda.empty_cache()\n",
    "    # gc.collect()\n",
    "\n",
    "    # for node_effects_group, effects_group_name in all_node_effects:\n",
    "    #     selected_features = select_features(\n",
    "    #         selection_method,\n",
    "    #         node_effects_group,\n",
    "    #         dict_size,\n",
    "    #         T_effects,\n",
    "    #         T_max_sideeffect,\n",
    "    #         verbose=verbose,\n",
    "    #     )\n",
    "\n",
    "    #     node_effects_group_classes = list(node_effects_group.keys())\n",
    "\n",
    "    #     with t.inference_mode():\n",
    "    #         # Now that we have collected node effects and selected features, we ablate the selected features and measure the change in probe accuracy\n",
    "    #         for ablated_class_idx in node_effects_group_classes:\n",
    "    #             class_accuracies[ablated_class_idx] = {}\n",
    "    #             print(f\"evaluating class {ablated_class_idx}\")\n",
    "\n",
    "    #             for T_effect in T_effects:\n",
    "    #                 class_accuracies[ablated_class_idx][T_effect] = {}\n",
    "    #                 selected_features_mask = selected_features[T_effect][ablated_class_idx]\n",
    "\n",
    "    #                 if t.all(selected_features_mask == 0):\n",
    "    #                     print(f\"No features selected for T_effect = {T_effect}\")\n",
    "    #                     # If no features are selected, we skip the ablation\n",
    "    #                     # We set the accuracy to the clean accuracy for ease of plotting later\n",
    "    #                     class_accuracies[ablated_class_idx][T_effect] = test_accuracies\n",
    "    #                     continue\n",
    "\n",
    "    #                 if verbose:\n",
    "    #                     print(f\"Running ablation for T_effect = {T_effect}\")\n",
    "    #                     print(f\"Ablating {selected_features_mask.sum()} features\")\n",
    "    #                 test_acts_ablated = {}\n",
    "    #                 for evaluated_class_idx in tqdm(\n",
    "    #                     node_effects_group_classes, desc=\"Getting activations\"\n",
    "    #                 ):\n",
    "    #                     test_acts_ablated[evaluated_class_idx] = get_all_acts_ablated(\n",
    "    #                         test_bios[evaluated_class_idx],\n",
    "    #                         model,\n",
    "    #                         submodules,\n",
    "    #                         dictionaries,\n",
    "    #                         selected_features_mask,\n",
    "    #                         llm_batch_size,\n",
    "    #                         probe_act_submodule,\n",
    "    #                     )\n",
    "\n",
    "    #                     if evaluated_class_idx in utils.PAIRED_CLASS_KEYS:\n",
    "    #                         paired_class_idx = utils.PAIRED_CLASS_KEYS[evaluated_class_idx]\n",
    "    #                         test_acts_ablated[paired_class_idx] = get_all_acts_ablated(\n",
    "    #                             test_bios[paired_class_idx],\n",
    "    #                             model,\n",
    "    #                             submodules,\n",
    "    #                             dictionaries,\n",
    "    #                             selected_features_mask,\n",
    "    #                             llm_batch_size,\n",
    "    #                             probe_act_submodule,\n",
    "    #                         )\n",
    "\n",
    "    #                 ablated_class_accuracies = probe_training.get_probe_test_accuracy(\n",
    "    #                     probes,\n",
    "    #                     node_effects_group_classes,\n",
    "    #                     test_acts_ablated,\n",
    "    #                     p_config.probe_batch_size,\n",
    "    #                 )\n",
    "\n",
    "    #                 class_accuracies[ablated_class_idx][T_effect] = ablated_class_accuracies\n",
    "\n",
    "    #                 for evaluated_class_idx in ablated_class_accuracies:\n",
    "    #                     if verbose:\n",
    "    #                         print(\n",
    "    #                             f\"Ablated {ablated_class_idx}, evaluated {evaluated_class_idx} test accuracy: {ablated_class_accuracies[evaluated_class_idx]['acc']}\"\n",
    "    #                         )\n",
    "\n",
    "    #     class_accuracies = utils.to_device(class_accuracies, \"cpu\")\n",
    "\n",
    "    #     save_log_files(\n",
    "    #         ae_path, class_accuracies, f\"class_accuracies{effects_group_name}\", \".pkl\"\n",
    "    #     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
